<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>ETC3250/5250: Introduction to Machine Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Professor Di Cook" />
    <script src="lib/header-attrs-2.7/header-attrs.js"></script>
    <link href="lib/remark-css-0.0.1/ninjutsu.css" rel="stylesheet" />
    
    <!--
    <script defer src="assets/all.min.js"></script>

    Need below to enable css contents

    <script>
      window.FontAwesomeConfig = {
        searchPseudoElements: true
      }
    </script>

    -->
    <link rel="stylesheet" href="assets/font-awesome-all.css" type="text/css" />
    <link rel="stylesheet" href="assets/tachyons-addon.css" type="text/css" />
    <link rel="stylesheet" href="assets/animate.css" type="text/css" />
    <link rel="stylesheet" href="assets/fira-code.css" type="text/css" />
    <link rel="stylesheet" href="assets/boxes.css" type="text/css" />
    <link rel="stylesheet" href="assets/table.css" type="text/css" />
    <link rel="stylesheet" href="assets/styles.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-brand.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-fonts.css" type="text/css" />
    <link rel="stylesheet" href="assets/slide-types.css" type="text/css" />
    <link rel="stylesheet" href="assets/custom.css" type="text/css" />
    <link rel="stylesheet" href="assets/panelset.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">






class: middle center hide-slide-number monash-bg-gray80





.info-box.w-50.bg-white[
These slides are viewed best by Chrome or Firefox and occasionally need to be refreshed if elements did not load properly. See &lt;a href=lecture-04a.pdf&gt;here for the PDF &lt;i class="fas fa-file-pdf"&gt;&lt;/i&gt;&lt;/a&gt;. 
]

&lt;br&gt;

.white[Press the **right arrow** to progress to the next slide!]



---

class: title-slide
count: false
background-image: url("images/bg-02.png")

# .monash-blue[ETC3250/5250: Introduction to Machine Learning]

&lt;h1 class="monash-blue" style="font-size: 30pt!important;"&gt;&lt;/h1&gt;

&lt;br&gt;

&lt;h2 style="font-weight:900!important;"&gt;Categorical response: Discriminant analysis&lt;/h2&gt;

.bottom_abs.width100[

Lecturer: *Professor Di Cook*

Department of Econometrics and Business Statistics

&lt;i class="fas fa-envelope"&gt;&lt;/i&gt;  ETC3250.Clayton-x@monash.edu

&lt;i class="fas fa-calendar-alt"&gt;&lt;/i&gt; Week 4a

&lt;br&gt;

]





---
# Linear Discriminant Analysis

Logistic regression involves directly modeling `\(P(Y = k|X = x)\)` using the logistic function. Rounding the probabilities produces class predictions, in two class problems; selecting the class with the highest probability produces class predictions in multi-class problems.

Another approach for building a classification model is .monash-orange2[linear discriminant analysis]. This involves assuming the .monash-orange2[distribution of the predictors] is a multivariate normal, with the same variance-covariance matrix, separately for each class.

---

class: center

# Compare the pair

&lt;div style="line-height:80%;"&gt;
    &lt;br&gt;
&lt;/div&gt;

| &lt;span style="color:#3F9F7A"&gt; Logistic Regression &lt;/span&gt;    | &lt;span style="color:#3F9F7A"&gt; Linear Discriminant Analysis  &lt;/span&gt;   |
| :-------------------: |:-------------------:|
| **Goal** - directly estimate `\(P(Y \lvert X)\)` (*the dashed line*)     | **Goal** - estimate `\(P(X \lvert Y)\)` (*the contours*) to then deduce `\(P(Y \lvert X)\)`  |
| **Assumptions** - no assumptions on predictor space      | **Assumptions** - predictors are normally distributed      |
| &lt;img src="images/lecture-04a/LR.JPG", width="60%"&gt; | &lt;img src="images/lecture-04a/LDA.JPG", width="60%"&gt;      |



---

.flex[
.w-45[
&lt;img src="https://imgs.xkcd.com/comics/when_you_assume.png", width="70%"&gt;

.font_smaller2[Source: https://xkcd.com]

]

.w-45[
# Assumptions are critical in LDA

- All samples come from normal populations
- All the groups have the same variance-covariance matrix

]
]



---

# Bayes Theorem

Let `\(f_k(x)\)` be the density function for predictor `\(x\)` for class `\(k\)`. If `\(f\)` is small, the probability that `\(x\)` belongs to class `\(k\)` is small, and conversely if `\(f\)` is large.

Bayes theorem (for `\(K\)` classes) states:

.info-box[

`$$P(Y = k|X = x) = p_k(x) = \frac{\pi_kf_k(x)}{\sum_{i=1}^K \pi_kf_k(x)}$$`
]

where `\(\pi_k = P(Y = k)\)` is the prior probability that the observation comes from class `\(k\)`.



---
# LDA with `\(p=1\)` predictors

We assume `\(f_k(x)\)` is univariate .monash-orange2[Normal] (Gaussian):

`$$f_k(x) = \frac{1}{\sqrt{2 \pi} \sigma_k} \text{exp}~ \left( - \frac{1}{2 \sigma^2_k} (x - \mu_k)^2 \right)$$`

where `\(\mu_k\)` and `\(\sigma^2_k\)` are the mean and variance parameters for the `\(k\)`th class. Further assume that `\(\sigma_1^2 = \sigma_2^2 = \dots = \sigma_K^2\)`; then the conditional probabilities are

`$$p_k(x) = \frac{\pi_k \frac{1}{\sqrt{2 \pi} \sigma} \text{exp}~ \left( - \frac{1}{2 \sigma^2} (x - \mu_k)^2 \right) }{ \sum_{l = 1}^K \pi_l \frac{1}{\sqrt{2 \pi} \sigma} \text{exp}~ \left( - \frac{1}{2 \sigma^2} (x - \mu_l)^2 \right) }$$`




---
# LDA with `\(p=1\)` predictors

The Bayes classifier is assign new observation `\(X=x_0\)` to the class with the highest `\(p_k(x_0)\)`. A simplification of `\(p_k(x_0)\)` yields the .monash-orange2[discriminant functions]:

`$$\delta_k(x_0) = x_0 \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2 \sigma^2} + log(\pi_k)$$`
and the rule Bayes classifier will assign `\(x_0\)` to the class with the largest value.


---
# LDA with `\(p=1\)` predictors

If `\(K = 2\)` and `\(\pi_1 = \pi_2\)`, we assign `\(x_0\)` to class 1 if

`$$\delta_1(x_0) &gt; \delta_2(x_0)$$`

$$x_0 \frac{\mu_1}{\sigma^2} - \frac{\mu_1^2}{2 \sigma^2} + \log(\pi) &gt; x_0 \frac{\mu_2}{\sigma^2} - \frac{\mu_2^2}{2 \sigma^2} + \log(\pi) $$

which simplifies to  `\(x_0 &gt; \frac{\mu_1+\mu_2}{2}\)`.

.info-box[
This is estimated on the data with
`\(x_0 &gt; \frac{\bar{x}_1 + \bar{x}_2}{2}\)`.
]

---
# LDA with `\(p=1\)` predictors

&lt;img src="images/lecture-04a/unnamed-chunk-3-1.png" width="900" style="display: block; margin: auto;" /&gt;



---
# Multivariate LDA

To indicate that a p-dimensional random variable X has a multivariate Gaussian distribution with `\(E[X] = \mu\)` and `\(\text{Cov}(X) = \Sigma\)`, we write `\(X \sim N(\mu, \Sigma)\)`.

The multivariate normal density function is:

`$$f(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\}$$`

with `\(x, \mu\)` are `\(p\)`-dimensional vectors, `\(\Sigma\)` is a `\(p\times p\)` variance-covariance matrix.

---
# Multivariate LDA

The discriminant functions are:

`$$\delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log(\pi_k)$$`

and Bayes classifier is .monash-orange2[assign a new observation] `\(x_0\)` .monash-orange2[to the class with the highest] `\(\delta_k(x_0)\)`.

When `\(K=2\)` and `\(\pi_1=\pi_2\)` this reduces to

Assign observation `\(x_0\)` to class 1 if

`$$x_0^T\underbrace{\Sigma^{-1}(\mu_1-\mu_2)}_{dimension~reduction} &gt; \frac{1}{2}(\mu_1+\mu_2)^T\underbrace{\Sigma^{-1}(\mu_1-\mu_2)}_{dimension~reduction}$$`

.think-box[Class 1 and 2 need to be mapped to the classes in the your data. The class "to the right" on the reduced dimension will correspond to class 1 in this equation.]

---
class: transition

# Dimension reduction

---
# Dimension reduction via LDA

.monash-orange2[Discriminant space]: a benefit of LDA is that it provides a low-dimensional projection of the `\(p\)`-dimensional space, where the groups are the most separated. For `\(K=2\)`, this is

`$$\Sigma^{-1}(\mu_1-\mu_2)$$`
.info-box[This corresponds to the biggest separation between means relative to the variance-covariance.] 

&lt;br&gt;&lt;br&gt;
For `\(K&gt;2\)`, the discriminant space is found be taking an eigen-decomposition of `\(\Sigma^{-1}\Sigma_B\)`, where

`\(\Sigma_B = \frac{1}{K}\sum_{i=1}^{K} (\mu_i-\mu)(\mu_i-\mu)^T\)`



---
## Discriminant space


The dashed lines are the Bayes decision boundaries. Ellipses
that contain 95% of the probability for each of the three classes are shown. Solid line corresponds to the class boundaries from the LDA model fit to the sample.

&lt;center&gt;
&lt;a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter4/4.6.pdf" target="_BLANK"&gt; &lt;img src="images/lecture-04a/4.6.png" style="width: 80%; align: center"/&gt; &lt;/a&gt;
&lt;/center&gt;

.font_smaller2[(Chapter4/4.6.pdf)]



---
# Discriminant space: using sample statistics

.info-box[.monash-orange2[Discriminant space]: is the low-dimensional space where the class means are the furthest apart relative to the common variance-covariance.]

The discriminant space is provided by the eigenvectors after making an eigen-decomposition of `\(\hat{\Sigma}^{-1}\hat{\Sigma}_B\)`, where

`$$\small{\hat{\Sigma}_B = \frac{1}{K}\sum_{i=1}^{K} (\bar{x}_i-\bar{x})(\bar{x}_i-\bar{x})^T}
~~~\text{and}~~~
\small{\hat{\Sigma} = \frac{1}{K}\sum_{k=1}^K\frac{1}{n_k}\sum_{i=1}^{n_k} (x_i-\bar{x}_k)(x_i-\bar{x}_k)^T}$$`




---

class: split-two
layout: false

.column[.pad50px[

## Mahalanobis distance

For two `\(p\)`-dimensional vectors, Euclidean distance is

`$$d(x,y) = \sqrt{(x-y)^T(x-y)}$$`
and Mahalanobs distance is

`$$d(x,y) = \sqrt{(x-y)^T\Sigma^{-1}(x-y)}$$`

Which points are closest according to .monash-orange2[Euclidean] distance?
Which points are closest relative to the .monash-orange2[variance-covariance]?



]]
.column[.content.vmiddle.center[



&lt;img src="images/lecture-04a/unnamed-chunk-6-1.png" width="100%" style="display: block; margin: auto;" /&gt;


]]

---
## Discriminant space

Both means the same. Two different variance-covariance matrices. .purple[Discriminant space] depends on the variance-covariance matrix.

&lt;img src="images/lecture-04a/unnamed-chunk-7-1.png" width="70%" style="display: block; margin: auto;" /&gt;



---
class: transition

# Quadratic Discriminant Analysis

If the groups have different variance-covariance matrices, but still come from a normal distribution

---
# Quadratic DA (QDA)

If the variance-covariance matrices for the groups are .monash-orange2[NOT EQUAL], then the discriminant functions are:

`$$\delta_k(x) = x^T\Sigma_k^{-1}x + x^T\Sigma_k^{-1}\mu_k - \frac12\mu_k^T\Sigma_k^{-1}\mu_k - \frac12 \log{|\Sigma_k|} + \log(\pi_k)$$`

where `\(\Sigma_k\)` is the population variance-covariance for class `\(k\)`, estimated by the sample variance-covariance `\(S_k\)`, and `\(\mu_k\)` is the population mean vector for class `\(k\)`, estimated by the sample mean `\(\bar{x}_k\)`.
---
## Quadratic DA (QDA)
A quadratic boundary is obtained by relaxing the assumption of equal variance-covariance, and assume that `\(\Sigma_k \neq \Sigma_l, ~~k\neq l, k,l=1,...,K\)`

&lt;center&gt;
&lt;a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter4/4.9.pdf" target="_BLANK"&gt; &lt;img src="images/lecture-04a/4.9.png" style="width: 80%; align: center"/&gt; &lt;/a&gt;
&lt;/center&gt;

.purple[true], LDA, .green[QDA]. 

.font_smaller2[(Chapter4/4.9.pdf)]

---
# QDA: Olive oils example

.flex[


.w-45[
Even if the population is NOT normally distributed, QDA might do reasonably. On this data, region 3 has a "banana-shaped" variance-covariance, and region 2 has two separate clusters. The quadratic boundary though does well to carve the space into neat sections dividing the two regions.
]
.w-45[
&lt;img src="images/lecture-04a/unnamed-chunk-10-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]
]

---




background-size: cover
class: title-slide
background-image: url("images/bg-02.png")

&lt;a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"&gt;&lt;img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /&gt;&lt;/a&gt;&lt;br /&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.


.bottom_abs.width100[

Lecturer: *Professor Di Cook*

Department of Econometrics and Business Statistics

&lt;i class="fas fa-envelope"&gt;&lt;/i&gt;  ETC3250.Clayton-x@monash.edu

&lt;i class="fas fa-calendar-alt"&gt;&lt;/i&gt; Week 4a

&lt;br&gt;

]




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="lib/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "magula",
"highlightLanguage": "r",
"highlightLines": true,
"highlightSpans": false,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%/%total%",
"navigation": {
"scroll": false,
"touch": true,
"click": false
},
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'lib/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
