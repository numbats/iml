---
title: "ETC3250/5250: Introduction to Machine Learning"
subtitle: "Dimension reduction"
author: "Professor Di Cook"
email: "ETC3250.Clayton-x@monash.edu"
date: "Week 4b"
length: "50 minutes"
department: "Department of Econometrics and Business Statistics"
titlebgimg: "images/bg-02.png"
output:
  xaringan::moon_reader:
    css:
      - ninjutsu 
      - "assets/font-awesome-all.css"
      - "assets/tachyons-addon.css"
      - "assets/animate.css"
      - "assets/fira-code.css"
      - "assets/boxes.css"
      - "assets/table.css"
      - "assets/styles.css"
      - "assets/monash-brand.css"
      - "assets/monash-fonts.css"
      - "assets/slide-types.css"
      - "assets/custom.css"
      - "assets/panelset.css"
    self_contained: false 
    seal: false 
    chakra: 'lib/remark-latest.min.js'
    includes:
      in_header: "assets/head.html"
    lib_dir: lib
    #includes:
    #  in_header: "assets/custom.html"
    mathjax: "lib/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    nature:
      highlightStyle: magula
      highlightLanguage: r 
      highlightLines: true
      highlightSpans: false 
      countIncrementalSlides: false
      slideNumberFormat: '%current%/%total%'
      navigation:
        scroll: false 
        touch: true
        click: false
      ratio: '16:9'
---

```{r, include = FALSE}
current_file <- knitr::current_input()
basename <- gsub(".Rmd$", "", current_file)

knitr::opts_chunk$set(
  fig.path = sprintf("images/%s/", basename),
  fig.width = 6,
  fig.height = 4,
  out.width = "100%",
  fig.align = "center",
  fig.retina = 3,
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  cache = FALSE,
  cache.path = "cache/"
)
```

```{r titleslide, child="assets/titleslide.Rmd"}
```

```{r}
library(ISLR)
library(tidyverse)
library(broom)
library(recipes)
library(parsnip)
library(rsample)
library(yardstick)
library(purrr)
library(patchwork)
```

---
class: middle
background-image: url(https://upload.wikimedia.org/wikipedia/commons/9/98/Andromeda_Galaxy_%28with_h-alpha%29.jpg)
background-position: 50% 50% class: center, bottom, inverse

.white[Space is big. You just won't believe how vastly, hugely, mind-bogglingly big it is. I mean, you may think it's a long way down the road to the chemist's, but that's just peanuts to space.] 

**.white[Douglas Adams, Hitchhiker's Guide to the Galaxy]**

---
# High Dimensional Data 

<br>
<br>

Remember, our data can be denoted as:

$\mathcal{D} = \{(x_i, y_i)\}_{i = 1}^n, ~~~ \mbox{where}~ x_i = (x_{i1}, \dots, x_{ip})^{T}$
<br>
<br>

then

.info-box[.monash-orange2[.content[Dimension ]] .content[of the data is *p*, ] .monash-orange2[.content[ the number of variables.]]]




---
# Cubes and Spheres


Space expands exponentially with dimension:

<img src="images/lecture-04b/hypercube.png" style="width: 50%; align: center" />
<img src="images/lecture-04b/cube_sphere.png" style="width: 30%; align: center" />

As dimension increases the .monash-orange2[volume of a sphere] of same radius as cube side length becomes much .monash-orange2[smaller than the volume of the cube].

---
.flex[
.w-45[
# Multivariate data

Mostly though, we're working on problems where $n>>p$, and $p>1$. This would more commonly be referred to as .monash-orange2[multivariate data].
]
.w-10.white[

]
.w-45[
# Sub-spaces

<br>
<br>
Data will often be confined to a region of the space having lower .monash-orange2[intrinsic dimensionality]. The data lives in a low-dimensional subspace.

Analyse the data by, .monash-orange2[reducing dimensionality], to the subspace containing the data.
]
]
---
# Principal Component Analysis (PCA)

<br>

.info-box[Principal component analysis (PCA) produces a low-dimensional representation of a
dataset. It finds a sequence of linear combinations of the
variables that have .monash-orange2[maximal variance], and are .monash-orange2[mutually uncorrelated]. It is an unsupervised learning method. 
]

<br>

# Why use PCA?


- We may have too many predictors for a regression. Instead, we can use the first few principal components. 
- Understanding relationships between variables.
- Data visualisation. We can plot a small number of variables more easily than a large number of variables.

---

# First principal component

The first principal component of a set of variables $x_1, x_2, \dots, x_p$ is the linear combination


$$z_1 = \phi_{11}x_1 + \phi_{21} x_2 + \dots + \phi_{p1} x_p$$


that has the largest variance such that  

$$\displaystyle\sum_{j=1}^p \phi^2_{j1} = 1$$

The elements $\phi_{11},\dots,\phi_{p1}$ are the .monash-orange2[loadings] of the first principal component.


---
# Geometry

- The loading vector $\phi_1 = [\phi_{11},\dots,\phi_{p1}]^T$
defines direction in feature space along which data
vary most.
- If we project the $n$ data points ${x}_1,\dots,{x}_n$ onto this
direction, the projected values are the principal component
scores $z_{11},\dots,z_{n1}$.
- The second principal component is the linear combination $z_{i2} = \phi_{12}x_{i1} + \phi_{22}x_{i2} + \dots + \phi_{p2}x_{ip}$ that has maximal variance among all linear
combinations that are *uncorrelated* with $z_1$.
- Equivalent to constraining $\phi_2$ to be orthogonal (perpendicular) to $\phi_1$. And so on.
- There are at most $\min(n - 1, p)$ PCs.


---
# Example

<center>
<a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter6/6.14.pdf" target="_BLANK"> <img src="images/lecture-04b/6.14.png" style="width: 60%; align: center"/> </a>
</center>

.monash-green2[First PC]; .blue[second PC]

.font_smaller2[(Chapter6/6.14.pdf)]



---
# Example

<center>

<a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter6/6.15.pdf" target="_BLANK"> <img src="images/lecture-04b/6.15.png" style="width: 80%; align: center"/> </a>

</center>

If you think of the first few PCs like a linear model fit, and the others as the error, it is like regression, except that errors are orthogonal to model. 

.font_smaller2[(Chapter6/6.15.pdf)]



---
# Computation


PCA can be thought of as fitting an $n$-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. The new variables produced by principal components correspond to .monash-orange2[rotating] and .monash-orange2[scaling] the ellipse .monash-orange2[into a circle].

```{r eval=FALSE}
library(tidyverse)
f.norm.vec<-function(x) {
  x<-x/f.norm(x)
  x
}
f.norm<-function(x) { sqrt(sum(x^2)) }
f.gen.sphere<-function(n=100,p=5) {
  x<-matrix(rnorm(n*p),ncol=p)
  xnew<-t(apply(x,1,f.norm.vec))
  xnew
}
f.vc.ellipse <- function(vc, xm, n=500) {
  p<-ncol(vc)
  x<-f.gen.sphere(n,p)

  evc<-eigen(vc)
  vc2<-(evc$vectors)%*%diag(sqrt(evc$values))%*%t(evc$vectors)
  x<-x%*%vc2

  x + matrix(rep(xm, each=n),ncol=p)
}
df <- f.vc.ellipse(vc=matrix(c(1,0.8,0.8,2), ncol=2), xm=c(0,0), n=1000)
df <- as_tibble(df)
ev <- tibble(e1=c(0.4847685, -0.8746425), e2=c(0.8746425, 0.4847685),
                group=c("pc1","pc2"))
```

```{r eval=FALSE}
library(gganimate)
ggplot(df, aes(x=V1, y=V2)) + geom_point() + 
  xlim(c(-1.5, 1.5)) + ylim(c(-1.5, 1.5)) + 
  geom_abline(data=ev, aes(intercept=0, slope=e2/e1, colour=group), size=2) +
  scale_colour_brewer("", palette="Dark2") +
  theme(legend.position="none") +
  transition_states(group, 1, 1) + enter_grow() + exit_shrink() +
  labs(title = "{closest_state}")
```

```{r eval=FALSE}
df_pca <- prcomp(df, retx=T, center=T, scale=TRUE)
df_lines <- tibble(V1=c(seq(-1,1, 0.5), seq(-1,1, 0.5), c(-1.5, 1.5), c(0, 0)), 
                   V2=c(seq(-1,1, 0.5)*ev$e2[1]/ev$e1[1], 
                        seq(-1,1, 0.5)*ev$e2[2]/ev$e1[2], c(0, 0), c(-1.5, 1.5)),
                   pc=c(rep("pc1", 5), rep("pc2", 5), "pc1", "pc1", "pc2", "pc2"),
                   group=c(rep("raw data", 10), rep("principal components", 4)))
df_pc <- as_tibble(df_pca$x) %>%
  rename(V1=PC1, V2=PC2) %>%
  mutate(V1 = V1/df_pca$sdev[1], V2 = V2/df_pca$sdev[2])
ggplot(df_pc, aes(x=V1, y=V2)) + geom_point() + theme(aspect.ratio=1)
all <- bind_rows(df, df_pc) %>% 
  mutate(group=c(rep("raw data", 1000), rep("principal components", 1000))) %>%
  mutate(group = factor(group, levels=c("raw data", "principal components")))
p <- ggplot(all, aes(x=V1, y=V2)) + geom_point(size=1) + 
  geom_line(data=df_lines, aes(x=V1, y=V2, group=interaction(group, pc), colour=pc)) +
  scale_colour_brewer("", palette="Dark2") +
  xlim(c(-2, 2)) + ylim(c(-2, 2)) + 
  xlab("") + ylab("") +
  theme(aspect.ratio=1) +
  transition_states(group, 1, 1) + enter_grow() + exit_shrink() +
  labs(title = "{closest_state}")
anim_save(filename = "images/pc-demo.gif", animation = p, 
          start_pause = 15, width = 480, height = 480, res = 150)
```

<center>
<img src="images/lecture-04b/pc-demo.gif" style="width: 40%; align: center" />
</center>


---
# Computation

Suppose we have a $n\times p$ data set $X = [x_{ij}]$. 

1. Centre each of the variables to have mean zero (i.e., the
column means of ${X}$ are zero).
2. Let $z_{i1} = \phi_{11}x_{i1} + \phi_{21} x_{i2} + \dots + \phi_{p1} x_{ip}$
3. Compute sample variance of $z_{i1}$ is $\displaystyle\frac1n\sum_{i=1}^n z_{i1}^2$.
4. Estimate $\phi_{j1}$

$$\mathop{\text{maximize}}_{\phi_{11},\dots,\phi_{p1}} \frac{1}{n}\sum_{i=1}^n 
\left(\sum_{j=1}^p \phi_{j1}x_{ij}\right)^{\!\!\!2} \text{ subject to }
\sum_{j=1}^p \phi^2_{j1} = 1$$

Repeat optimisation to estimate $\phi_{jk}$, with additional constraint that $\sum_{{j=1}, k<k'}^p \phi_{jk}\phi_{jk'} = 0$ (next vector is orthogonal to previous eigenvector).

---
.flex[
.w-45[
# Eigen-decomposition 
1. Compute the covariance matrix (after centering the columns of ${X}$)
$$S = {X}^T{X}$$
2. Find eigenvalues (diagonal elements of $D$) and eigenvectors ( $V$ ):
$${S}={V}{D}{V}^T$$
where columns of ${V}$ are orthonormal (i.e., ${V}^T{V}={I}$)
]
.w-10.white[ white space]
.w-45[

# Singular Value Decomposition

$$X = U\Lambda V^T$$


- $X$ is an $n\times p$ matrix
- $U$ is $n \times r$ matrix with orthonormal columns ( $U^TU=I$ )
- $\Lambda$ is $r \times r$ diagonal matrix with non-negative elements. (Square root of the eigenvalues.)
- $V$ is $p \times r$ matrix with orthonormal columns (These are the eigenvectors, and $V^TV=I$ ).

It is always possible to uniquely decompose a matrix in this way.
]
]

---
# Total variance

.monash-orange2[Total variance] in data (assuming variables centered at 0):

$$\text{TV} = \sum_{j=1}^p \text{Var}(x_j) = \sum_{j=1}^p \frac{1}{n}\sum_{i=1}^n x_{ij}^2$$

.info-box[**If variables are standardised, TV=number of variables!**]



.monash-orange2[Variance explained] by *m*'th PC: $V_m = \text{Var}(z_m) = \frac{1}{n}\sum_{i=1}^n z_{im}^2$

$$\text{TV} = \sum_{m=1}^M V_m \text{  where }M=\min(n-1,p).$$



---
# How to choose $k$?

<br>

PCA is a useful dimension reduction technique for large datasets, but deciding on how many dimensions to keep isn't often clear. `r emo::ji("thinking")`

<center>
.think-box[How do we know how many principal components to choose?]
</center>
---

# How to choose $k$?

<center>
.info-box[.monash-orange2[Proportion] of variance explained:
$$\text{PVE}_m = \frac{V_m}{TV}$$
]
</center>

Choosing the number of PCs that adequately summarises the variation in $X$, is achieved by examining the cumulative proportion of variance explained. 

<center>
.info-box[
.monash-orange2[Cumulative proportion] of variance explained:

$$\text{CPVE}_k = \sum_{m=1}^k\frac{V_m}{TV}$$
]
</center>

---
class: split-two
layout: false

.column[.pad50px[

# How to choose $k$?

<br>

.info-box[.monash-orange2[Scree plot: ].content[Plot of variance explained by each component vs number of component.]]

]]
.column[.content.vmiddle.center[

```{r}
library(tidyverse)
df <- tibble(npcs=1:10, evl=c(3.5,2.7,2.2,0.5,0.3,0.3,0.2,0.1,0.1,0.1))
p <- ggplot(df, aes(x=npcs, y=evl)) + geom_line() + 
  xlab("Number of PCs") + ylab("Eigenvalue") +
  scale_x_continuous(breaks=seq(0,10,1)) +
  theme_minimal(base_size=18)
p
```

]]

---


class: split-two
layout: false

.column[.pad50px[

# How to choose $k$?

<br>

.info-box[.monash-orange2[Scree plot: ].content[Plot of variance explained by each component vs number of component.]]

]]
.column[.content.vmiddle.center[

```{r}
p + geom_vline(xintercept=4, colour="orange", size=3, alpha=0.7) +
  annotate("text", x=4.5, y=3, label="Choose k=4", colour="orange", hjust = 0, size=8)
```



]]


---
# Example - track records

The data on national track records for women (as at 1984). 

```{r}
track <- read_csv(here::here("data/womens_track.csv"))
glimpse(track)
```

.font_smaller2[*Source*: Johnson and Wichern, Applied multivariate analysis]


---
.flex[
# Explore the data

```{r out.width="60%", fig.width=8, fig.height=8}
library(GGally)
ggscatmat(track[,1:7])
```

]

---
# Compute PCA

```{r}
options(digits=2)
```

```{r echo=TRUE}
track_pca <- prcomp(track[,1:7], center=TRUE, scale=TRUE)
track_pca
```


---
# Assess

Summary of the principal components: 

```{r}
library(kableExtra)
library(knitr)
track_pca_smry <- tibble(evl=track_pca$sdev^2) %>%
  mutate(p = evl/sum(evl), cum_p = cumsum(evl/sum(evl))) %>% t() 
colnames(track_pca_smry) <- colnames(track_pca$rotation)
rownames(track_pca_smry) <- c("Variance", "Proportion", "Cum. prop")
kable(track_pca_smry, digits=2, align="r") %>% 
  kable_styling(full_width = T) %>%
  row_spec(0, color="white", background = "#7570b3") %>%
  column_spec(1, width = "2.5em", color="white", background = "#7570b3") %>%
  column_spec(1:8, width = "2.5em") %>%
  row_spec(3, color="white", background = "#CA6627")
```

Increase in variance explained large until $k=3$ PCs, and then tapers off. A choice of .monash-orange2[3 PCs] would explain 97% of the total variance. 


---
class: split-two
layout: false

.column[.pad50px[

# Assess

<br>

.monash-green2[Scree plot: Where is the elbow?]

<br>

At $k=2$, thus the scree plot suggests 2 PCs would be sufficient to explain the variability.



]]
.column[.content.vmiddle.center[

```{r}
track_pca_var <- tibble(n=1:length(track_pca$sdev), evl=track_pca$sdev^2)
ggplot(track_pca_var, aes(x=n, y=evl)) + geom_line() +
  xlab("Number of PCs") + ylab("Eigenvalue") +
  theme_minimal(base_size = 18)
```


]]


---


class: split-two
layout: false

.column[.pad50px[

# Assess

<br>

.info-box[.monash-orange2[Visualise model using a biplot]: Plot the principal component scores, and also the contribution of the original variables to the principal component.
]



]]
.column[.content.vmiddle.center[

```{r}
library(ggrepel)
track_pca_pcs <- as_tibble(track_pca$x[,1:2]) %>%
  mutate(cnt=track$country)
track_pca_evc <- as_tibble(track_pca$rotation[,1:2]) %>% 
  mutate(origin=rep(0, 7), variable=colnames(track)[1:7],
         varname=rownames(track_pca$rotation)) %>%
  mutate(PC1s = PC1*(track_pca_var$evl[1]*2.5), 
         PC2s = PC2*(track_pca_var$evl[2]*2.5))
pca_p <- ggplot() + 
  geom_segment(data=track_pca_evc, aes(x=origin, xend=PC1s, y=origin, yend=PC2s), colour="orange") +
  geom_text_repel(data=track_pca_evc, aes(x=PC1s, y=PC2s, label=variable, nudge_y=sign(PC2)*0.1), colour="orange", nudge_x=0.1) +
  geom_point(data=track_pca_pcs, aes(x=PC1, y=PC2)) +
  geom_text(data=filter(track_pca_pcs, abs(PC1)>5), aes(x=PC1, y=PC2, label=cnt), nudge_y=0.1, nudge_x=-0.1) +
  geom_text(data=filter(track_pca_pcs, abs(PC2)>1.5), aes(x=PC1, y=PC2, label=cnt), nudge_y=0.1, nudge_x=-0.1) +
  xlab("PC1") + ylab("PC2") +
  theme(aspect.ratio=1)
pca_p
```

]]

---

# Significance of loadings

Bootstrap can be used to assess whether the coefficients of a PC are significantly different from 0. The 95% bootstrap confidence intervals can be computed by:

1. Generating B bootstrap samples of the data
2. Compute PCA, record the loadings
3. Re-orient the loadings, by choosing one variable with large coefficient to be the direction base
4. If B=1000, 25th and 975th sorted values yields the lower and upper bounds for confidence interval for each PC. 

```{r fig.height=1, fig.width=6, out.width="50%"}
library(ggpubr)
library(ggthemes)
l <- tibble(x=c(-1, 1), y=c(0,0))
p1 <-  ggplot(l) + geom_line(aes(x=x, y=y), arrow=arrow(ends="last")) + theme_map()
p2  <- ggplot(l) + geom_line(aes(x=x, y=y), arrow=arrow(ends="first")) + theme_map()
ggarrange(p1, p2, ncol=1)
```

---


```{r out.width="60%", fig.width=6, fig.height=4}
library(boot)
compute_PC1 <- function(data, index) {
  pc1 <- prcomp(data[index,], center=TRUE, scale=TRUE)$rotation[,1]
  # Coordinate signs
  if (sign(pc1[1]) < 0) 
    pc1 <- -pc1 
  return(pc1)
}
# Make sure sign of first PC element is positive
PC1_boot <- boot(data=track[,1:7], compute_PC1, R=1000)
colnames(PC1_boot$t) <- colnames(track[,1:7])
PC1_boot_ci <- as_tibble(PC1_boot$t) %>%
  gather(var, coef) %>% 
  mutate(var = factor(var, levels=c("m100", "m200", "m400", "m800", "m1500", "m3000", "marathon"))) %>%
  group_by(var) %>%
  summarise(q2.5 = quantile(coef, 0.025), 
            q5 = median(coef),
            q97.5 = quantile(coef, 0.975)) %>%
  mutate(t0 = PC1_boot$t0) 
  
ggplot(PC1_boot_ci, aes(x=var, y=t0)) + 
  geom_hline(yintercept=1/sqrt(7), linetype=2, colour="red") +
  geom_point() +
  geom_errorbar(aes(ymin=q2.5, ymax=q97.5), width=0.1) +
  geom_hline(yintercept=0, size=3, colour="white") +
  xlab("") + ylab("coefficient") 
``` 


All of the coefficients on PC1 are significantly different from 0, and positive, approximately equal, .monash-orange2[not significantly different from being equal].


---

# Loadings for PC2

```{r out.width="60%", fig.width=6, fig.height=4}
compute_PC2 <- function(data, index) {
  pc2 <- prcomp(data[index,], center=TRUE, scale=TRUE)$rotation[,2]
  # Coordinate signs
  if (sign(pc2[1]) < 0) 
    pc2 <- -pc2 
  return(pc2)
}
# Make sure sign of first PC element is positive
PC2_boot <- boot(data=track[,1:7], compute_PC2, R=1000)
colnames(PC2_boot$t) <- colnames(track[,1:7])
PC2_boot_ci <- as_tibble(PC2_boot$t) %>%
  gather(var, coef) %>% 
  mutate(var = factor(var, levels=c("m100", "m200", "m400", "m800", "m1500", "m3000", "marathon"))) %>%
  group_by(var) %>%
  summarise(q2.5 = quantile(coef, 0.025), 
            q5 = median(coef),
            q97.5 = quantile(coef, 0.975)) %>%
  mutate(t0 = PC2_boot$t0) 
ggplot(PC2_boot_ci, aes(x=var, y=t0)) + 
  geom_hline(yintercept=0, size=3, colour="white") +
  geom_hline(yintercept=c(1/sqrt(7), -1/sqrt(7)), linetype=2, colour="red") +
  geom_point() +
  geom_errorbar(aes(ymin=q2.5, ymax=q97.5), width=0.1) +
  xlab("") + ylab("coefficient") 
``` 

On PC2 m100 and m200 contrast m1500 and m3000 (and possibly marathon). These are significantly different from 0. 

---

# Loadings for PC3

```{r out.width="60%", fig.width=6, fig.height=4}
compute_PC3 <- function(data, index) {
  pc3 <- prcomp(data[index,], center=TRUE, scale=TRUE)$rotation[,3]
  # Coordinate signs
  if (sign(pc3[3]) < 0) 
    pc3 <- -pc3 
  return(pc3)
}
# Make sure sign of first PC element is positive
PC3_boot <- boot(data=track[,1:7], compute_PC3, R=1000)
colnames(PC3_boot$t) <- colnames(track[,1:7])
PC3_boot_ci <- as_tibble(PC3_boot$t) %>%
  gather(var, coef) %>% 
  mutate(var = factor(var, levels=c("m100", "m200", "m400", "m800", "m1500", "m3000", "marathon"))) %>%
  group_by(var) %>%
  summarise(q2.5 = quantile(coef, 0.025), 
            q5 = median(coef),
            q97.5 = quantile(coef, 0.975)) %>%
  mutate(t0 = PC3_boot$t0) 
ggplot(PC3_boot_ci, aes(x=var, y=t0)) + 
  geom_hline(yintercept=0, size=3, colour="white") +
  geom_hline(yintercept=c(1/sqrt(7), -1/sqrt(7)), linetype=2, colour="red") +
  geom_point() +
  geom_errorbar(aes(ymin=q2.5, ymax=q97.5), width=0.1) +
  xlab("") + ylab("coefficient") 
``` 

On PC3 m400 and m800 (and possibly marathon) are significantly different from 0. 

---
# Interpretation

- PC1 measures overall magnitude, the strength of the athletics program. High positive values indicate .monash-orange2[poor] programs with generally slow times across events. 
- PC2 measures the .monash-orange2[contrast] in the program between .monash-orange2[short and long distance] events. Some countries have relatively stronger long distance atheletes, while others have relatively stronger short distance athletes.
- There are several .monash-orange2[outliers] visible in this plot, `wsamoa`, `cookis`, `dpkorea`. PCA, because it is computed using the variance in the data, can be affected by outliers. It may be better to remove these countries, and re-run the PCA. 
- PC3, may or may not be useful to keep. The interpretation would that this variable summarises countries with different middle distance performance.

---
class: transition

# Other techniques

---
# Projection pursuit (PP) generalises PCA

PCA:
$$\mathop{\text{maximize}}_{\phi_{11},\dots,\phi_{p1}} \frac{1}{n}\sum_{i=1}^n 
\left(\sum_{j=1}^p \phi_{j1}x_{ij}\right)^{\!\!\!2} \text{ subject to }
\sum_{j=1}^p \phi^2_{j1} = 1$$

PP:

$$\mathop{\text{maximize}}_{\phi_{11},\dots,\phi_{p1}} ~~f\left(\sum_{j=1}^p \phi_{j1}x_{ij}\right) \text{ subject to }
\sum_{j=1}^p \phi^2_{j1} = 1$$


---
# MDS generalises PCA

.tip[.orange[Multidimensional scaling (MDS)] finds a low-dimensional layout of points that minimises the difference between distances computed in the *p*-dimensional space, and those computed in the low-dimensional space. ]

$$\mbox{Stress}_D(x_1, ..., x_N) = \left(\sum_{i, j=1; i\neq j}^N (d_{ij} - d_k(i,j))^2\right)^{1/2}$$

where $D$ is an $N\times N$ matrix of distances $(d_{ij})$ between all pairs of points, and $d_k(i,j)$ is the distance between the points in the low-dimensional space.




---
class: split-two


.column[.pad50px[
# MDS can do nonlinear dimension reduction

- Classical MDS similar results to PCA
- Metric MDS incorporates power transformations on the distances, $d_{ij}^r$.
- Non-metric MDS incorporates a monotonic transformation of the distances, e.g. rank

```{r echo=TRUE}
track <- read_csv(here::here("data/womens_track.csv"))
track_mds <- 
  cmdscale(dist(track[,1:7])) %>% #<<
  as_tibble() %>%
  mutate(country = track$country)
```
]]

.column[.pad50px[
```{r fig.width = 4, fig.height = 4, out.width="60%"}
pca_p + ggtitle("PCA biplot")

mds_p <- ggplot() + 
  geom_point(data=track_mds, aes(x=V1, y=V2)) +
  geom_text_repel(data=filter(track_mds, V1>50), aes(x=V1, y=V2, label=country)) +
  geom_text_repel(data=filter(track_mds, abs(V2)>4), aes(x=V1, y=V2, label=country)) +
  xlab("MDS1") + ylab("MDS2") +
  theme(aspect.ratio=1) + ggtitle("Classical MDS")
mds_p
```
]]

---
# Challenge

For each of these distance matrices, find a layout in 1 or 2D that accurately reflects the full distances.

```{r}
d1 <- tibble(name = c("A", "B", "C"), A = c(0.1, 3.2, 3.9), B=c(3.2, -0.1, 5.1), C=c(3.9, 5.1, 0))
d1
d2 <- tibble(name = c("A", "B", "C", "D"), A = c(0.1, 0.9, 2.1, 3.0), B=c(0.9, 0.0, 1.1, 1.9), C=c(2.1,1.1,0.1,1.1), D=c(3.0,1.9,1.1,-0.1))
d2
```

---
# Non-linear dimension reduction

<br>

- .orange[T-distributed Stochastic Neighbor Embedding (t-SNE)]: similar to MDS, except emphasis is placed on grouping observations into clusters. Observations within a cluster are placed close in the low-dimensional representation, but clusters themselves are placed far apart.
- .orange[Local linear embedding (LLE)]: Finds nearest neighbours of points, defines interpoint distances relative to neighbours, and preserves these proximities in the low-dimensional mapping. Optimisation is used to solve an eigen-decomposition of the knn distance construction.
- .orange[Self-organising maps (SOM)]: First clusters the observations into $k \times k$ groups. Uses the mean of each group laid out in a constrained 2D grid to create a 2D projection.


---

```{r endslide, child="assets/endslide.Rmd"}
```
