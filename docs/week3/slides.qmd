---
title: "ETC3250/5250 Introduction to Machine Learning"
title-slide-attributes: 
  data-background-image: "../images/bg.png"
  data-background-size: 100%
subtitle: "Week 3: Re-sampling and regularisation"
author: 
 - name: "Professor Di Cook"
   email: "etc3250.clayton-x@monash.edu"
institute: "Department of Econometrics and Business Statistics"
footer: "ETC3250/5250 Lecture 3 | [iml.numbat.space](iml.numbat.space)"
format:
  revealjs:
    multiplex: false
    slide-number: c/t
    slide-tone: false
    theme: "../assets/monash.scss"
    width: 1600
    height: 900
    margin: 0.05
    embed-resources: true
---

```{r, include = FALSE}
source("../setup.R")
```

## Overview

We will cover:

* Common re-sampling methods: **bootstrap**, **cross-validation**, **permutation**
* **Cross-validation** for parameter tuning
* **Bootstrapping** for understanding variance of parameter estimates
* **Permutation** to understand significance of associations between variables, and variable importance
* What can go wrong in high-d, and how to adjust using **regularisation** methods

## Model development and choice 

<br>

<center>
<img src="../images/newdata.jpg" style="width: 60%; align: center"/> </a>
</center>

## How do you get new data? {.transition-slide .center}

## Common re-sampling methods

:::: {.columns}
::: {.column}
::: {.info}
- [**Cross-validation**]{.monash-blue2}: Splitting the data into multiple samples.
- [**Bootstrap**]{.monash-blue2}: Sampling with replacement
- [**Permutation**]{.monash-blue2}: Re-order the values of one or more variables
:::

::: {.incremental}
- [Cross-validation]{.monash-orange2}: This is used to gain some understanding of the [variance]{.monash-orange2} (as in *bias-variance trade-off* ) of models, and how parameter or algorithm choices affect the performance of the model on future samples.
:::

:::
::: {.column}
::: {.incremental}

- [Bootstrap]{.monash-pink2}: Compute [confidence intervals]{.monash-pink2} for model parameters, or the model fit statistics. can be used similarly to cross-validation samples but avoids the complication of smaller sample size that may affect interpretation of cross-validation samples.
- [Permutation]{.monash-green2}: Used to assess [significance of relationships]{.monash-green2}, especially to assess the importance of individual variables or combinations of variables for a fitted model.


:::
:::
::::

## Cross-validation

- [Training/test split]{.monash-blue2}: make one split of your data, keeping one purely for assessing future performance.

After making that split, we would use [these methods on the training sample]{.monash-orange2}:

- [Leave-one-out]{.monash-orange2}: make $n$ splits, fitting multiple models and using left-out observation for assessing variability. 
- [$k$-fold]{.monash-orange2}: break data into $k$ subsets, fitting multiple models with one group left out each time.

## Training/test split [(1/3)]{.smaller}

<a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter5/5.1.pdf" target="_BLANK"> <img src="../images/5.1.png" style="width: 100%; align: center"/> </a>

[A set of $n$ observations are randomly split into a training set (blue, containing observations 7, 22, 13, ...) and a test set (yellow, all other observations not in training set).]{.smaller}

- Need to [stratify the sampling]{.monash-orange2} to ensure training and test groups are appropriately [balanced]{.monash-orange2}.
- Only one split of data made, may have a lucky or unlucky split, accurately estimating test error relies on the one sample. 
 

<br><br>
[(Chapter5/5.1.pdf)]{.smallest}

## Training/test split [(2/3)]{.smaller}

:::: {.columns}
::: {.column}

[With tidymodels, the function `initial_split()` creates the indexes of observations to be allocated into training or test samples. To generate these samples use `training()` and `test()` functions.]{.smaller}

```{r}
#| label: balanced-data1
d_bal <- tibble(y=c(rep("A", 6), rep("B", 6)),
                x=c(runif(12)))
d_bal$y
set.seed(130)
d_bal_split <- initial_split(d_bal, prop = 0.70)
training(d_bal_split)$y
testing(d_bal_split)$y
```

::: {.fragment}
How do you ensure that you get [0.70 in each class]{.monash-orange2}?
:::

:::
::: {.column}

::: {.fragment}

[Stratify the sampling]{.monash-orange2}

```{r}
#| label: balanced-data2
d_bal$y
set.seed(1225)
d_bal_split <- initial_split(d_bal, 
                             prop = 0.70, 
                             strata=y)
training(d_bal_split)$y
testing(d_bal_split)$y
```

Now the test set has 2 A's and 2 B'2. [This is best practice!]{.monash-orange2}

:::

:::
::::

## Training/test split [(3/3)]{.smaller}

:::: {.columns}
::: {.column}

Not stratifying can cause major problems with unbalanced samples.

```{r}
#| label: unbalanced-data
d_unb <- tibble(y=c(rep("A", 2), rep("B", 10)),
                x=c(runif(12)))
d_unb$y
set.seed(132)
d_unb_split <- initial_split(d_unb, prop = 0.70)
training(d_unb_split)$y
testing(d_unb_split)$y
```

[The test set is missing one entire class!]{.monash-red2}
:::

::: {.column}

::: {.fragment}

::: {.info}
Always [stratify splitting]{.monash-orange2} by sub-groups, especially response variable classes, and possibly other variables too.
:::

```{r}
#| label: unbalanced-split
d_unb_strata <- initial_split(d_unb, 
                              prop = 0.70, 
                              strata=y)
training(d_unb_strata)$y
testing(d_unb_strata)$y
```

[Now there is an A in the test set!]{.monash-green2}
:::

:::
::::

## Checking the training/test split: response

:::: {.columns}
::: {.column width=30%}

<center>
[GOOD]{.monash-green2}
</center>

```{r}
#| echo: false
#| label: penguins-good-split1
#| fig-width: 3
#| fig-height: 6
#| out-width: 70%
library(palmerpenguins)
p_tidy <- penguins |>
  select(species, bill_length_mm:body_mass_g) |>
  rename(bl=bill_length_mm,
         bd=bill_depth_mm,
         fl=flipper_length_mm,
         bm=body_mass_g) |>
  filter(!is.na(bl)) |>
  arrange(species)
set.seed(224)
p_split <- initial_split(p_tidy, 
                         prop = 0.7,
                         strata = species)
p_tr <- training(p_split) |> 
  mutate(set = "train")
p_ts <- testing(p_split) |> 
  mutate(set = "test")
p_both <- bind_rows(p_tr, p_ts) |>
  mutate(set = factor(set, levels=c("train", "test")))
ggplot(p_both, aes(x=species, fill=species)) +
  geom_bar() +
  facet_wrap(~set, ncol=1, scales="free_x") +
  scale_fill_discrete_divergingx(palette="Zissou 1") +
  xlab("") +
  theme(legend.position="none") + coord_flip()
```
:::
::: {.column width=30%}

<center>
[BAD]{.monash-red2}
</center>

```{r}
#| echo: false
#| label: penguins-bad-split1
#| fig-width: 3
#| fig-height: 6
#| out-width: 70%
p_bad <- p_tidy |>
  mutate(set = factor(ifelse(species == "Adelie", "train", "test"),
                      levels=c("train", "test")))
ggplot(p_bad, aes(x=species, fill=species)) +
  geom_bar() +
  facet_wrap(~set, ncol=1) +
  scale_fill_discrete_divergingx(palette="Zissou 1") +
  xlab("") +
  theme(legend.position="none") + coord_flip()
```
:::

::: {.column width=30%}

<br><br><br>
You need to have similar numbers of each class in both sets.

:::
::::

## Checking the training/test split: predictors

:::: {.columns}
::: {.column width="33%"}

<center>
[GOOD]{.monash-green2}
</center>

```{r}
#| echo: false
#| label: penguins-good-split2
#| eval: false
render_gif(p_both[,2:5], 
           grand_tour(),
           display_xy(col=p_both$species,
             pch=p_both$set, shapeset=c(4, 16), rescale=TRUE),
           rescale=TRUE,
           gif_file = "../gifs/p_split_good.gif",
           apf = 1/30,
           frames = 500,
           width = 400, 
           height = 400)
```

![](../gifs/p_split_good.gif){width=500}

[Training and test sets similar.]{.smaller}

:::
::: {.column width="33%"}

<center>
[Looks good]{.monash-pink2 .smaller}
</center>

```{r}
#| echo: false
#| label: penguins-bad-split2
#| fig-width: 6
#| fig-height: 3
#| out-width: 80%
p_bad2 <- p_tidy |>
  mutate(set = factor(ifelse((species == "Adelie" & bd > 17.5 & fl > 183) | (species == "Chinstrap" & bd > 17.8 & fl > 187) |
        (species == "Gentoo" & bd > 14.2 & fl > 211), "train", "test"),
                      levels=c("train", "test")))
ggplot(p_bad2, aes(x=species, fill=species)) +
  geom_bar() +
  facet_wrap(~set, ncol=2, scales="free") +
  scale_fill_discrete_divergingx(palette="Zissou 1") +
  xlab("") +
  theme(legend.position="none") + coord_flip()
```

[Training and test sets have similar proportions of each class.]{.smaller}
:::
::: {.column width="33%"}

<center>
[But BAD]{.monash-red2 .smaller}
</center>

```{r}
#| echo: false
#| eval: false
render_gif(p_bad2[,2:5], 
           grand_tour(),
           display_xy(col=p_bad2$species,
             pch=p_bad2$set, shapeset=c(4, 16), rescale=TRUE),
           rescale=TRUE,
           gif_file = "../gifs/p_split_bad.gif",
           apf = 1/30,
           frames = 500,
           width = 400, 
           height = 400)
```

![](../gifs/p_split_bad.gif){width=500}

[Test set has smaller penguins on at least two of the variables.]{.smaller}
:::
::::

##

```{r}
#| echo: false
# Function that will compute the test MSE for 
# four different models
library(ISLR)
compute_mse <- function(d) {
  spl <- initial_split(d, 2/3)
  d_train <- training(spl)
  d_test <- testing(spl)

  auto_rec1 <-    
    recipe(mpg ~ horsepower, 
         data = d_train) %>% 
    step_poly(horsepower, degree = 1) 
  auto_rec2 <-    
    recipe(mpg ~ horsepower, 
         data = d_train) %>% 
    step_poly(horsepower, degree = 2) 
  auto_rec3 <-    
    recipe(mpg ~ horsepower, 
         data = d_train) %>% 
    step_poly(horsepower, degree = 3) 
  auto_rec4 <-    
    recipe(mpg ~ horsepower, 
         data = d_train) %>% 
    step_poly(horsepower, degree = 4) 
  
  lm_mod <- 
  linear_reg() %>% 
  set_engine("lm")
  
  auto_wf1 <- workflow() %>%
    add_model(lm_mod) %>% 
    add_recipe(auto_rec1)
  auto_wf2 <- workflow() %>%
    add_model(lm_mod) %>% 
    add_recipe(auto_rec2)
  auto_wf3 <- workflow() %>%
    add_model(lm_mod) %>% 
    add_recipe(auto_rec3)
  auto_wf4 <- workflow() %>%
    add_model(lm_mod) %>% 
    add_recipe(auto_rec4)

  sample_split <- tibble(id = 1:nrow(d)) %>%
    mutate(id_in = ifelse(1:nrow(d) %in% spl$in_id, 
                            "y", "n"))
  
  fit1 <- fit(auto_wf1, data=d_train)
  fit2 <- fit(auto_wf2, data=d_train)
  fit3 <- fit(auto_wf3, data=d_train)
  fit4 <- fit(auto_wf4, data=d_train)

  auto_test_pred1 <- augment(fit1,
                             d_test)
  auto_test_pred2 <- augment(fit2,
                             d_test)
  auto_test_pred3 <- augment(fit3,
                             d_test)
  auto_test_pred4 <- augment(fit4,
                             d_test)
  auto_mse <- tibble(poly = c(1,2,3,4)) %>%
    mutate(mse = c(
      rmse(auto_test_pred1, truth = mpg, 
        estimate = .pred)$.estimate^2, 
      rmse(auto_test_pred2, truth = mpg, 
        estimate = .pred)$.estimate^2,
      rmse(auto_test_pred3, truth = mpg, 
        estimate = .pred)$.estimate^2,
      rmse(auto_test_pred4, truth = mpg, 
        estimate = .pred)$.estimate^2))

  return(list(auto_mse, sample_split))
}
```

```{r}
#| echo: false
# Compute test MSE for four training/test splits
set.seed(1111)
run1 <- compute_mse(Auto)
run2 <- compute_mse(Auto)
run3 <- compute_mse(Auto)
run4 <- compute_mse(Auto)
auto_runs <- run1[[1]] %>%
  rename(mse1 = mse) %>%
  select(mse1) %>%
  bind_cols(run2[[1]]) %>%
  rename(mse2 = mse) %>%
  select(mse1, mse2) %>%
  bind_cols(run3[[1]]) %>%
  rename(mse3 = mse) %>%
  select(mse1, mse2, mse3) %>%
  bind_cols(run4[[1]]) %>%
  rename(mse4 = mse) %>%
  select(poly, mse1, mse2, mse3, mse4) 

auto_samples <- run1[[2]] %>%
  rename(id_in1 = id_in) %>%
  select(id_in1) %>%
  bind_cols(run2[[2]]) %>%
  rename(id_in2 = id_in) %>%
  select(id_in1, id_in2) %>%
  bind_cols(run3[[2]]) %>%
  rename(id_in3 = id_in) %>%
  select(id_in1, id_in2, id_in3) %>%
  bind_cols(run4[[2]]) %>%
  rename(id_in4 = id_in) %>%
  select(id, id_in1, id_in2, id_in3, id_in4) 
# Make into a heatmap
auto_samples <- auto_samples %>%
  mutate(col = (id-1)%/%20 + 1, 
         row = (id-1)%%20 + 1)
```


## Significance of loadings

Bootstrap can be used to assess whether the coefficients of a PC are significantly different from 0. The 95% bootstrap confidence intervals can be computed by:

1. Generating B bootstrap samples of the data
2. Compute PCA, record the loadings
3. Re-orient the loadings, by choosing one variable with large coefficient to be the direction base
4. If B=1000, 25th and 975th sorted values yields the lower and upper bounds for confidence interval for each PC. 

```{r}
#| fig-height: 1 
#| fig-width: 6 
#| out-width: 50%
#| echo: false
library(ggpubr)
library(ggthemes)
l <- tibble(x=c(-1, 1), y=c(0,0))
p1 <-  ggplot(l) + geom_line(aes(x=x, y=y), arrow=arrow(ends="last")) + theme_map()
p2  <- ggplot(l) + geom_line(aes(x=x, y=y), arrow=arrow(ends="first")) + theme_map()
ggarrange(p1, p2, ncol=1)
```

## Loadings for PC 1

```{r}
#| echo: false
#| out-width: 60%
#| fig-width: 6
#| fig-height: 4
track <- read_csv(here::here("data/womens_track.csv"))
library(boot)
compute_PC1 <- function(data, index) {
  pc1 <- prcomp(data[index,], center=TRUE, scale=TRUE)$rotation[,1]
  # Coordinate signs
  if (sign(pc1[1]) < 0) 
    pc1 <- -pc1 
  return(pc1)
}
# Make sure sign of first PC element is positive
PC1_boot <- boot(data=track[,1:7], compute_PC1, R=1000)
colnames(PC1_boot$t) <- colnames(track[,1:7])
PC1_boot_ci <- as_tibble(PC1_boot$t) %>%
  gather(var, coef) %>% 
  mutate(var = factor(var, levels=c("m100", "m200", "m400", "m800", "m1500", "m3000", "marathon"))) %>%
  group_by(var) %>%
  summarise(q2.5 = quantile(coef, 0.025), 
            q5 = median(coef),
            q97.5 = quantile(coef, 0.975)) %>%
  mutate(t0 = PC1_boot$t0) 
  
ggplot(PC1_boot_ci, aes(x=var, y=t0)) + 
  geom_hline(yintercept=1/sqrt(7), linetype=2, colour="red") +
  geom_point() +
  geom_errorbar(aes(ymin=q2.5, ymax=q97.5), width=0.1) +
  geom_hline(yintercept=0, size=3, colour="white") +
  xlab("") + ylab("coefficient") 
``` 


All of the coefficients on PC1 are significantly different from 0, and positive, approximately equal, [not significantly different from being equal]{.monash-orange2}.


## Loadings for PC 2

```{r out.width="60%", fig.width=6, fig.height=4}
compute_PC2 <- function(data, index) {
  pc2 <- prcomp(data[index,], center=TRUE, scale=TRUE)$rotation[,2]
  # Coordinate signs
  if (sign(pc2[1]) < 0) 
    pc2 <- -pc2 
  return(pc2)
}
# Make sure sign of first PC element is positive
PC2_boot <- boot(data=track[,1:7], compute_PC2, R=1000)
colnames(PC2_boot$t) <- colnames(track[,1:7])
PC2_boot_ci <- as_tibble(PC2_boot$t) %>%
  gather(var, coef) %>% 
  mutate(var = factor(var, levels=c("m100", "m200", "m400", "m800", "m1500", "m3000", "marathon"))) %>%
  group_by(var) %>%
  summarise(q2.5 = quantile(coef, 0.025), 
            q5 = median(coef),
            q97.5 = quantile(coef, 0.975)) %>%
  mutate(t0 = PC2_boot$t0) 
ggplot(PC2_boot_ci, aes(x=var, y=t0)) + 
  geom_hline(yintercept=0, size=3, colour="white") +
  geom_hline(yintercept=c(1/sqrt(7), -1/sqrt(7)), linetype=2, colour="red") +
  geom_point() +
  geom_errorbar(aes(ymin=q2.5, ymax=q97.5), width=0.1) +
  xlab("") + ylab("coefficient") 
``` 

On PC2 m100 and m200 contrast m1500 and m3000 (and possibly marathon). These are significantly different from 0. 

## Loadings for PC 3

```{r out.width="60%", fig.width=6, fig.height=4}
compute_PC3 <- function(data, index) {
  pc3 <- prcomp(data[index,], center=TRUE, scale=TRUE)$rotation[,3]
  # Coordinate signs
  if (sign(pc3[3]) < 0) 
    pc3 <- -pc3 
  return(pc3)
}
# Make sure sign of first PC element is positive
PC3_boot <- boot(data=track[,1:7], compute_PC3, R=1000)
colnames(PC3_boot$t) <- colnames(track[,1:7])
PC3_boot_ci <- as_tibble(PC3_boot$t) %>%
  gather(var, coef) %>% 
  mutate(var = factor(var, levels=c("m100", "m200", "m400", "m800", "m1500", "m3000", "marathon"))) %>%
  group_by(var) %>%
  summarise(q2.5 = quantile(coef, 0.025), 
            q5 = median(coef),
            q97.5 = quantile(coef, 0.975)) %>%
  mutate(t0 = PC3_boot$t0) 
ggplot(PC3_boot_ci, aes(x=var, y=t0)) + 
  geom_hline(yintercept=0, size=3, colour="white") +
  geom_hline(yintercept=c(1/sqrt(7), -1/sqrt(7)), linetype=2, colour="red") +
  geom_point() +
  geom_errorbar(aes(ymin=q2.5, ymax=q97.5), width=0.1) +
  xlab("") + ylab("coefficient") 
``` 

On PC3 m400 and m800 (and possibly marathon) are significantly different from 0. 



## Other techniques {.transition-slide .center}

## Projection pursuit (PP) generalises PCA

PCA:
$$\mathop{\text{maximize}}_{\phi_{11},\dots,\phi_{p1}} \frac{1}{n}\sum_{i=1}^n 
\left(\sum_{j=1}^p \phi_{j1}x_{ij}\right)^{\!\!\!2} \text{ subject to }
\sum_{j=1}^p \phi^2_{j1} = 1$$

PP:

$$\mathop{\text{maximize}}_{\phi_{11},\dots,\phi_{p1}} ~~f\left(\sum_{j=1}^p \phi_{j1}x_{ij}\right) \text{ subject to }
\sum_{j=1}^p \phi^2_{j1} = 1$$



## Next: Logistic regression and discriminant analysis {.transition-slide .center}


