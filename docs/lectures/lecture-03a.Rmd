---
title: "ETC3250/5250: Introduction to Machine Learning"
subtitle: "Categorical response regression"
author: "Professor Di Cook"
email: "ETC3250.Clayton-x@monash.edu"
date: "Week 3a"
length: "50 minutes"
department: "Department of Econometrics and Business Statistics"
titlebgimg: "images/bg-02.png"
output:
  xaringan::moon_reader:
    css:
      - ninjutsu 
      - "assets/font-awesome-all.css"
      - "assets/tachyons-addon.css"
      - "assets/animate.css"
      - "assets/fira-code.css"
      - "assets/boxes.css"
      - "assets/table.css"
      - "assets/styles.css"
      - "assets/monash-brand.css"
      - "assets/monash-fonts.css"
      - "assets/slide-types.css"
      - "assets/custom.css"
      - "assets/panelset.css"
    self_contained: false 
    seal: false 
    chakra: 'lib/remark-latest.min.js'
    includes:
      in_header: "assets/head.html"
    lib_dir: lib
    #includes:
    #  in_header: "assets/custom.html"
    mathjax: "lib/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    nature:
      highlightStyle: magula
      highlightLanguage: r 
      highlightLines: true
      highlightSpans: false 
      countIncrementalSlides: false
      slideNumberFormat: '%current%/%total%'
      navigation:
        scroll: false 
        touch: true
        click: false
      ratio: '16:9'
---

```{r, include = FALSE}
current_file <- knitr::current_input()
basename <- gsub(".Rmd$", "", current_file)

knitr::opts_chunk$set(
  fig.path = sprintf("images/%s/", basename),
  fig.width = 6,
  fig.height = 4,
  out.width = "100%",
  fig.align = "center",
  fig.retina = 3,
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE,
  cache.path = "cache/"
)
```

```{r titleslide, child="assets/titleslide.Rmd"}
```

```{r}
library(ISLR)
library(tidyverse)
library(broom)
library(parsnip)
#library(countdown)
```

---
# Categorical responses

In **classification**, the output $Y$ is a .monash-orange2[categorical variable]. For example,

- Loan approval: $Y \in \{\mbox{successful}, \mbox{unsuccessful}\}$ 
- Type of business culture: $Y \in \{\mbox{clan}, \mbox{adhocracy}, \mbox{market}, \mbox{hierarchical}\}$
- Historical document author: $Y \in \{\mbox{Austen}, \mbox{Dickens}, \mbox{Imitator}\}$
- Email: $Y \in \{\mbox{spam}, \mbox{ham}\}$

Map the categories to a numeric variable, or possibly a binary matrix.


<!--
- A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?
- An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user's IP address, past transaction history, and so forth.
- On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not.
- An email comes into the server. Should it be moved into the inbox or the junk mail box, based on header text, sender, origin, time of day, ...?
-->

---
# When linear regression is not appropriate

.flex[
.w-40[

<br>
Consider the following data `simcredit` in the ISLR R package (textbook) which looks at the default status based on credit balance.

<br><br>

.question-box[Why is a linear model less than ideal for this data?]

]
.w-10[

.white[this is just space]

]
.w-45[

<br>
```{r}
data(Default)
simcredit <- Default %>%
  mutate(default_bin = ifelse(default=="Yes", 1, 0))
ggplot(simcredit, aes(x=balance, y=default_bin)) +
  geom_point() +
  geom_smooth(method="lm", colour = "#027EB6", se = FALSE) +
  ylab("default") +
  theme_minimal(base_size=18)
```


]
]
---
# Modelling binary responses

.flex[
.w-45[
```{r}
ggplot(simcredit, aes(x=balance, y=default_bin)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE, color = "#027EB6") +
  geom_smooth(method="glm", 
              method.args = list(family = "binomial"), 
              colour = "#D93F00", se = FALSE) +
  theme_minimal(base_size=18) +
  ylab("default")
```

.monash-orange2[Orange] line is a loess smooth of the data. It's much better than the linear fit. 

]
.w-45[

<br>

- To model **binary data**, we need to .monash-orange2[link] our **predictors** to our response using a *link function*. Another way to think about it is that we will transform $Y$, to convert it to a proportion, and then build the linear model on the transformed response.
- There are many different types of link functions we could use, but for a binary response we typically use the .monash-orange2[logistic] link function.

]
.w-10[

.white[this is just space]

]

]
---
# The logistic function

.flex[
.w-40[

Instead of predicting the outcome directly, we instead predict the probability of being class 1, given the (linear combination of) predictors, using the .monash-orange2[logistic] link function.

$$ p(y=1|\beta_0 + \beta_1 x)  = f(\beta_0 + \beta_1 x) $$
where

$$f(\beta_0 + \beta_1 x) = \frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$$

]
.w-10[

.white[this is just space]

]
.w-45[
```{r, echo=FALSE, fig.retina=4, warning=F, message=F}
x <- rep(seq(-10, 10, by=0.1), 2)
logistic <- 1 / (1 + exp(-x))
#Link <- c(rep("logistic", length(x.vals)))

library(latex2exp)

data <- tibble(x, logistic)
ggplot(data, aes(x=x, y=logistic)) +
  geom_line(size=1.4, color = "#D93F00") + 
  xlab(TeX('$\\beta_0 + \\beta_1 x$')) + 
  ylab(TeX('$p(y=1|\\beta_0 + \\beta_1 x) = f(\\beta_0 + \\beta_1 x)$')) +
  theme_minimal() +
  theme(text = element_text(size=18))
```

]
]

---

.flex[
.w-40[

# Logistic function

Transform the function:

$$~~~~y = \frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$$

$\longrightarrow  y = \frac{1}{1/e^{\beta_0+\beta_1x}+1}$

$\longrightarrow  1/y = 1/e^{\beta_0+\beta_1x}+1$

$\longrightarrow 1/y - 1 = 1/e^{\beta_0+\beta_1x}$

$\longrightarrow  \frac{1}{1/y - 1} = e^{\beta_0+\beta_1x}$

$\longrightarrow \frac{y}{1 - y} = e^{\beta_0+\beta_1x}$

$\longrightarrow \log_e\frac{y}{1 - y} = \beta_0+\beta_1x$
]
.w-10[

.white[this is just space]

]
]

---
count: false

.flex[
.w-40[

# Logistic function

Transform the function:

$$~~~~y = \frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$$

$\longrightarrow  y = \frac{1}{1/e^{\beta_0+\beta_1x}+1}$

$\longrightarrow  1/y = 1/e^{\beta_0+\beta_1x}+1$

$\longrightarrow 1/y - 1 = 1/e^{\beta_0+\beta_1x}$

$\longrightarrow  \frac{1}{1/y - 1} = e^{\beta_0+\beta_1x}$

$\longrightarrow \frac{y}{1 - y} = e^{\beta_0+\beta_1x}$

$\longrightarrow \log_e\frac{y}{1 - y} = \beta_0+\beta_1x$
]
.w-10[

.white[this is just space]

]
.w-40[


<br>
<br>
<br>
<br>

Transforming the response $\log_e\frac{y}{1 - y}$ makes it possible to use a linear model fit.
 
<br>
<br>
<br>
<br>


.info-box[The left-hand side, $\log_e\frac{y}{1 - y}$, is known as the .monash-orange2[log-odds ratio] or logit.
<i class="fas fa-dice" style="color: #D93F00"></i>
]

]]

---
# The logistic regression model

The fitted model, where $P(Y=0|X) = 1 - P(Y=1|X)$, is then written as:

<center>
.info-box[
$\log_e\frac{P(Y=1|X)}{1 - P(Y=1|X)} = \beta_0+\beta_1X$
]
</center>

<br><br>
When there are .monash-blue2[more than two] categories:
- the formula can be extended, using dummy variables.
- follows from the above, extended to provide probabilities for each level/category, and the last category is 1-sum of the probabilities of other categories.
- the sum of all probabilities has to be 1.

---
# Interpretation

- .monash-blue2[**Linear regression**]
    - $\beta_1$ gives the average change in $Y$ associated with a one-unit increase in $X$
--
- .monash-blue2[**Logistic regression**]
    - Because the model is not linear in $X$, $\beta_1$ does not correspond to the change in response associated with a one-unit increase in $X$.
    - However, increasing $X$ by one unit changes the log odds by $\beta_1$, or equivalently it multiplies the odds by $e^{\beta_1}$

---
# Maximum Likelihood Estimation

Given the logistic $p(x_i) = \frac{1}{e^{-(\beta_0+\beta_1x_i)}+1}$
choose parameters $\beta_0, \beta_1$ to maximize the likelihood:

$$\mathcal{l}_n(\beta_0, \beta_1) = \prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}.$$

It is more convenient to maximize the *log-likelihood*:

\begin{align*}
\log  l_n(\beta_0, \beta_1) &= \sum_{i = 1}^n \big( y_i\log p(x_i) + (1-y_i)\log(1-p(x_i))\big)\\
&= \sum_{i=1}^n\big(y_i(\beta_0+\beta_1x_i)-\log{(1+e^{\beta_0+\beta_1x_i})}\big)
\end{align*}


---
# Making predictions

.flex[
.w-45[
With estimates from the model fit, $\hat{\beta}_0, \hat{\beta}_1$, we can predict the **probability of belonging to class 1** using:


$$p(y=1|\hat{\beta}_0 + \hat{\beta}_1 x) = \frac{e^{\hat{\beta}_0+ \hat{\beta}_1x}}{1+e^{\hat{\beta}_0+ \hat{\beta}_1x}}$$
<br>

- .monash-orange2[Round to 0 or 1] for class prediction.
- Residual could be calculated as the difference between observed and predicted. Mostly, the misclassification (correct or incorrect) is used to assess the model fit.
]
.w-45[

```{r}
fit <- glm(default~balance,  
           data=simcredit, family="binomial") 
simcredit_fit <- augment(fit, simcredit, type.predict="response")
ggplot(simcredit_fit, aes(x=balance, y=default_bin)) +
  geom_point() +
  geom_point(aes(y=.fitted), colour = "#D93F00") +
  theme_minimal(base_size=18) 
```
.monash-orange2[Orange points] are fitted values, $\hat{y}_i$. Black points are observed response, $y_i$ (either 0 or 1). 
]
]

---
.flex[
.w-45[
## Fitting credit data in R <i class="fas fa-credit-card" style="color: #D93F00"></i>

<br>

 We use the `glm` function in R to fit a logistic regression model. The `glm` function can support many response types, so we specify `family="binomial"` to let R know that our response is *binary*.
]
.w-45[

<br><br><br>

```{r logistic_fit, echo=TRUE}
logistic_mod <- logistic_reg() %>% 
  set_engine("glm") %>% #<<
  set_mode("classification") %>% #<<
  translate()

logistic_fit <- 
  logistic_mod %>% 
  fit(default ~ balance, 
      data = simcredit)

```

]
]


---

# Examine the fit

<br>

```{r echo=TRUE}
tidy(logistic_fit) #<<
glance(logistic_fit) #<<
```

---
# Write out the model

$\hat{\beta}_0 =$ `r broom::tidy(logistic_fit)[1, 2]`

$\hat{\beta}_1 =$ `r broom::tidy(logistic_fit)[2, 2]`

<br><br><br>

# Model fit summary

Null model deviance `r round(broom::glance(logistic_fit)[1], 1)` (think of this as TSS)

Model deviance `r round(broom::glance(logistic_fit)[6], 1)` (think of this as RSS)

---
# Check model fit

```{r echo=TRUE}
simcredit_fit <- augment(logistic_fit, simcredit) #<<
simcredit_fit %>% 
  count(default, .pred_class) %>%
  pivot_wider(names_from = "default", values_from = n)
```

<br>
<br>
<center>
.info-box[Note: Residuals not typically used.]
<center>

---
.flex[
.w-45[

# A warning for using GLMs!

<br>

Logistic regression model fitting fails when the data is *perfectly* separated.

MLE fit will try and fit a step-wise function to this graph, pushing coefficients sizes towards infinity and produce large standard errors.

.monash-orange2[Pay attention to warnings!]

]
.w-45[

```{r echo=FALSE, warning = FALSE}
simcredit <- simcredit %>%
  mutate(default_new = as.factor(case_when(balance <= 1500 ~ 0,
                                 balance > 1500 ~ 1)))

ggplot(simcredit, aes(x=balance, y=default_new)) +
  geom_point() +
  geom_vline(xintercept = 1500, col = "grey50", size = 1.1, linetype = "dashed") +
  theme_minimal(base_size=18)
```

]]


```{r echo = TRUE, warning = TRUE}
logistic_fit <- 
  logistic_mod %>% 
  fit(default_new ~ balance, 
      data = simcredit)
```


---
class: informative middle center
# More on supervised classification to come

Logistic regression is a technique for supervised classification. We'll see a lot more techniques: linear discriminant analysis, trees, forests, support vector machines, neural networks. 

---

```{r endslide, child="assets/endslide.Rmd"}
```
