<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>ETC3250/5250: Introduction to Machine Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Professor Di Cook" />
    <script src="lib/header-attrs-2.7/header-attrs.js"></script>
    <link href="lib/remark-css-0.0.1/ninjutsu.css" rel="stylesheet" />
    
    <!--
    <script defer src="assets/all.min.js"></script>

    Need below to enable css contents

    <script>
      window.FontAwesomeConfig = {
        searchPseudoElements: true
      }
    </script>

    -->
    <link rel="stylesheet" href="assets/font-awesome-all.css" type="text/css" />
    <link rel="stylesheet" href="assets/tachyons-addon.css" type="text/css" />
    <link rel="stylesheet" href="assets/animate.css" type="text/css" />
    <link rel="stylesheet" href="assets/fira-code.css" type="text/css" />
    <link rel="stylesheet" href="assets/boxes.css" type="text/css" />
    <link rel="stylesheet" href="assets/table.css" type="text/css" />
    <link rel="stylesheet" href="assets/styles.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-brand.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-fonts.css" type="text/css" />
    <link rel="stylesheet" href="assets/slide-types.css" type="text/css" />
    <link rel="stylesheet" href="assets/custom.css" type="text/css" />
    <link rel="stylesheet" href="assets/panelset.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">






class: middle center hide-slide-number monash-bg-gray80





.info-box.w-50.bg-white[
These slides are viewed best by Chrome or Firefox and occasionally need to be refreshed if elements did not load properly. See &lt;a href=lecture-08b.pdf&gt;here for the PDF &lt;i class="fas fa-file-pdf"&gt;&lt;/i&gt;&lt;/a&gt;. 
]

&lt;br&gt;

.white[Press the **right arrow** to progress to the next slide!]



---

class: title-slide
count: false
background-image: url("images/bg-02.png")

# .monash-blue[ETC3250/5250: Introduction to Machine Learning]

&lt;h1 class="monash-blue" style="font-size: 30pt!important;"&gt;&lt;/h1&gt;

&lt;br&gt;

&lt;h2 style="font-weight:900!important;"&gt;Regularisation&lt;/h2&gt;

.bottom_abs.width100[

Lecturer: *Professor Di Cook*

Department of Econometrics and Business Statistics

&lt;i class="fas fa-envelope"&gt;&lt;/i&gt;  ETC3250.Clayton-x@monash.edu

&lt;i class="fas fa-calendar-alt"&gt;&lt;/i&gt; Week 8b

&lt;br&gt;

]



---
# Too many variables

Fitting a linear regression model requires:

`\begin{align*}
		%\underset{{\beta} \in \mathbb{R}^{p} }{\operatorname{minimize}}~&amp; \sum_{i = 1}^n (y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij})^2 \\
		\underset{{\beta} \in \mathbb{R}^{p} }{\text{minimize}}&amp; \left\{ \sum_{i = 1}^n \left(y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij} \right)^2 \right\} \\
		\equiv \underset{{\beta} \in \mathbb{R}^{p} }{\operatorname{minimize}}~&amp; ({y} - {X}{\beta})' ({y} - {X}{\beta}) 
\end{align*}`

The least square solution for `\(\beta\)` is

$$ {\hat \beta} = \color{orange}{({X}' {X})^{-1}} {X}' {y} $$

To .monash-orange2[invert] a matrix, requires it to be .monash-orange2[full rank].

---

# Example: Using simulation

- 20 observations
- 2 classes: A, B
- One variable with separation, 99 noise variables

&lt;img src="images/lecture-08b/unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;" /&gt;

.monash-orange2[What will be the optimal LDA coefficients?]

---

Fit linear discriminant analysis on .monash-orange2[first two variables].


```
## Call:
## lda(cl ~ ., data = tr[, c(1:2, 101)], prior = c(0.5, 0.5))
## 
## Prior probabilities of groups:
##   A   B 
## 0.5 0.5 
## 
## Group means:
##           x1            x2
## A  0.8918346  0.0009586256
## B -0.8918346 -0.0009586256
## 
## Coefficients of linear discriminants:
##            LD1
## x1 -2.41606038
## x2  0.05224863
```

Coefficient for `x1` MUCH higher than `x2`. .monash-orange2[As expected!]

---
class: split-50

.column[.pad50px[
Predict the training and test sets



```
##    
##      A  B
##   A 10  0
##   B  0 10
```

```
##    
##     A B
##   A 5 0
##   B 0 5
```

&lt;img src="images/lecture-08b/unnamed-chunk-5-1.png" width="80%" style="display: block; margin: auto;" /&gt;


]]
.column[.pad50px[
&lt;img src="images/lecture-08b/unnamed-chunk-6-1.png" width="150%" style="display: block; margin: auto;" /&gt;
]]

---
class: transition middle center

### What happens to test set (and predicted training values) as number of noise variables increases

---

&lt;img src="images/lecture-08b/unnamed-chunk-7-.gif" width="100%" style="display: block; margin: auto;" /&gt;

---

class: transition middle center
### What happens to the estimated coefficients as dimensions of noise increase?

Remember, the noise variables should have coefficient = ZERO.

---
&lt;img src="images/lecture-08b/unnamed-chunk-8-.gif" width="100%" style="display: block; margin: auto;" /&gt;

---
class: transition middle

# How do we tackle high-dimension, low sample size problems?

---
# Subset selection

Identify a subset `\(s\)` of the `\(p\)` predictors, most related to response.

`\begin{align*}
	&amp; \underset{{\beta}}{\text{minimize}} \left\{ \sum_{i = 1}^n \left(y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij} \right)^2 \right\} \\
	&amp; \text{subject to}\sum_{j = 1}^p {I}(\beta_j \neq 0) \leq k, \quad  k \geq 0.
\end{align*}`
where `\(k \geq 0\)` is a tuning parameter.

- Need to consider `\({p \choose k}\)` models containing `\(s\)` predictors computationally infeasible when `\(p\)` and `\(s\)` are large
- Stepwise procedures: forward, backward, etc.

---

# Model fit statistics

These can be used to decide on choice  of `\(k\)`.

- `\(MSE = RSS/n\)`, but the training `\(MSE\)` is an under-estimate of test `\(MSE\)`, and it will decrease with larger `\(p\)`.
- Methods for adjusting the training error for model size include Mallows `\(C_p\)`, Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC) and adjusted `\(R^2\)`.

&lt;center&gt;
&lt;a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter6/6.2.pdf" target="_BLANK"&gt; &lt;img src="images/lecture-08b/6.2.png" style="width: 70%; align: center"/&gt; &lt;/a&gt;
&lt;/center&gt;

---

# Mallows `\(C_p\)`

&lt;br&gt;

For a fitted least squares model containing `\(d\)` predictors, a reasonable estimate of the test MSE is:

`$$C_p = \frac{1}{n} (RSS+2d\hat{\sigma}^2)$$`
where `\(\hat{\sigma}^2\)` is an estimate of the variance of the error `\(\varepsilon\)`, computed from the full model containing all predictors. 

The additional part penalises the training RSS to adjust for the under-estimation of test error.


---
# AIC and BIC

&lt;br&gt;

`$$AIC = \frac{1}{n\hat{\sigma}^2} (RSS+2d\hat{\sigma}^2)$$`
and hence is `\(\propto C_p\)`. 

`$$BIC = \frac{1}{n\hat{\sigma}^2} (RSS+\log(n)d\hat{\sigma}^2)$$`

all tend to take on low values for models with small test error.


---
# Adjusted `\(R^2\)`

&lt;br&gt;
&lt;br&gt;

`$$\mbox{Adjusted }R^2 = 1-\frac{RSS/(n-d-1)}{TSS/(n-1)}$$`

The intuition is that once all of the correct variables have been included in the model, adding additional *noise* variables will lead to only a very small decrease in RSS.

---

# Best subset selection algorithm

&lt;br&gt;

1. Let `\({\mathcal M}_o\)` denote the null model, which contains no predictors. This
model simply predicts the sample mean for each observation.
2. For `\(k=1,2,...,p\)`:&lt;br&gt;
    a. Fit all `\({p \choose k}\)` models that contain exactly `\(k\)` predictors.&lt;br&gt;
    b. Pick the best among these `\({p \choose k}\)` models, and call it `\({\mathcal M}_k\)`. Best means smallest RSS (or largest `\(R^2\)`).
3. Select a single best model from among `\({\mathcal M}_o\)`, . . . , `\({\mathcal M}_p\)` using cross- validated prediction error, `\(C_p\)` (AIC), BIC, or adjusted `\(R^2\)`.

---
# Best subset selection algorithm


.monash-orange2[Best subset selection] algorithm applied to the 11 predictors of the Credit data.

&lt;center&gt;
&lt;a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter6/6.1.pdf" target="_BLANK"&gt; &lt;img src="images/lecture-08b/6.1.png" style="width: 80%; align: center"/&gt; &lt;/a&gt;
&lt;/center&gt;

.font_smaller2[(Chapter 6/6.1)]

---
# Forward stepwise selection 

.monash-orange2[Forward stepwise selection] is a computationally efficient alternative to best subset selection. It considers a much smaller set of models.

When `\(p = 20\)`, best subset selection requires fitting 1,048,576 models, whereas forward stepwise selection requires fitting only 211 models.

---

# Forward stepwise selection - algorithm

&lt;br&gt;

1. Let `\({\mathcal M}_o\)` denote the null model, which contains no predictors. This
model simply predicts the sample mean for each observation.
2. For `\(k=0,1,2,...,p-1\)`:&lt;br&gt;
    a. Consider all `\({p - k}\)` models that augment `\({\mathcal M}_k\)` with *one additional predictor*.&lt;br&gt;
    b. Pick the best among these `\({p - k}\)` models, and call it `\({\mathcal M}_{k+1}\)`. Best means smallest RSS (or largest `\(R^2\)`).
3. Select a single best model from among `\({\mathcal M}_o\)`, . . . , `\({\mathcal M}_p\)` using cross- validated prediction error, `\(C_p\)` (AIC), BIC, or adjusted `\(R^2\)`.

---

# Shrinkage methods

&lt;br&gt;


.monash-orange2[Shrinkage methods] fit a model containing all `\(p\)` predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that .monash-orange2[shrinks some of the coefficient estimates towards zero].

&lt;br&gt;
.tip[There are two main methods: .monash-orange2[Ridge] regression and .monash-orange2[Lasso].]


---

# Ridge regression

`\begin{align*}
	&amp; \mbox{RSS} =  \sum_{i = 1}^n \left(y_i - \beta_0 - \sum_{j = 1}^p \beta_j x_{ij} \right)^2 
\end{align*}`

Least squares: 

`\begin{align*}
	&amp; \underset{{\beta}}{\text{minimize}} \mbox{ RSS}
\end{align*}`

.monash-orange2[Ridge] regression: 

`\begin{align*}
	&amp; \underset{{\beta}}{\text{minimize}} \mbox{ RSS} \color{orange}{+ \lambda \sum_{j=1}^p \beta_j^2}
\end{align*}`
where `\(\lambda \geq 0\)` is a tuning parameter.

---
# Ridge regression

`$$\lambda \sum_{j=1}^p \beta_j^2$$` 
is called a .monash-orange2[shrinkage penalty]. It is small when `\(\beta_1, ..., \beta_p\)` are close to 0.

`\(\lambda\)` serves as a .monash-orange2[tuning parameter], controlling the relative impact of these two terms on the regression coefficient estimates. When it is 0, the penalty term has no effect on the fit. 

Ridge regression will produce a .monash-orange2[different set of coefficients] for each `\(\lambda\)`, call them `\(\hat{\beta}_{\lambda}^R\)`. Tuning `\(\lambda\)`, typically by cross-validation, is critical component of fitting the model. 


---
class: split-60
layout: false

.column[.pad10px[
.monash-orange2[Standardized ridge regression coefficients for the Credit data set. ]


&lt;a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter6/6.4.pdf" target="_BLANK"&gt; &lt;img src="images/lecture-08b/6.4.png" style="width: 100%; align: center"/&gt; &lt;/a&gt;

.font_smaller2[(Chapter6/6.4.pdf)]


]]
.column[.top50px[

- `\(p=10\)`
- Left side of plot corresponds to least squares.
- When `\(\lambda\)` is extremely large, then all of the ridge coefficient estimates are basically zero, which is the null model.
- 4 of 10 variables have larger coefficients, and one, Rating, initially increases with `\(\lambda\)`. 
- Right-side plot, `\(x\)`-axis indicates amount the coefficients shrink to 0, value of 1 indicates LS.
]]


---
class: split-50
layout: false

.column[.pad10px[

The scale of variables can affect ridge regression performance.

.monash-orange2[It is important to standardise the scale of predictors prior to ridge regression.]


`$$\tilde{x}_{ij} = \frac{x_{ij}}{\sigma_{x_{j}}}$$`
]]
.column[.pad10px[

Simulation scenario! Ridge regression improves on least squares, for large number of variables, in the bias-variance tradeoff. It .monash-orange2[sacrifices some bias] for the benefit of .monash-orange2[decreased variance].  

&lt;a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter6/6.5.pdf" target="_BLANK"&gt; &lt;img src="images/lecture-08b/6.5.png" style="width: 100%; align: center"/&gt; &lt;/a&gt;

.black[bias]
.green[variance]
.purple[test error]



.font_smaller2[(Chapter6/6.5.pdf)]

]]
---
# The Lasso

Ridge regression: 

`\begin{align*}
	&amp; \underset{{\beta}}{\text{minimize}} \mbox{ RSS} + \lambda \sum_{j=1}^p \beta_j^2
\end{align*}`

.monash-orange2[Lasso:]

`\begin{align*}
	&amp; \underset{{\beta}}{\text{minimize}} \mbox{ RSS} + \lambda \sum_{j=1}^p \color{orange}{|\beta_j|}
\end{align*}`

and same `\(\lambda \geq 0\)` is a tuning parameter.



---
class: split-60
layout: false

.column[.pad10px[
Standardized lasso coefficients for the Credit data set. 

&lt;a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter6/6.6.pdf" target="_BLANK"&gt; &lt;img src="images/lecture-08b/6.6.png" style="width: 100%; align: center"/&gt; &lt;/a&gt;

.font_smaller2[(Chapter6/6.6.pdf)]


]]
.column[.top50px[

- `\(p=10\)`
- Has the effect of forcing some variables exactly to 0.
- Cleaner solution than ridge regression.
]]

---

# Simulation scenario! 

Bias-variance tradeoff with lasso, and comparison against ridge regression.


&lt;center&gt;
&lt;a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter6/6.8.pdf" target="_BLANK"&gt; &lt;img src="images/lecture-08b/6.8.png" style="width: 80%; align: center"/&gt; &lt;/a&gt;
&lt;/center&gt;

.center[
.black[Bias]
.green[Variance]
.purple[Test error]]


.font_smaller2[(Chapter6/6.5.pdf)]


---
class: transition middle

# Examples of regularised techniques

---
# Penalised LDA

Recall: LDA involves the eigen-decomposition of `\(\color{orange}{\Sigma^{-1}\Sigma_B}\)`, where

`$$\small{\Sigma_B = \frac{1}{K}\sum_{i=1}^{K} (\mu_i-\mu)(\mu_i-\mu)'}$$`

`$$\small{\Sigma = \frac{1}{n} \sum_{i=1}^{n} (x_i-\mu_i)(x_i-\mu_i)'}$$`

The eigen-decomposition is an analytical solution to a sequential optimisation problem: 


`\begin{align*}
&amp; \small{\underset{{\beta_k}}{\text{maximize}}~~ \beta_k^T\hat{\Sigma}_B \beta_k} \\
&amp; \small{\mbox{ subject to  }  \beta_k^T\hat{\Sigma} \beta_k \leq 1, ~~\beta_k^T\hat{\Sigma}\beta_j = 0 \mbox{  } \forall i&lt;k}
\end{align*}`


---
# Penalised LDA

The problem is inverting `\(\color{orange}{\Sigma^{-1}}\)`, fix it by

`\begin{align*}
&amp; \underset{{\beta_k}}{\text{maximize}} \left(\beta_k^T\hat{\Sigma}_B \beta_k + \lambda_k \sum_{j=1}^p |\hat{\sigma}_j\beta_{kj}|\right)\\
&amp; \mbox{ subject to  }  \beta_k^T\tilde{\Sigma} \beta_k \leq 1
\end{align*}`

where `\(\hat{\sigma}_j\)` is the within-class standard deviation for variable `\(j\)`. This is 
.orange[penalised LDA], and see [reference](https://faculty.washington.edu/dwitten/Papers/JRSSBPenLDA.pdf), and the [R package](https://cran.r-project.org/web/packages/penalizedLDA/index.html). 




---
# PDA Index

&lt;br&gt;

Penalised LDA projection pursuit index. Available in the `tourr` package. 

`\begin{align*}
I_{PDA}(A,\lambda) =
1-\frac{\Big|A'\big\{(1-\lambda)\hat{\Sigma}+n\lambda I_p\big\}A\Big|} {\Big|A'\big\{(1-\lambda)(\hat{\Sigma}_B +\hat{\Sigma})+n\lambda I_p\big\} A\Big|}
\end{align*}`

Optimising this function over `\(p\times d\)` projection matrix `\(A\)`. 



---
# Lasso regression

Read the example of lasso regression or watch the screencast by Julia Silge [here](https://juliasilge.com/blog/lasso-the-office/)

&lt;center&gt;
&lt;iframe width="720" height="405" src="https://www.youtube.com/embed/R32AsuKICAY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/center&gt;
---




background-size: cover
class: title-slide
background-image: url("images/bg-02.png")

&lt;a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"&gt;&lt;img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /&gt;&lt;/a&gt;&lt;br /&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.


.bottom_abs.width100[

Lecturer: *Professor Di Cook*

Department of Econometrics and Business Statistics

&lt;i class="fas fa-envelope"&gt;&lt;/i&gt;  ETC3250.Clayton-x@monash.edu

&lt;i class="fas fa-calendar-alt"&gt;&lt;/i&gt; Week 8b

&lt;br&gt;

]




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="lib/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "magula",
"highlightLanguage": "r",
"highlightLines": true,
"highlightSpans": false,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%/%total%",
"navigation": {
"scroll": false,
"touch": true,
"click": false
},
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'lib/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
