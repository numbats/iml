<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>ETC3250/5250: Introduction to Machine Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Professor Di Cook" />
    <script src="lib/header-attrs-2.13/header-attrs.js"></script>
    <link href="lib/remark-css-0.0.1/ninjutsu.css" rel="stylesheet" />
    
    <!--
    <script defer src="assets/all.min.js"></script>

    Need below to enable css contents

    <script>
      window.FontAwesomeConfig = {
        searchPseudoElements: true
      }
    </script>

    -->
    <link rel="stylesheet" href="assets/font-awesome-all.css" type="text/css" />
    <link rel="stylesheet" href="assets/tachyons-addon.css" type="text/css" />
    <link rel="stylesheet" href="assets/animate.css" type="text/css" />
    <link rel="stylesheet" href="assets/fira-code.css" type="text/css" />
    <link rel="stylesheet" href="assets/boxes.css" type="text/css" />
    <link rel="stylesheet" href="assets/table.css" type="text/css" />
    <link rel="stylesheet" href="assets/styles.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-brand.css" type="text/css" />
    <link rel="stylesheet" href="assets/monash-fonts.css" type="text/css" />
    <link rel="stylesheet" href="assets/slide-types.css" type="text/css" />
    <link rel="stylesheet" href="assets/custom.css" type="text/css" />
    <link rel="stylesheet" href="assets/panelset.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">






class: middle center hide-slide-number monash-bg-gray80





.info-box.w-50.bg-white[
These slides are viewed best by Chrome or Firefox and occasionally need to be refreshed if elements did not load properly. See &lt;a href=lecture-07b.pdf&gt;here for the PDF &lt;i class="fas fa-file-pdf"&gt;&lt;/i&gt;&lt;/a&gt;. 
]

&lt;br&gt;

.white[Press the **right arrow** to progress to the next slide!]



---

class: title-slide
count: false
background-image: url("images/bg-02.png")

# .monash-blue[ETC3250/5250: Introduction to Machine Learning]

&lt;h1 class="monash-blue" style="font-size: 30pt!important;"&gt;&lt;/h1&gt;

&lt;br&gt;

&lt;h2 style="font-weight:900!important;"&gt;Support vector machines&lt;/h2&gt;

.bottom_abs.width100[

Lecturer: *Professor Di Cook*

Department of Econometrics and Business Statistics

&lt;i class="fas fa-envelope"&gt;&lt;/i&gt;  ETC3250.Clayton-x@monash.edu

&lt;i class="fas fa-calendar-alt"&gt;&lt;/i&gt; Week 7b

&lt;br&gt;

]





---
# Separating hyperplanes



In a `\(p\)`-dimensional space, a .monash-orange2[hyperplane] is a linear subspace of dimension `\(p - 1\)`.

&lt;center&gt;
&lt;img src="images/lecture-07b/9.1.png" style="width: 40%"/&gt;
&lt;/center&gt;


.font_smaller2[(ISLR: Fig 9.1)]
---
# Separating hyperplanes 

The equation of `\(p\)`-dimensional hyperplane is given by

`$$\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p = 0$$`


For the `\(i^{th}\)` observation, 

`\begin{align*}
x_i = \left(\begin{array}{cccc}
x_{i1} &amp; x_{i2} &amp; \cdots &amp; x_{ip} 
\end{array} \right)
\end{align*}`

and `\(y_i\)` coded as  `\(\{-1, 1\}\)`, `\(i = 1, \dots, n\)`, then

`$$\beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} &gt; 0 \mbox{ if }  y_i = 1,$$` 
	
`$$\beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} &lt; 0 \mbox{ if } y_i = -1$$`


Equivalently,

`$$y_i (\beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip}) &gt; 0$$`



---
# Separating hyperplanes


- A new observation is assigned a class depending on .monash-orange2[which side] of the hyperplane it is located
- Classify the test observation `\(x_0\)` based on the .monash-orange2[sign] of 
`$$s(x_0) = \beta_0 + \beta_1 x_{01} + \dots + \beta_p x_{0p}$$`
- If `\(s(x_0) &gt; 0\)`, class `\(1\)`, and if `\(s(x_0) &lt; 0\)`, class `\(-1\)`, i.e. `\(h(x_0) = \mbox{sign}(s(x_0)).\)`
- `\(s(x_0) \mbox{ far from zero } \rightarrow\)` `\(x_0\)` lies far from the hyperplane + **more confident** about our classification
- `\(s(x_0) \mbox{ close to zero } \rightarrow\)` `\(x_0\)` near the hyperplane + **less confident** about our classification

---

# Separating hyperplane classifiers

.monash-blue2[*Three*] different types of hyperplane classifiers.

- .monash-orange2[Maximal marginal classifier] for when the data is perfectly separated by a hyperplane
- .monash-orange2[Support vector classifier/soft margin classifier] for when data is NOT perfectly separated by a hyperplane but still has a linear decision boundary, and
- .monash-orange2[Support vector machines] used for when the data has nonlinear decision boundaries.

&lt;br&gt;&lt;br&gt; All are support vector machines.

---
# Maximal margin classifier


All are separating hyperplanes. .monash-orange2[Which is best?]

&lt;center&gt;
&lt;img src="images/lecture-07b/svm-hyperplanes.jpg" style="width: 60%"/&gt;
&lt;/center&gt;


---
# Maximal margin classifier


&lt;center&gt;
&lt;img src="images/lecture-07b/svm_diagram.png" style="width: 80%" /&gt;
&lt;/center&gt;

---

# From LDA to SVM

- Linear discriminant analysis uses the difference between means to set the separating hyperplane.
- Support vector machines uses .monash-orange2[gaps] between points on the outer edge of clusters to set the separating hyperplane.

.flex[
&lt;img src="images/lecture-07b/unnamed-chunk-3-1.png" width="100%" style="display: block; margin: auto;" /&gt;

.w-35[
```
svm_mod &lt;-
  svm_rbf(cost = 10) %&gt;%
  set_mode("classification") %&gt;%
  set_engine("kernlab", 
  kernel="vanilladot")

olive_svm &lt;- svm_mod %&gt;%
  fit(region~eicosenoic+linoleic, 
  data=olive)
```
]
]


---
# SVM 

- If our data can be perfectly separated using a hyperplane, then there will in fact exist an **infinite number of such hyperplanes**.
- We compute the (perpendicular) distance from each training observation to a given separating hyperplane. The .monash-orange2[smallest] such distance is known as the .monash-orange2[margin].
- The .monash-orange2[optimal separating hyperplane] (or maximal margin hyperplane)  is the separating hyperplane for which the margin is .monash-orange2[largest]. 
- We can then classify a test observation based on which side of the maximal margin hyperplane it lies. This is known as the .monash-orange2[maximal margin classifier].


---
# Support vectors

&lt;img src="images/lecture-07b/sv_diagram.png" style="width: 70%" /&gt;

See more detailed explanations [here](https://math.stackexchange.com/questions/1305925/why-is-the-svm-margin-equal-to-frac2-mathbfw).
---
# Support vectors

- The .monash-orange2[support vectors] are equidistant from the maximal margin hyperplane and lie along the dashed lines indicating the width of the margin. 
- They .monash-orange2[support] the maximal margin hyperplane in the sense that if these points were moved slightly then the maximal margin hyperplane would move as well


.center[ **The maximal margin hyperplane depends directly on the support vectors, but .monash-orange2[not on the other observations]**]

---
# Support vectors define the maximal margin classifier


The separating hyperplane is defined as

`$$\{x:\beta_0+x^T\beta=0\}$$`

where `\(\beta=\sum_{k=1}^s (\alpha_ky_k)x_k\)` and `\(s\)` is the number of support vectors. Then the .monash-orange2[maximal margin hyperplane] is fitted by .monash-blue2[finding] `\(\beta\)` (ie `\(\alpha\)`) to 

&lt;br&gt;&lt;br&gt;
.monash-blue2[*maximise the margin*] `\(M = \frac{2}{||\beta||}\)`, subject to `\(\sum_{j=1}^p\beta_j^2=1\)`, and `\(y_i(x_i^T\beta+\beta_0)\geq M, i=1, ..., n\)`.



---
# Example: Support vectors

.flex[
.w-39[


```r
indx &lt;- olive_svm$fit@SVindex 
svs &lt;- olive[indx,]
```
]
.w-58[
&lt;img src="images/lecture-07b/unnamed-chunk-5-1.png" width="120%" style="display: block; margin: auto;" /&gt;
]
]



---
class: split-60
layout: false

.column[.pad50px[

# Non-separable case

The maximal margin classifier only works when we have perfect separability in our data.

.monash-blue2[What do we do if data is not perfectly separable by a hyperplane?]

 .monash-orange2[**The support vector classifier**] allows points to either lie on the wrong side of the margin, or on the wrong side of the hyperplane altogether. 
 
 &lt;br&gt;&lt;br&gt;
 &lt;br&gt;&lt;br&gt;
 .font_smaller2[Right: ISLR Fig 9.6]
]]

.column[.content.vmiddle.center[


&lt;img src="images/lecture-07b/9.6-flip.png" style="width: 80%"&gt;

]]

---
# Support vector classifier - optimisation 

&lt;br&gt;

*Maximise* `\(M\)`, subject to `\(\sum_{j=1}^p\beta_j^2=1\)`, and `\(y_i(x_i^T\beta+\beta_0)\geq M(1-\varepsilon_i), i=1, ..., n\)`, AND `\(\varepsilon_i\geq 0, \sum_{i=1}^n\varepsilon_i\leq C\)`.

`\(\varepsilon_i\)` indicates where the `\(i\)`th observation is located and `\(C\)` is a nonnegative .monash-orange2[tuning parameter].

- `\(\varepsilon_i = 0\)`: correct side of the margin,
- `\(\varepsilon_i &gt; 0\)`: wrong side of the margin (violation of the margin),
- `\(\varepsilon_i &gt; 1\)`: wrong side of the hyperplane.


---
# Non-separable case

.monash-orange2[Tuning parameter]: decreasing the value of *C*


&lt;center&gt;
&lt;a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter9/9.7.pdf" target="_BLANK"&gt; &lt;img src="images/lecture-07b/9.7.png" style="width: 45%; align: center"/&gt; &lt;/a&gt;
&lt;/center&gt;

---

# Nonlinear boundaries

The support vector classifier doesn't work well for nonlinear boundaries. .monash-blue2[What solution do we have?]

&lt;center&gt;

&lt;img src="images/lecture-07b/9.8.jpg" style="width: 80%"&gt;

&lt;/center&gt;
---

# Enlarging the feature space

Consider the following 2D non-linear classification problem. We can transform this to a linear problem separated by a maximal margin hyperplane by introducing an additional third dimension.

&lt;center&gt;
&lt;img src="images/lecture-07b/kernel-trick.png" style="width: 80%"&gt;
&lt;/center&gt;

.font_smaller2[Source: Grace Zhang @zxr.nju]
---

# The inner product

Consider two `\(p\)`-vectors 
$$
`\begin{align*}
\mathbf{x} &amp; = (x_1, x_2, \dots, x_p) \in \mathbb{R}^p \\
\mbox{and} \quad \mathbf{y} &amp; = (y_1, y_2, \dots, y_p) \in \mathbb{R}^p.
\end{align*}`
$$
The inner product is defined as 

`$$\langle \mathbf{x}, \mathbf{y}\rangle = x_1y_1 + x_2y_2 + \dots + x_py_p = \sum_{j=1}^{p} x_jy_j$$`

.monash-orange2[A linear measure of similarity, and allows geometric constrctions such as the maximal marginal hyperplane.]

---

# Kernel functions

A kernel function is an inner product of vectors mapped to a (higher dimensional) feature space `\(\mathcal{H} = \mathbb{R}^d, d &gt; p\)`.

$$
\mathcal{K}(\mathbf{x}, \mathbf{y})  = \langle \psi(\mathbf{x}), \psi(\mathbf{y}) \rangle
$$
$$
\psi: \mathbb{R}^p \rightarrow \mathcal{H}
$$

.monash-orange2[Non-linear measure of similarity, and allows geometric constructions in high dimensional space.]

---

# Examples of kernels

Standard kernels include:
$$
`\begin{align*}
\mbox{Linear} \quad  \mathcal{K}(\mathbf{x}, \mathbf{y}) &amp; = \langle\mathbf{x}, \mathbf{y} \rangle \\
\mbox{Polynomial} \quad \mathcal{K}(\mathbf{x}, \mathbf{y}) &amp; = (\langle\mathbf{x}, \mathbf{y} \rangle + 1)^d \\
\mbox{Radial} \quad \mathcal{K}(\mathbf{x}, \mathbf{y}) &amp; = \exp(-\gamma\lvert\lvert\mathbf{x}-\mathbf{y}\lvert\lvert^2)
\end{align*}`
$$

---
class: split-60
layout: false

.column[.pad50px[
# Support Vector Machines


.monash-orange2[The kernel trick]

The linear support vector classifier can be represented as follows:

`$$f(x) = \beta_0 +  \sum_{i \in \mathcal{S}} \alpha_i \langle x, x_i \rangle.$$`

We can generalise this by replacing the inner product with the kernel function as follows:

`$$f(x) = \beta_0 +  \sum_{i \in \mathcal{S}} \alpha_i \mathcal{K}( x, x_i ).$$`

]]

.column[.content.vmiddle.center[

&lt;img src="images/lecture-07b/svm_kernels.png" style="width: 80%; align: center" /&gt;
]]

---
# Your turn

Let `\(\mathbf{x}\)` and `\(\mathbf{y}\)` be vectors in `\(\mathbb{R}^2\)`. By expanding `\(\mathcal{K}(\mathbf{x}, \mathbf{y}) = (1 + \langle \mathbf{x}, \mathbf{y}\rangle) ^2\)` show that this is equivalent to an inner product in `\(\mathcal{H} = \mathbb{R}^6\)`.

&lt;br&gt;

.monash-blue2[Remember:] `\(\langle \mathbf{x}, \mathbf{y}\rangle =\sum_{j=1}^{p} x_jy_j\)`.


---
# Solution

$$
`\begin{align*}
\mathcal{K}(\mathbf{x}, \mathbf{y}) &amp; = (1 + \langle \mathbf{x}, \mathbf{y}\rangle) ^2 \\
                                    &amp; = \left(1 + \sum_{j = 1}^2 x_jy_j \right) ^2 \\
                                    &amp; = (1 + x_1y_1 + x_2y_2)^2 \\
                                    &amp; = (1 + x_1^2y_1^2 + x_2^2y_2^2 + 2x_1y_1 + 2x_2y_2 + 2x_1x_2y_1y_2) \\
                                    &amp; = \langle \psi(\mathbf{x}), \psi(\mathbf{y}) \rangle
\end{align*}`
$$
&lt;br&gt;
where `\(\psi(\mathbf{x}) = (1, x_1^2, x_2^2, \sqrt2x_1, \sqrt2x_2, \sqrt2x_1x_2)\)`.

---
# The kernel trick - why is it a trick?

We do not need to know what the high dimensional enlarged feature space `\(\mathcal{H}\)` really looks like.

We just need to know the which kernel function is most appropriate as a measure of similarity.

&lt;br&gt;

.tip[The Support Vector Machine (SVM) is a maximal margin hyperplane in `\\(\mathcal{H}\\)` built by using a kernel function in the low dimensional feature space `\\(\mathbb{R}^p\\)`.]


---
# Non-linear boundaries
.monash-orange2[Polynomial] and .monash-orange2[radial] kernel SVMs

&lt;a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter9/9.9.pdf" target="_BLANK"&gt; &lt;img src="images/lecture-07b/9.9.png" style="width: 100%; align: center"/&gt; &lt;/a&gt;
---
# Non-linear boundaries

Italian olive oils: Regions 2, 3 (North and Sardinia)

&lt;img src="images/lecture-07b/unnamed-chunk-6-1.png" width="70%" style="display: block; margin: auto;" /&gt;

---
class: center 

# Comparing decision boundaries

&lt;img src="images/lecture-07b/unnamed-chunk-7-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# Increasing the value of `cost` in `svm`

&lt;img src="images/lecture-07b/unnamed-chunk-8-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---
# SVM in high dimensions

Examining misclassifications and which points are selected to be support vectors

&lt;center&gt;

&lt;video width="600" controls&gt; &lt;source src="http://www.ggobi.org/book/chap-class/SVM.mov"&gt; &lt;/video&gt;

&lt;/center&gt;



---
# SVM in high dimensions

[Examining boundaries](http://www.ggobi.org/book/chap-class/classifly.mov)

&lt;center&gt;
&lt;video width="600" controls&gt; &lt;source src="http://www.ggobi.org/book/chap-class/classifly.mov"&gt; &lt;/video&gt;
&lt;/center&gt;

---
# SVM in high dimensions

[Boundaries of a radial kernel in 3D](https://vimeo.com/125405961)

&lt;center&gt;
&lt;video width="700" controls&gt; &lt;source src="https://vimeo.com/125405961"&gt; &lt;/video&gt;
&lt;/center&gt;

---
# SVM in high dimensions

[Boundaries of a polynomial kernel in 5D](https://vimeo.com/125405962)

&lt;center&gt;
&lt;video width="700" controls&gt; &lt;source src="https://vimeo.com/125405962"&gt; &lt;/video&gt;
&lt;/center&gt;


---




background-size: cover
class: title-slide
background-image: url("images/bg-02.png")

&lt;a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"&gt;&lt;img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /&gt;&lt;/a&gt;&lt;br /&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.


.bottom_abs.width100[

Lecturer: *Professor Di Cook*

Department of Econometrics and Business Statistics

&lt;i class="fas fa-envelope"&gt;&lt;/i&gt;  ETC3250.Clayton-x@monash.edu

&lt;i class="fas fa-calendar-alt"&gt;&lt;/i&gt; Week 7b

&lt;br&gt;

]




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="lib/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "magula",
"highlightLanguage": "r",
"highlightLines": true,
"highlightSpans": false,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%/%total%",
"navigation": {
"scroll": false,
"touch": true,
"click": false
},
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'lib/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
