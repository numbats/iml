---
title: "ETC3250/5250 Introduction to Machine Learning"
title-slide-attributes: 
  data-background-image: "../images/bg.png"
  data-background-size: 100%
subtitle: "Week 11: Evaluating your clustering model"
author: 
 - name: "Professor Di Cook"
   email: "etc3250.clayton-x@monash.edu"
institute: "Department of Econometrics and Business Statistics"
footer: "ETC3250/5250 Lecture 11 | [iml.numbat.space](iml.numbat.space)"
format:
  revealjs:
    multiplex: false
    slide-number: c/t
    slide-tone: false
    theme: "../assets/monash.scss"
    width: 1600
    height: 900
    margin: 0.05
    embed-resources: true
---

```{r, include = FALSE}
source("../setup.R")
```

## Overview

We will cover:

* Confusion tables
* Cluster metrics
* Numerical summaries of solution
* Visual summaries
* Low-dimensional representations

## Evaluating your clustering {.transition-slide .center}

## Confusion tables: comparing results [(1/3)]{.smallest}

:::: {.columns}
::: {.column}

- Two clusterings can be compared with a confusion matrix, similarly to supervised classification.
- The main difference though is [cluster labels are not consistent]{.monash-blue2} between methods. Comparison is better if the labels are matched optimally, as a first step.

:::
::: {.column}

```{r}
#| echo: false
set.seed(20190512)
df <- data.frame(x1=scale(c(rnorm(50, -4), rnorm(50, 4))),
                   x2=scale(c(rnorm(100))))
df_hc1 <- hclust(dist(df), method="complete")
df_hc2 <- hclust(dist(df), method="ward.D2")
pd1 <- ggdendrogram(df_hc1) + 
  theme(axis.text = element_text(size=0),
        panel.border = element_blank()) +
  ggtitle("Method 1")
pd2 <- ggdendrogram(df_hc2) + 
  theme(axis.text = element_text(size=0),
        panel.border = element_blank()) +
  ggtitle("Method 2")
pd1 + pd2 + plot_layout(ncol=2)
```

The dendrograms look similar. Are the clusters similar?

:::
::::


## Confusion tables: comparing results [(2/3)]{.smallest}

:::: {.columns}
::: {.column}

```{r}
#| echo: false
df_cl <- df |>
  mutate(cl1 = factor(cutree(df_hc1, 2)),
         cl2 = factor(cutree(df_hc2, 2)))
df_cl |> count(cl1, cl2) |>
  group_by(cl1) |>
  pivot_wider(names_from = cl2, values_from = n) |>
  ungroup()
```


::: {.fragment}
[Match labels]{.monash-blue2}: What method 1 calls cluster `1`, method 2 calls cluster `2`. Rearrange the confusion matrix, or re-label one methods clusters, so that [large counts are on (top-left to bottom-right) diagonal]{.monash-blue2}.

```{r}
#| echo: false
df_cl |> count(cl1, cl2) |>
  arrange(desc(cl1)) |>
  group_by(cl1) |>
  pivot_wider(names_from = cl2, values_from = n)
```
:::


:::
::: {.column}


::: {.fragment}

The two clusterings are indeed quite different. If you look at the model in the data space, it is clear. 

```{r}
#| echo: false
pd3 <- ggplot(df_cl, aes(x1, x2, colour=cl1)) + 
  geom_point() +
  scale_color_discrete_divergingx(palette="Zissou 1") +
  theme(legend.position = "none")
pd4 <- ggplot(df_cl, aes(x1, x2, colour=cl2)) + 
  geom_point() +
  scale_color_discrete_divergingx(palette="Zissou 1") +
  theme(legend.position = "none")
pd3 + pd4 + plot_layout(ncol=2)
```
:::

:::
::::

## Confusion tables: comparing results [(3/3)]{.smallest}

:::: {.columns}
::: {.column}

```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 6
p_hc1 <- hclust(dist(p_std[,2:5]), method="average")
p_hc2 <- hclust(dist(p_std[,2:5]), method="ward.D2")
pd5 <- ggdendrogram(p_hc1) + 
  theme(axis.text = element_text(size=0),
        panel.border = element_blank()) +
  ggtitle("Average linkage")
pd6 <- ggdendrogram(p_hc2) + 
  theme(axis.text = element_text(size=0),
        panel.border = element_blank()) +
  ggtitle("Wards linkage")
pd5 + pd6 + plot_layout(ncol=2)
p_std_cl <- p_std |>
  mutate(
    cl1 = factor(cutree(p_hc1, 5), levels=1:5),
    cl2 = factor(cutree(p_hc2, 5), levels=1:5))
```


- Method 1: 2 or 5 clusters?
- Method 2: 2, 3, 4 or 5 clusters?

:::
::: {.column}

```{r}
#| echo: false
p_std_cl |> count(cl1, cl2) |>
  arrange(cl1, cl2) |>
  group_by(cl1) |>
  pivot_wider(names_from = cl2, 
              values_from = n, 
              values_fill = 0) |>
  ungroup()
```

::: {.fragment style="font-size: 80%;"}
<br>

- You might think that average linkage would give similar results to Wards linkage, but [results here are very different]{.monash-orange2}.
- Average divides the data into three large clusters with two clusters with few observations.
- Wards divides the data into five large clusters, with one having twice the points as the others.
:::

:::
::::

## Cluster metrics [(1/2)]{.smallest}

:::: {.columns}
::: {.column style="font-size: 70%;"}
We covered: within.cluster.ss, WBRatio, Hubert Gamma, Dunn, Calinski-Harabasz Index.

They measure:

```{r}
#| echo: false
w_b <- simple_clusters[c(1:5, 74:78), ]
w_b_edges <- tibble(from = c(1, 2, 3, 4, 1, 2, 3, 1, 2, 1),
                    to =   c(2, 3, 4, 5, 3, 4, 5, 4, 5, 5))
w_b_edges <- bind_rows(w_b_edges,
              tibble(from = w_b_edges$from + 5,
                     to = w_b_edges$to + 5)       
                       )
w_b_segments <- tibble(x = w_b$x1[w_b_edges$from],
                       xend = w_b$x1[w_b_edges$to],
                       y = w_b$x2[w_b_edges$from],
                       yend = w_b$x2[w_b_edges$to])
ggplot() +
  geom_point(data=w_b, aes(x1, x2)) +
  geom_segment(data=w_b_segments, aes(x=x, xend=xend,
                                      y=y, yend=yend),
               colour="#3B99B1") + 
  geom_text(aes(x=-0.8, y=0, label="WITHIN"),
            colour="#3B99B1") +
  geom_text(aes(x=0.8, y=0.8, label="WITHIN"),
            colour="#3B99B1") +
  geom_segment(aes(x=-0.7, xend=0.7, y=-0.8, yend=1.2),
               colour="#F5191C") +
  geom_text(aes(x=0.2, y=0, label="BETWEEN"),
            colour="#F5191C") +
  theme(axis.text = element_blank(),
        axis.title = element_blank())
  
```

In an [idealistic setting]{.monash-orange2}, these are all [perfect]{.monash-orange2} metrics.

:::
::: {.column style="font-size: 70%;"}

::: {.fragment}
Remember the [world has many shapes]{.monash-orange2}:

```{r}
#| message: false
#| echo: false
#| out-width: 80%
#| fig-width: 6
#| fig-height: 6

stdd <- function(x) (x-mean(x))/sd(x)
set.seed(20230513)
d_sep_cl <- matrix(rnorm(99*2), ncol=2)
d_sep_cl[1:33,1] <-
  d_sep_cl[1:33,1]+8
d_sep_cl[34:66,2] <-
  d_sep_cl[34:66,2]+8
d_sep_cl[34:66,1] <-
  d_sep_cl[34:66,1]+4
d_sep_cl <- data.frame(x1=stdd(d_sep_cl[,1]), 
                       x2=stdd(d_sep_cl[,2]))

x <- (runif(20)-0.5)*4
y <- x
d_nuis_pts <- data.frame(x1 = stdd(c(rnorm(50, -3), 
                            rnorm(50, 3), x)),
                 x2 = stdd(c(rnorm(50, -3), 
                            rnorm(50, 3), y)))

d_nuis_vars <- matrix(rnorm(99*2), ncol=2)
d_nuis_vars[1:49,1] <- d_nuis_vars[1:49,1]+8
d_nuis_vars <-
  data.frame(x1=stdd(d_nuis_vars[,1]),
             x2=stdd(d_nuis_vars[,2]))

d_odd_shapes <- matrix(rnorm(99*2),ncol=2)
d_odd_shapes[1:66,2] <- (d_odd_shapes[1:66,1])^2-5 + rnorm(66)*0.6
d_odd_shapes[1:66,1] <- d_odd_shapes[1:66,1]*3
d_odd_shapes <-
  data.frame(x1=stdd(d_odd_shapes[,1]),
             x2=stdd(d_odd_shapes[,2]))

p1 <- ggplot(d_sep_cl, aes(x=x1, y=x2)) + 
  geom_point(colour="#3B99B1", alpha=0.7) + 
  annotate("text", -2.5, 2.5, label="a") +
  xlim(-2.8, 2.8) + ylim(-2.8, 2.8) +
    theme(
        axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank())
p2 <- ggplot(d_nuis_pts, aes(x=x1, y=x2)) + 
  geom_point(colour="#3B99B1", alpha=0.7) + 
  annotate("text", -2.5, 2.5, label="b") +
  xlim(-2.8, 2.8) + ylim(-2.8, 2.8) +
    theme(
        axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank())
p3 <- ggplot(d_nuis_vars, aes(x=x1, y=x2)) + 
  geom_point(colour="#3B99B1", alpha=0.7) + 
  annotate("text", -2.5, 2.5, label="c") +
  xlim(-2.8, 2.8) + ylim(-2.8, 2.8) +
    theme(
        axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank())
p4 <- ggplot(d_odd_shapes, aes(x=x1, y=x2)) + 
  geom_point(colour="#3B99B1", alpha=0.7) + 
  annotate("text", -2.5, 2.5, label="d") +
  xlim(-2.8, 2.8) + ylim(-2.8, 2.8) +
    theme(
        axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank())
p1 + p2 + p3 + p4 + plot_layout(ncol=2)

```
:::

:::
::::

## Cluster metrics [(2/2)]{.smallest}

:::: {.columns}
::: {.column}
Clustering algorithms not primarily looking for gaps. Gaps can be local or global in size, or just not exist. Clustering that partitions data can still be useful. Know how an algorithm will partition.

[What would happen to these data sets?]{.monash-blue2}

```{r}
#| echo: false
#| fig-width: 9
#| fig-height: 3
set.seed(914)
blob1 <- rmvnorm(n=155, mean=c(0,0), sigma=matrix(c(1, 0, 0, 1), ncol=2, byrow=TRUE)) |> as.tibble()
blob2 <- rmvnorm(n=155, mean=c(0,0), sigma=matrix(c(1, 0.6, 0.6, 1), ncol=2, byrow=TRUE)) |> as.tibble()
blob3 <- rmvnorm(n=155, mean=c(0,0), sigma=matrix(c(1, 0.9, 0.9, 1), ncol=2, byrow=TRUE)) |> as.tibble()
b1 <- ggplot(blob1, aes(V1, V2)) + 
  geom_point() +
  theme(axis.text = element_blank(),
        axis.title = element_blank())
b2 <- ggplot(blob2, aes(V1, V2)) + 
  geom_point() +
  theme(axis.text = element_blank(),
        axis.title = element_blank())
b3 <- ggplot(blob3, aes(V1, V2)) + 
  geom_point() +
  theme(axis.text = element_blank(),
        axis.title = element_blank())
b1 + b2 + b3 + plot_layout(ncol=3)
```
:::
::: {.column}

::: {.fragment}

```{r}
#| echo: false
#| fig-width: 9
#| fig-height: 3
set.seed(855)
b1_km <- kmeans(blob1, 4)
b2_km <- kmeans(blob2, 4)
b3_km <- kmeans(blob3, 4)
blob1_cl <- blob1 |>
  mutate(cl = factor(b1_km$cluster))
blob2_cl <- blob2 |>
  mutate(cl = factor(b2_km$cluster))
blob3_cl <- blob3 |>
  mutate(cl = factor(b3_km$cluster))
b4 <- ggplot(blob1_cl, aes(V1, V2, colour=cl)) + 
  geom_point() +
  scale_color_discrete_divergingx(palette="Zissou 1") +
  theme(legend.position = "none", 
        axis.text = element_blank(),
        axis.title = element_blank())
b5 <- ggplot(blob2_cl, aes(V1, V2, colour=cl)) + 
  geom_point() +
  scale_color_discrete_divergingx(palette="Zissou 1") +
  theme(legend.position = "none", 
        axis.text = element_blank(),
        axis.title = element_blank())
b6 <- ggplot(blob3_cl, aes(V1, V2, colour=cl)) + 
  geom_point() +
  scale_color_discrete_divergingx(palette="Zissou 1") +
  theme(legend.position = "none", 
        axis.text = element_blank(),
        axis.title = element_blank())
b4 + b5 + b6 + plot_layout(ncol=3)
```

<br><br>
The shape (strictly correlation here) strongly affects how an algorithm divides or partitions a blob.
:::

:::
::::
## Numerical summaries

:::: {.columns}
::: {.column width=30%}

- Means
- Standard deviations
- Variances and covariances: but unwieldy to print all
- Cluster size

:::
::: {.column width=70% style="font-size: 70%;"}

```{r}
#| echo: false
p_std_cl <- p_std |>
  select(-species) |>
  mutate(cl = factor(cutree(p_hc2, 3)))
p_std_cl_m <- p_std_cl |>
  group_by(cl) |>
  dplyr::summarise_at(vars(bl:bm), mean)
p_std_cl_s <- p_std_cl |>
  group_by(cl) |>
  dplyr::summarise_at(vars(bl:bm), sd)
p_std_cl_n <- p_std_cl |> count(cl)
# Organise into a nice table
p_std_cl_m <- p_std_cl_m |>
  pivot_longer(bl:bm, names_to="var", values_to="mean") |>
  pivot_wider(names_from=cl, values_from=mean) |>
  rename(m1 = `1`, m2 = `2`, m3 = `3`)
p_std_cl_s <- p_std_cl_s |>
  pivot_longer(bl:bm, names_to="var", values_to="sd") |>
  pivot_wider(names_from=cl, values_from=sd) |>
  rename(s1 = `1`, s2 = `2`, s3 = `3`)
p_std_cl_n <- p_std_cl_n |>
  pivot_wider(names_from=cl, values_from=n) 
p_std_cl_smry <- bind_cols(p_std_cl_m, p_std_cl_s[,-1]) |>
  pivot_longer(m1:s3, names_to = "stat", 
               values_to = "value") |>
  mutate(cl = str_sub(stat, 2, 2),
         stat = str_sub(stat, 1, 1)) |>
  pivot_wider(names_from=cl, values_from=value) |>
  mutate(stat = ifelse(stat == "m", "mean", "sd"))
p_std_cl_smry <- bind_rows(p_std_cl_smry, 
   tibble(var="", stat="n", p_std_cl_n))
p_std_cl_smry$var[c(2,4,6,8)] <- ""
kable(p_std_cl_smry, 
      col.names=c("Var", "Stat", "1", "2", "3"),
      digits = 2) |>
  kable_minimal() |>
  add_header_above(c(" " = 2, "Clusters" = 3)) |>
  row_spec(0, hline_after = TRUE) |>
  row_spec(1, extra_css = "border-bottom-style: none") |>
  row_spec(3, extra_css = "border-bottom-style: none") |>
  row_spec(5, extra_css = "border-bottom-style: none") |>
  row_spec(7, extra_css = "border-bottom-style: none") |>
  row_spec(2, italic=TRUE, color="#a7a7a7", extra_css = "") |>
  row_spec(4, italic=TRUE, color="#a7a7a7", extra_css = "") |>
  row_spec(6, italic=TRUE, color="#a7a7a7", extra_css = "") |>
  row_spec(8, italic=TRUE, color="#a7a7a7", extra_css = "")  
```

:::
::::

## Visual summaries

- [Statistics]{.monash-blue2}: Show the mean and variances for each cluster, as a parallel coordinate plot.
- [Model in the data space]{.monash-blue2}: 
    - Colour observations by their cluster label, and use multivariate plots.
    - [Dendrograms]{.monash-blue2}: Draw the connections of points being joined to form clusters as a high-d graph.
    - [Ellipses]{.monash-blue2}: As done in summarising model-based fit.
    - [Net]{.monash-blue2}: As done for SOM.
- [Convex hulls]{.monash-blue2}: Bounding shapes of each cluster, can be done in high-D, too.

## Statistics

:::: {.columns}
::: {.column}

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 4
p_std_cl_m <- p_std_cl_m |>
  pivot_longer(m1:m3, names_to = "cluster", 
               values_to = "mean") |>
  mutate(cluster = factor(as.numeric(str_sub(cluster, 2, 2))))
p_std_cl_s <- p_std_cl_s |>
  pivot_longer(s1:s3, names_to = "cluster", 
               values_to = "sd") |>
  mutate(cluster = factor(as.numeric(str_sub(cluster, 2, 2))))
p_std_cl_pcp <- bind_cols(p_std_cl_m, p_std_cl_s[,3]) |>
  mutate(var = factor(var))
ggplot(p_std_cl_pcp) +
  geom_ribbon(aes(x=as.numeric(var), 
                  ymin=mean-sd, 
                  ymax=mean+sd, 
                  fill=cluster), alpha=0.5) +
  geom_line(aes(x=as.numeric(var), 
                y=mean, 
                colour=cluster)) +
  scale_x_continuous("", labels=c("bl", "bd", "fl", "bm")) +
  ylab("") +
  scale_color_discrete_divergingx(palette="Zissou 1") +
  scale_fill_discrete_divergingx(palette="Zissou 1") +
  theme(aspect.ratio=0.7)
```

:::
::: {.column}

- Cluster 1 has [high values]{.monash-green2} of `bl` and [low values]{.monash-green2} of `bd`, `fl` and `bm`. 
- Cluster 2 has [low values]{.monash-green2} of `bl`,  and [high values]{.monash-green2} on `bd`, `fl` and `bm`. 
- Cluster 3 has [high values]{.monash-green2} of `bl`, `bd` and [low values]{.monash-green2} of `fl` and `bm`. 
:::
::::

## Model-in-the-data-space [(1/3)]{.smallest}

:::: {.columns}
::: {.column style="font-size: 70%;"}

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 8
#| out-width: 80%
ggscatmat(p_std_cl, columns=1:4, color = "cl") +
  scale_color_discrete_divergingx(palette="Zissou 1")
```

The clusters are neatly distinguished in different pairs of variables.
:::
::: {.column style="font-size: 70%;"}

```{r}
#| echo: false
#| eval: false
render_gif(p_std_cl[,1:4],
           grand_tour(),
           display_xy(col=p_std_cl$cl),
           gif_file = "../gifs/p_clusters.gif",
           width = 400,
           height = 400, 
           frames = 300)
```

<center>
![](../gifs/p_clusters.gif)
</center>

Cluster 2 is neatly separated. Clusters 1 and 3 are distinct but there is no gap.

:::
::::

## Model-in-the-data-space [(2/3)]{.smallest}

:::: {.columns}
::: {.column style="font-size: 60%;"}

The dendrogram can be drawn in the data space. 

- Add extra points at average between clusters indicating joins.
- Create variable specifying point type, "data" or "node".
- Add edges data, specifying which points are connected to make dendrogram.

```{r}
#| echo: false
#| out-width: 70%
#| fig-width: 6
#| fig-height: 6
data(simple_clusters)

# Compute hierarchical clustering with Ward's linkage
cl_hw <- hclust(dist(simple_clusters[,1:2]),
                method="ward.D2")
cl_ggd <- dendro_data(cl_hw, type = "triangle")

# Compute dendrogram in the data
cl_hfly <- hierfly(simple_clusters, cl_hw, scale=FALSE)

# Show result
simple_clusters <- simple_clusters |>
  mutate(clw = factor(cutree(cl_hw, 2)))

# Plot the data
pd <- ggplot(simple_clusters, aes(x=x1, y=x2)) +
  geom_point(colour="#3B99B1", size=2, alpha=0.8) +
  ggtitle("Data") + 
  theme_minimal() +
  theme(aspect.ratio=1) 

# Plot the dendrogram
ph <- ggplot() +
  geom_segment(data=cl_ggd$segments, 
               aes(x = x, y = y, 
                   xend = xend, yend = yend)) + 
  geom_point(data=cl_ggd$labels, aes(x=x, y=y),
             colour="#3B99B1", alpha=0.8) +
  ggtitle("Dendrogram") + 
  theme_minimal() +
  theme_dendro()

# Plot the dendrogram on the data
pdh <- ggplot() +
  geom_segment(data=cl_hfly$segments, 
                aes(x=x, xend=xend,
                    y=y, yend=yend)) +
  geom_point(data=cl_hfly$data, 
             aes(x=x1, y=x2,
                 shape=factor(node),
                 colour=factor(node),
                 size=1-node), alpha=0.8) +
  xlab("x1") + ylab("x2") +
  scale_shape_manual(values = c(16, 3)) +
  scale_colour_manual(values = c("#3B99B1", "black")) +
  scale_size(limits=c(0,17)) +
  ggtitle("Dendrogram on data") + 
  theme_minimal() +
  theme(aspect.ratio=1, legend.position="none")

# Plot the resulting clusters
pc <- ggplot(simple_clusters) +
  geom_point(aes(x=x1, y=x2, colour=clw), 
             size=2, alpha=0.8) +
  scale_colour_discrete_divergingx(palette = "Zissou 1",
                                   nmax=5, rev=TRUE) +
  ggtitle("Result") + 
  theme_minimal() +
  theme(aspect.ratio=1, legend.position="none")

pd + ph + pdh + pc + plot_layout(ncol=2)
```

:::
::: {.column style="font-size: 60%;"}

Why?

```{r}
#| echo: false
# Nuisance observations
set.seed(20190514)
x <- (runif(20)-0.5)*4
y <- x
d1 <- data.frame(x1 = c(rnorm(50, -3), 
                            rnorm(50, 3), x),
                 x2 = c(rnorm(50, -3), 
                            rnorm(50, 3), y),
                 cl = factor(c(rep("A", 50), 
                             rep("B", 70))))
d1 <- d1 |> 
  mutate_if(is.numeric, function(x) (x-mean(x))/sd(x))

# Compute single linkage
d1_hs <- hclust(dist(d1[,1:2]),
                method="single")
d1_ggds <- dendro_data(d1_hs, type = "triangle")
pd1s <- ggplot() +
  geom_segment(data=d1_ggds$segments, 
               aes(x = x, y = y, 
                   xend = xend, yend = yend)) + 
  geom_point(data=d1_ggds$labels, aes(x=x, y=y),
             colour="#3B99B1", alpha=0.8) +
  theme_minimal() +
  ggtitle("Single linkage") +
  theme_dendro()

# Compute dendrogram in data
d1_hflys <- hierfly(d1, d1_hs, scale=FALSE)

pd1hs <- ggplot() +
  geom_segment(data=d1_hflys$segments, 
                aes(x=x, xend=xend,
                    y=y, yend=yend)) +
  geom_point(data=d1_hflys$data, 
             aes(x=x1, y=x2,
                 shape=factor(node),
                 colour=factor(node),
                 size=1-node), alpha=0.8) +
  scale_shape_manual(values = c(16, 3)) +
  scale_colour_manual(values = c("#3B99B1", "black")) +
  scale_size(limits=c(0,17)) +
  theme_minimal() +
  theme(aspect.ratio=1, legend.position="none")

# Show result
d1 <- d1 |>
  mutate(cls = factor(cutree(d1_hs, 2)))
pc_d1s <- ggplot(d1) +
  geom_point(aes(x=x1, y=x2, colour=cls), 
             size=2, alpha=0.8) +
  scale_colour_discrete_divergingx(palette = "Zissou 1",
                                   nmax=4, rev=TRUE) +
  theme_minimal() +
  theme(aspect.ratio=1, legend.position="none")

# Compute Wards linkage
d1_hw <- hclust(dist(d1[,1:2]),
                method="ward.D2")
d1_ggdw <- dendro_data(d1_hw, type = "triangle")
pd1w <- ggplot() +
  geom_segment(data=d1_ggdw$segments, 
               aes(x = x, y = y, 
                   xend = xend, yend = yend)) + 
  geom_point(data=d1_ggdw$labels, aes(x=x, y=y),
             colour="#3B99B1", alpha=0.8) +
  ggtitle("Ward's linkage") +
  theme_minimal() +
  theme_dendro()

# Compute dendrogram in data
d1_hflyw <- hierfly(d1, d1_hw, scale=FALSE)

pd1hw <- ggplot() +
  geom_segment(data=d1_hflyw$segments, 
                aes(x=x, xend=xend,
                    y=y, yend=yend)) +
  geom_point(data=d1_hflyw$data, 
             aes(x=x1, y=x2,
                 shape=factor(node),
                 colour=factor(node),
                 size=1-node), alpha=0.8) +
  scale_shape_manual(values = c(16, 3)) +
  scale_colour_manual(values = c("#3B99B1", "black")) +
  scale_size(limits=c(0,17)) +
  theme_minimal() +
  theme(aspect.ratio=1, legend.position="none")

# Show result
d1 <- d1 |>
  mutate(clw = factor(cutree(d1_hw, 2)))
pc_d1w <- ggplot(d1) +
  geom_point(aes(x=x1, y=x2, colour=clw), 
             size=2, alpha=0.8) +
  scale_colour_discrete_divergingx(palette = "Zissou 1",
                                   nmax=4, rev=TRUE) +
  theme_minimal() +
  theme(aspect.ratio=1, legend.position="none")

pd1s + pd1hs + pc_d1s + 
  pd1w + pd1hw + pc_d1w +
  plot_layout(ncol=3)
```

Because it shows us how the algorithm is sequentially joining the observations, relative to their values. 

:::
::::

## Model-in-the-data-space [(3/3)]{.smallest}

:::: {.columns}
::: {.column width=30%}

```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 12
#| out-width: 80%
p_dist <- dist(p_std[,2:5])
p_hcw <- hclust(p_dist, method="ward.D2")
p_hcs <- hclust(p_dist, method="single")

p_clw <- p_std |> 
  mutate(cl = factor(cutree(p_hcw, 3))) |>
  as.data.frame()
p_cls <- p_std |> 
  mutate(cl = factor(cutree(p_hcs, 3))) |>
  as.data.frame()

p_w_hfly <- hierfly(p_clw, p_hcw, scale=FALSE)
p_s_hfly <- hierfly(p_cls, p_hcs, scale=FALSE)
# Generate the dendrograms in 2D
p_hcw_dd <- dendro_data(p_hcw)
pw_dd <- ggplot() +
  geom_segment(data=p_hcw_dd$segments, 
               aes(x = x, y = y, 
                   xend = xend, yend = yend)) + 
  geom_point(data=p_hcw_dd$labels, aes(x=x, y=y),
             alpha=0.8) +
  ggtitle("Ward's linkage") +
  theme_dendro()

p_hcs_dd <- dendro_data(p_hcs)
ps_dd <- ggplot() +
  geom_segment(data=p_hcs_dd$segments, 
               aes(x = x, y = y, 
                   xend = xend, yend = yend)) + 
  geom_point(data=p_hcs_dd$labels, aes(x=x, y=y),
             alpha=0.8) +
  ggtitle("Single linkage") +
  theme_dendro()
ps_dd + pw_dd + plot_layout(ncol=1)
```

:::
::: {.column width=30%}
::: {.fragment}

![](https://dicook.github.io/mulgar_book/gifs/penguins_hflys.gif){width=370}

![](https://dicook.github.io/mulgar_book/gifs/penguins_hflyw.gif){width=370}

:::
:::
::: {.column width=30%}
::: {.fragment}

![](https://dicook.github.io/mulgar_book/gifs/penguins_s3.gif){width=370}

![](https://dicook.github.io/mulgar_book/gifs/penguins_w3.gif){width=370}

:::
:::
::::

## Convex hulls

:::: {.columns}
::: {.column style="font-size: 70%;"}

Convex hulls are a common method for visually displaying the region in the data space corresponding to each cluster. It avoids making any assumption about the shapes.

```{r}
#| echo: false
p_dist <- dist(p_std[,2:5])
p_hcw <- hclust(p_dist, method="ward.D2")

p_cl <- data.frame(cl_w = cutree(p_hcw, 3))


# Arranging by cluster id is important to define edges 
penguins_cl <- p_std |>
  mutate(cl_w = p_cl$cl_w) |>
  arrange(cl_w)
```

```{r}
#| echo: false
# Penguins in 2D
# Duplicate observations need to be removed for convex hull calculation
psub <- penguins_cl |>
  select(bl, bd) 
dup <- duplicated(psub)
psub <- penguins_cl |>
  select(bl, bd, cl_w) |>
  filter(!dup) |>
  arrange(cl_w)

ncl <- psub |>
  count(cl_w) |>
  arrange(cl_w) |>
  mutate(cumn = cumsum(n))
phull <- NULL
for (i in unique(psub$cl_w)) {
  x <- psub |>
    dplyr::filter(cl_w == i) |>
    select(bl, bd) 
  ph <- cxhull(as.matrix(x))$edges
  if (i > 1) {
    ph <- ph + ncl$cumn[i-1]
  }
  ph <- cbind(ph, rep(i, nrow(ph)))
  phull <- rbind(phull, ph)
}
phull <- as.data.frame(phull)
colnames(phull) <- c("from", "to", "cl_w") 
phull_segs <- data.frame(x = psub$bl[phull$from],
                         y = psub$bd[phull$from],
                         xend = psub$bl[phull$to],
                         yend = psub$bd[phull$to],
                         cl_w = phull$cl_w)
phull_segs$cl_w <- factor(phull$cl_w) 
psub$cl_w <- factor(psub$cl_w)
ggplot() +
  geom_point(data=psub, aes(x=bl, y=bd, 
                            colour=cl_w)) + 
  geom_segment(data=phull_segs, aes(x=x, xend=xend,
                                    y=y, yend=yend,
                                    colour=cl_w)) +
  scale_colour_discrete_divergingx(palette = "Zissou 1") +
  theme_minimal() +
  theme(aspect.ratio = 1)

```
:::
::: {.column style="font-size: 70%;"}
::: {.fragment}

Convex hulls are defined in $p$ dimensions also.

```{r}
#| echo: false
#| eval: false
ncl <- penguins_cl |>
  count(cl_w) |>
  arrange(cl_w) |>
  mutate(cumn = cumsum(n))
phull <- NULL
for (i in unique(penguins_cl$cl_w)) {
  x <- penguins_cl |>
    dplyr::filter(cl_w == i) 
  ph <- cxhull(as.matrix(x[,2:5]))$edges
  if (i > 1) {
    ph <- ph + ncl$cumn[i-1]
  }
  ph <- cbind(ph, rep(i, nrow(ph)))
  phull <- rbind(phull, ph)
}
phull <- as.data.frame(phull)
colnames(phull) <- c("from", "to", "cl_w") 
phull$cl_w <- factor(phull$cl_w)
penguins_cl$cl_w <- factor(penguins_cl$cl_w)

animate_xy(penguins_cl[,2:5], col=penguins_cl$cl_w,
           edges=as.matrix(phull[,1:2]), edges.col=phull$cl_w)
render_gif(penguins_cl[,2:5], 
           tour_path = grand_tour(),
           display = display_xy(col=penguins_cl$cl_w,
                                edges=as.matrix(phull[,1:2]),
                                edges.col=phull$cl_w),
           gif_file = "gifs/penguins_chull.gif",
           frames = 500, 
           width = 400,
           height = 400)
```

<center>
![](https://dicook.github.io/mulgar_book/gifs/penguins_chull.gif){width=400}
</center>

Are clusters 1 and 3 separated/distinct?

:::
:::
::::

## Low-dimensional representations

:::: {.columns}
::: {.column}

- PCA, but not PCA
- Perhaps, projection pursuit
- Nonlinear dimension reduction
- Self-organising map
- When you have a clustering, linear discriminant analysis
:::
::: {.column style="font-size: 70%;"}

::: {.fragment}
```{r}
#| echo: false
options(digits=2, width=100)
c1_pca <- prcomp(c1, scale = FALSE)
summary(c1_pca)
```

Proportion of total variance is 0.97 with 2 PCs. But if you only used 2 PCs, you would miss the clusters visible only with PC4. 

```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 6
#| out-width: 70%
c1_pca_df <- as_tibble(c1_pca$x)
ggscatmat(c1_pca_df) + 
  theme(axis.text = element_blank())
```

:::
:::
::::

## How do you know you got it right?

*All "models" are wrong by some are useful.*

- Are the subsets useful, eg if you market differently to the different subsets do you get more purchases than marketing the same way to all.
- Can you reduce the number of parameters needed in a high-dimensional regression model, by grouping genomes into a smaller set?
- Does it match the shape of the data?
- Bootstrap or CV
- Domain knowledge and interpretability

## Wrapping up {.transition-slide .center}

## What we have covered

:::: {.columns}
::: {.column}
- [Computational and statistical models for a categorical response]{.monash-blue2}: logistic, discriminant, trees, forests, support vectors, multilayer perceptron neural network
- [Discovering class labels by clustering]{.monash-blue2}: $k$-means, hierarchical, model-based, self-organising
- [Dimension reduction]{.monash-blue2}: princi**pal** components, multidimensional scaling t-SNE, UMAP
- [Visualising your high-dimensional data and models]{.monash-blue2}

:::

::: {.column}
::: {.fragment}
**Not covered**

- *Continuous, numerical response*: $\longrightarrow$ ETC2420/5242, ETC3580/5580
    - Fit statistics easier: MSE, AIC, $R^2$, but not all applicable for categorical response
    - Visualisation a little harder
- *Full range of neural networks*: convolutional, recurrent, autoencoder, generative adversarial, recursive, ... $\longrightarrow$ ETC5555
:::
:::
::::

## Philosophy {.center}

<br><br><br>
Some technicians brag that they can pull an appliance into parts. That is not as impressive as the technician who can put it back together, and make it work. I would hire this person in a heartbeat. [*~Raymond Bruce Cook*]{.smallest}
<br><br>



## Preparing for the exam

:::: {.columns}
::: {.column}
[The exam is like]{.monash-orange2} the [quizzes]{.monash-blue2}, [tutorial]{.monash-blue2} and [assignment exercises]{.monash-blue2} all together, without the programming elements. 

<br>
<br>
It is open. You can use whatever resources you like during the exam, including AI. 

<br>
It is supervised. You can only take one device.

<br>
Open exams need solid preparation.
:::
::: {.column style="font-size: 80%;"}

::: {.fragment}
Ways to prepare:

- *Review your quizzes*.
- *Review the tutorial and assignment materials*: what results mean, what problems are detected in the model, conceptual thinking, and explanations of results and diagnostics.
- [Resources](https://iml.numbat.space/resources.html) provided have several books with exercises at the end of chapters. Work your way through some where you feel you don't know the topic well enough.
- *Form a small study group*: make up questions for each other to tackle, compare and contrast your answers.
- *Try asking GAI questions*, and evaluate the answers as useful or misguided based on your learning.
:::

:::

::::
## Next: What the ETC5250 students have learned from the kaggle challenge {.transition-slide .center}


