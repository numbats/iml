[
  {
    "objectID": "next-time.html",
    "href": "next-time.html",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "",
    "text": "Quizzes\nq02_6a,b Write out matrices as\nhtml"
  },
  {
    "objectID": "week8/index.html",
    "href": "week8/index.html",
    "title": "Week 8: Support vector machines and nearest neighbours",
    "section": "",
    "text": "ISLR 9.1-9.3"
  },
  {
    "objectID": "week8/index.html#main-reference",
    "href": "week8/index.html#main-reference",
    "title": "Week 8: Support vector machines and nearest neighbours",
    "section": "",
    "text": "ISLR 9.1-9.3"
  },
  {
    "objectID": "week8/index.html#what-you-will-learn-this-week",
    "href": "week8/index.html#what-you-will-learn-this-week",
    "title": "Week 8: Support vector machines and nearest neighbours",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nSeparating hyperplanes\nNon-linear kernels\nNaive models using nearest neighbours"
  },
  {
    "objectID": "week8/index.html#assignments",
    "href": "week8/index.html#assignments",
    "title": "Week 8: Support vector machines and nearest neighbours",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 3 is due on Friday 26 April."
  },
  {
    "objectID": "week6/index.html",
    "href": "week6/index.html",
    "title": "Week 6: Neural networks and deep learning",
    "section": "",
    "text": "ISLR 10.1-10.3, 10.7"
  },
  {
    "objectID": "week6/index.html#main-reference",
    "href": "week6/index.html#main-reference",
    "title": "Week 6: Neural networks and deep learning",
    "section": "",
    "text": "ISLR 10.1-10.3, 10.7"
  },
  {
    "objectID": "week6/index.html#what-you-will-learn-this-week",
    "href": "week6/index.html#what-you-will-learn-this-week",
    "title": "Week 6: Neural networks and deep learning",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nStructure of a neural network\nFitting neural networks\nDiagnosing the fit"
  },
  {
    "objectID": "week6/index.html#assignments",
    "href": "week6/index.html#assignments",
    "title": "Week 6: Neural networks and deep learning",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 2 is due on Friday 12 April.\nAssignment 3 is due on Friday 26 April."
  },
  {
    "objectID": "week4/index.html",
    "href": "week4/index.html",
    "title": "Week 4: Logistic regression and discriminant analysis",
    "section": "",
    "text": "ISLR 4.3, 4.4"
  },
  {
    "objectID": "week4/index.html#main-reference",
    "href": "week4/index.html#main-reference",
    "title": "Week 4: Logistic regression and discriminant analysis",
    "section": "",
    "text": "ISLR 4.3, 4.4"
  },
  {
    "objectID": "week4/index.html#what-you-will-learn-this-week",
    "href": "week4/index.html#what-you-will-learn-this-week",
    "title": "Week 4: Logistic regression and discriminant analysis",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nFitting a categorical response using logistic curves\nMultivariate summary statistics\nLinear discriminant analysis, assuming samples are elliptically shaped and equal in size\nQuadratic discriminant analysis, assuming samples are elliptically shaped and different in size\nDiscriminant space: making a low-dimensional visual summary"
  },
  {
    "objectID": "week4/index.html#assignments",
    "href": "week4/index.html#assignments",
    "title": "Week 4: Logistic regression and discriminant analysis",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 1 is due on Friday 22 March.\nAssignment 2 is due on Friday 12 April."
  },
  {
    "objectID": "week3/index.html",
    "href": "week3/index.html",
    "title": "Week 3: Re-sampling and regularisation",
    "section": "",
    "text": "ISLR 5.1, 5.2, 6.2, 6.4"
  },
  {
    "objectID": "week3/index.html#main-reference",
    "href": "week3/index.html#main-reference",
    "title": "Week 3: Re-sampling and regularisation",
    "section": "",
    "text": "ISLR 5.1, 5.2, 6.2, 6.4"
  },
  {
    "objectID": "week3/index.html#what-you-will-learn-this-week",
    "href": "week3/index.html#what-you-will-learn-this-week",
    "title": "Week 3: Re-sampling and regularisation",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nCommon re-sampling methods: bootstrap, cross-validation, permutation\nCross-validation for parameter tuning\nBootstrapping for understanding variance of parameter estimates\nPermutation to understand significance of associations between variables, and variable importance\nWhat can go wrong in high-d, and how to adjust using regularisation methods"
  },
  {
    "objectID": "week3/index.html#lecture-slides",
    "href": "week3/index.html#lecture-slides",
    "title": "Week 3: Re-sampling and regularisation",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week3/index.html#tutorial-instructions",
    "href": "week3/index.html#tutorial-instructions",
    "title": "Week 3: Re-sampling and regularisation",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:\n\nhtml\nqmd"
  },
  {
    "objectID": "week3/index.html#assignments",
    "href": "week3/index.html#assignments",
    "title": "Week 3: Re-sampling and regularisation",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week3/index.html#assignments-1",
    "href": "week3/index.html#assignments-1",
    "title": "Week 3: Re-sampling and regularisation",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 1 is due on Friday 22 March."
  },
  {
    "objectID": "week2/slides.html#overview",
    "href": "week2/slides.html#overview",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Overview",
    "text": "Overview\nIn this week we will cover:\n\nConceptual framing for visualisation\nCommon methods: scatterplot matrix, parallel coordinates, tours\nDetails on using tours for examining clustering and class structure\nDimension reduction\n\nLinear: principal component analysis\nNon-linear: multidimensional scaling, t-stochastic neighbour embedding (t-SNE), uniform manifold approximation and projection (UMAP)\n\nUsing tours to assess dimension reduction"
  },
  {
    "objectID": "week2/slides.html#concepts",
    "href": "week2/slides.html#concepts",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Concepts",
    "text": "Concepts"
  },
  {
    "objectID": "week2/slides.html#model-in-the-data-space",
    "href": "week2/slides.html#model-in-the-data-space",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Model-in-the-data-space",
    "text": "Model-in-the-data-space\n\n\n\n\n\nFrom XKCD\n\n\n\n We plot the model on the data to assess whether it fits or is a misfit!\n\n\nDoing this in high-dimensions is considered difficult!\n\n\nSo it is common to only plot the data-in-the-model-space."
  },
  {
    "objectID": "week2/slides.html#data-in-the-model-space",
    "href": "week2/slides.html#data-in-the-model-space",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Data-in-the-model-space",
    "text": "Data-in-the-model-space\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive probabilities are aspects of the model. It is useful to plot. What do we learn here?\n\nBut it doesnâ€™t tell you why there is a difference."
  },
  {
    "objectID": "week2/slides.html#model-in-the-data-space-1",
    "href": "week2/slides.html#model-in-the-data-space-1",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Model-in-the-data-space",
    "text": "Model-in-the-data-space\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel is displayed, as a grid of predicted points in the original variable space. Data is overlaid, using text labels. What do you learn?\n\nOne model has a linear boundary, and the other has the highly non-linear boundary, which matches the class cluster better. Also â€¦"
  },
  {
    "objectID": "week2/slides.html#how-do-you-visualise-beyond-2d",
    "href": "week2/slides.html#how-do-you-visualise-beyond-2d",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "How do you visualise beyond 2D?",
    "text": "How do you visualise beyond 2D?"
  },
  {
    "objectID": "week2/slides.html#scatterplot-matrix",
    "href": "week2/slides.html#scatterplot-matrix",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Scatterplot matrix",
    "text": "Scatterplot matrix\n\n\n Start simply! Make static plots that organise the variables on a page. \nPlot all the pairs of variables. When laid out in a matrix format this is called a scatterplot matrix.\n Here, we see linear association, clumping and clustering, potentially some outliers."
  },
  {
    "objectID": "week2/slides.html#scatterplot-matrix-drawbacks",
    "href": "week2/slides.html#scatterplot-matrix-drawbacks",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Scatterplot matrix: drawbacks",
    "text": "Scatterplot matrix: drawbacks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is an outlier in the data on the right, like the one in the left, but it is hidden in a combination of variables. Itâ€™s not visible in any pair of variables."
  },
  {
    "objectID": "week2/slides.html#perception",
    "href": "week2/slides.html#perception",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Perception",
    "text": "Perception\n\nAspect ratio for scatterplots needs to be equal, or square!\n\nWhen you make a scatterplot of two variables from a multivariate data set, most software renders it with an unequal aspect ratio, as a rectangle. You need to over-ride this and force the square aspect ratio. Why?\n\n\n\nBecause it adversely affects the perception of correlation and association between variables."
  },
  {
    "objectID": "week2/slides.html#parallel-coordinate-plot",
    "href": "week2/slides.html#parallel-coordinate-plot",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Parallel coordinate plot",
    "text": "Parallel coordinate plot\n\n\n\nggparcoord(p_tidy, columns = 2:5, alphaLines = 0.5) + \n  xlab(\"\") + ylab(\"\") + \n  theme(aspect.ratio=0.8)\n\n\n\n\n\n\n\n\n\n Parallel coordinate plots are side-by-side dotplots with values from a row connected with a line.\nExamine the direction and orientation of lines to perceive multivariate relationships.\nCrossing lines indicate negative association. Lines with same slope indicate positive association. Outliers have a different up/down pattern to other points. Groups of lines with same pattern indicate clustering."
  },
  {
    "objectID": "week2/slides.html#parallel-coordinate-plot-drawbacks",
    "href": "week2/slides.html#parallel-coordinate-plot-drawbacks",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Parallel coordinate plot: drawbacks",
    "text": "Parallel coordinate plot: drawbacks\n\n\n\nHard to follow lines - need interactivity\nOrder of variables\nScaling of variables\n\n\n\nBut the advantage is that you can pack a lot of variables into the single page."
  },
  {
    "objectID": "week2/slides.html#parallel-coordinate-plot-effect-of-scaling",
    "href": "week2/slides.html#parallel-coordinate-plot-effect-of-scaling",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Parallel coordinate plot: effect of scaling",
    "text": "Parallel coordinate plot: effect of scaling\n\n\n\nggparcoord(p_tidy, columns = 2:5, alphaLines = 0.5,\n           scale=\"globalminmax\") + \n  xlab(\"\") + ylab(\"\") + \n  theme(aspect.ratio=0.8)\n\n\n\n\n\n\n\n\n\n\nggparcoord(p_tidy, columns = 2:5, alphaLines = 0.5,\n           scale=\"uniminmax\") + \n  xlab(\"\") + ylab(\"\") + \n  theme(aspect.ratio=0.8)"
  },
  {
    "objectID": "week2/slides.html#parallel-coordinate-plot-effect-of-ordering",
    "href": "week2/slides.html#parallel-coordinate-plot-effect-of-ordering",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Parallel coordinate plot: effect of ordering",
    "text": "Parallel coordinate plot: effect of ordering\n\n\n\nggparcoord(p_tidy, columns = 2:5, alphaLines = 0.5,\n           groupColumn = 1) + \n  scale_color_discrete_divergingx(palette = \"Zissou 1\") +\n  xlab(\"\") + ylab(\"\") +\n  theme(legend.position=\"none\", aspect.ratio=0.8)\n\n\n\n\n\n\n\n\n\n\nggparcoord(p_tidy, columns = 2:5, alphaLines = 0.5,\n           groupColumn = 1, order=c(4, 2, 5, 3)) + \n  scale_color_discrete_divergingx(palette = \"Zissou 1\") +\n  xlab(\"\") + ylab(\"\") +\n  theme(legend.position=\"none\", aspect.ratio=0.8)"
  },
  {
    "objectID": "week2/slides.html#adding-interactivity-to-static-plots-scatterplot-matrix",
    "href": "week2/slides.html#adding-interactivity-to-static-plots-scatterplot-matrix",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Adding interactivity to static plots: scatterplot matrix",
    "text": "Adding interactivity to static plots: scatterplot matrix\n\n\n\nlibrary(plotly)\ng &lt;- ggpairs(p_tidy, columns=2:5) +\n  theme(axis.text = element_blank()) \n\n Selecting points, using plotly, allows you to see where this observation lies in the other plots (pairs of variables).\n\n\nggplotly(g, width=600, height=600)"
  },
  {
    "objectID": "week2/slides.html#adding-interactivity-to-static-plots-parallel-coordinates",
    "href": "week2/slides.html#adding-interactivity-to-static-plots-parallel-coordinates",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Adding interactivity to static plots: parallel coordinates",
    "text": "Adding interactivity to static plots: parallel coordinates\n\n\n\np_pcp &lt;- p_tidy |&gt;\n  na.omit() |&gt;\n  plot_ly(type = 'parcoords',\n          line = list(),\n          dimensions = list(\n            list(range = c(172, 231),\n                 label = 'fl', values = ~fl),\n            list(range = c(32, 60),\n                 label = 'bl', values = ~bl),\n           list(range = c(2700, 6300),\n                 label = 'bm', values = ~bm),\n            list(range = c(13, 22),\n                 label = 'bd', values = ~bd)\n             )\n          )\n\n\n\np_pcp"
  },
  {
    "objectID": "week2/slides.html#what-is-high-dimensions",
    "href": "week2/slides.html#what-is-high-dimensions",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "What is high-dimensions?",
    "text": "What is high-dimensions?"
  },
  {
    "objectID": "week2/slides.html#high-dimensions-in-statistics",
    "href": "week2/slides.html#high-dimensions-in-statistics",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "High-dimensions in statistics",
    "text": "High-dimensions in statistics\n\n\n\nIncreasing dimension adds an additional orthogonal axis.\n\nIf you want more high-dimensional shapes there is an R package, geozoo, which will generate cubes, spheres, simplices, mobius strips, torii, boy surface, klein bottles, cones, various polytopes, â€¦\nAnd read or watch Flatland: A Romance of Many Dimensions (1884) Edwin Abbott."
  },
  {
    "objectID": "week2/slides.html#remember",
    "href": "week2/slides.html#remember",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Remember",
    "text": "Remember\nData\n\\[\\begin{eqnarray*}\nX_{~n\\times p} =\n[X_{~1}~X_{~2}~\\dots~X_{~p}]_{~n\\times p} = \\left[ \\begin{array}{cccc}\nx_{~11} & x_{~12} & \\dots & x_{~1p} \\\\\nx_{~21} & x_{~22} & \\dots & x_{~2p}\\\\\n\\vdots & \\vdots &  & \\vdots \\\\\nx_{~n1} & x_{~n2} & \\dots & x_{~np} \\end{array} \\right]_{~n\\times p}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "week2/slides.html#remember-1",
    "href": "week2/slides.html#remember-1",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Remember",
    "text": "Remember\nProjection\n\\[\\begin{eqnarray*}\nA_{~p\\times d} = \\left[ \\begin{array}{cccc}\na_{~11} & a_{~12} & \\dots & a_{~1d} \\\\\na_{~21} & a_{~22} & \\dots & a_{~2d}\\\\\n\\vdots & \\vdots &  & \\vdots \\\\\na_{~p1} & a_{~p2} & \\dots & a_{~pd} \\end{array} \\right]_{~p\\times d}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "week2/slides.html#remember-2",
    "href": "week2/slides.html#remember-2",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Remember",
    "text": "Remember\nProjected data\n\\[\\begin{eqnarray*}\nY_{~n\\times d} = XA = \\left[ \\begin{array}{cccc}\ny_{~11} & y_{~12} & \\dots & y_{~1d} \\\\\ny_{~21} & y_{~22} & \\dots & y_{~2d}\\\\\n\\vdots & \\vdots &  & \\vdots \\\\\ny_{~n1} & y_{~n2} & \\dots & y_{~nd} \\end{array} \\right]_{~n\\times d}\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "week2/slides.html#tours-of-linear-projections",
    "href": "week2/slides.html#tours-of-linear-projections",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Tours of linear projections",
    "text": "Tours of linear projections\n\n\n\n\nData is 2D: \\(~~p=2\\)\nProjection is 1D: \\(~~d=1\\)\n\n\\[\\begin{eqnarray*}\nA_{~2\\times 1} = \\left[ \\begin{array}{c}\na_{~11} \\\\\na_{~21}\\\\\n\\end{array} \\right]_{~2\\times 1}\n\\end{eqnarray*}\\]\n\n\n Notice that the values of \\(A\\) change between (-1, 1). All possible values being shown during the tour.\n  \n \\[\\begin{eqnarray*}\nA = \\left[ \\begin{array}{c}\n1 \\\\\n0\\\\\n\\end{array} \\right]\n~~~~~~~~~~~~~~~~\nA = \\left[ \\begin{array}{c}\n0.7 \\\\\n0.7\\\\\n\\end{array} \\right]\n~~~~~~~~~~~~~~~~\nA = \\left[ \\begin{array}{c}\n0.7 \\\\\n-0.7\\\\\n\\end{array} \\right]\n\n\\end{eqnarray*}\\]\n\n\n watching the 1D shadows we can see:\n\nunimodality\nbimodality, there are two clusters.\n\n\n\n What does the 2D data look like? Can you sketch it?"
  },
  {
    "objectID": "week2/slides.html#tours-of-linear-projections-1",
    "href": "week2/slides.html#tours-of-linear-projections-1",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Tours of linear projections",
    "text": "Tours of linear projections\n\n\n\n\n\n\n\n\n\n\n\n\n  âŸµ  The 2D data"
  },
  {
    "objectID": "week2/slides.html#tours-of-linear-projections-2",
    "href": "week2/slides.html#tours-of-linear-projections-2",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Tours of linear projections",
    "text": "Tours of linear projections\n\n\n\nData is 3D: \\(p=3\\)\nProjection is 2D: \\(d=2\\)\n\\[\\begin{eqnarray*}\nA_{~3\\times 2} = \\left[ \\begin{array}{cc}\na_{~11} & a_{~12} \\\\\na_{~21} & a_{~22}\\\\\na_{~31} & a_{~32}\\\\\n\\end{array} \\right]_{~3\\times 2}\n\\end{eqnarray*}\\]\n\n\n Notice that the values of \\(A\\) change between (-1, 1). All possible values being shown during the tour.\n\n\nSee:\n\ncircular shapes\nsome transparency, reveals middle\nhole in in some projections\nno clustering"
  },
  {
    "objectID": "week2/slides.html#tours-of-linear-projections-3",
    "href": "week2/slides.html#tours-of-linear-projections-3",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Tours of linear projections",
    "text": "Tours of linear projections\n\n\n\nData is 4D: \\(p=4\\)\nProjection is 2D: \\(d=2\\)\n\\[\\begin{eqnarray*}\nA_{~4\\times 2} = \\left[ \\begin{array}{cc}\na_{~11} & a_{~12} \\\\\na_{~21} & a_{~22}\\\\\na_{~31} & a_{~32}\\\\\na_{~41} & a_{~42}\\\\\n\\end{array} \\right]_{~4\\times 2}\n\\end{eqnarray*}\\]\n\n How many clusters do you see?\n\n\nthree, right?\none separated, and two very close,\nand they each have an elliptical shape.\n\n\n\n\ndo you also see an outlier or two?"
  },
  {
    "objectID": "week2/slides.html#intuitively-tours-are-like",
    "href": "week2/slides.html#intuitively-tours-are-like",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Intuitively, tours are like â€¦",
    "text": "Intuitively, tours are like â€¦"
  },
  {
    "objectID": "week2/slides.html#and-help-to-see-the-datamodel-as-a-whole",
    "href": "week2/slides.html#and-help-to-see-the-datamodel-as-a-whole",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "And help to see the data/model as a whole",
    "text": "And help to see the data/model as a whole\n\n\nAvoid misinterpretation â€¦\n\n\n\n\n\n\nâ€¦ see the bigger picture!\n\n\n\n\n\n\n\n\nImage: Sketchplanations."
  },
  {
    "objectID": "week2/slides.html#anomaly-is-no-longer-hidden",
    "href": "week2/slides.html#anomaly-is-no-longer-hidden",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Anomaly is no longer hidden",
    "text": "Anomaly is no longer hidden\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWait for it!"
  },
  {
    "objectID": "week2/slides.html#how-to-use-a-tour-in-r",
    "href": "week2/slides.html#how-to-use-a-tour-in-r",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "How to use a tour in R",
    "text": "How to use a tour in R\n\n\nThis is a basic tour, which will run in your RStudio plot window.\n\nlibrary(tourr)\nanimate_xy(flea[, 1:6], rescale=TRUE)\n\n\n This data has a class variable, species.\n\n\nflea |&gt; slice_head(n=3)\n\n   species tars1 tars2 head aede1 aede2 aede3\n1 Concinna   191   131   53   150    15   104\n2 Concinna   185   134   50   147    13   105\n3 Concinna   200   137   52   144    14   102\n\n\n\nUse this to colour points with:\n\nanimate_xy(flea[, 1:6], \n           col = flea$species, \n           rescale=TRUE)\n\n\n\n\nYou can specifically guide the tour choice of projections using\n\nanimate_xy(flea[, 1:6], \n           tour_path = guided_tour(holes()), \n           col = flea$species, \n           rescale = TRUE, \n           sphere = TRUE)\n\n\n\n and you can manually choose a variable to control with:\n\nset.seed(915)\nanimate_xy(flea[, 1:6], \n           radial_tour(basis_random(6, 2), \n                       mvar = 6), \n           rescale = TRUE,\n           col = flea$species)"
  },
  {
    "objectID": "week2/slides.html#how-to-save-a-tour",
    "href": "week2/slides.html#how-to-save-a-tour",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "How to save a tour",
    "text": "How to save a tour\n\n\n\n\n\n\nTo save as an animated gif:\n\nset.seed(645)\nrender_gif(penguins_sub[,1:4],\n           grand_tour(),\n           display_xy(col=\"#EC5C00\",\n             half_range=3.8, \n             axes=\"bottomleft\", cex=2.5),\n           gif_file = \"../gifs/penguins1.gif\",\n           apf = 1/60,\n           frames = 1500,\n           width = 500, \n           height = 400)"
  },
  {
    "objectID": "week2/slides.html#dimension-reduction",
    "href": "week2/slides.html#dimension-reduction",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Dimension reduction",
    "text": "Dimension reduction"
  },
  {
    "objectID": "week2/slides.html#pca",
    "href": "week2/slides.html#pca",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "PCA",
    "text": "PCA\n\n\nFor this 2D data, sketch a line or a direction that if you squashed the data into it would provide most of the information.\n\n\n\n\n\n\n\n\n\n\n\n What about this data?"
  },
  {
    "objectID": "week2/slides.html#pca-1",
    "href": "week2/slides.html#pca-1",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "PCA",
    "text": "PCA\n\nPrincipal component analysis (PCA) produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated. It is an unsupervised learning method.\n\nUse it, when:\n\nYou have too many predictors for a regression. Instead, we can use the first few principal components.\nNeed to understand relationships between variables.\nTo make plots summarising the variation in a large number of variables."
  },
  {
    "objectID": "week2/slides.html#first-principal-component",
    "href": "week2/slides.html#first-principal-component",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "First principal component",
    "text": "First principal component\nThe first principal component is a new variable created from a linear combination\n\\[z_1 = \\phi_{11}x_1 + \\phi_{21} x_2 + \\dots + \\phi_{p1} x_p\\]\nof the original \\(x_1, x_2, \\dots, x_p\\) that has the largest variance. The elements \\(\\phi_{11},\\dots,\\phi_{p1}\\) are the loadings of the first principal component and are constrained by:\n\\[\n\\displaystyle\\sum_{j=1}^p \\phi^2_{j1} = 1\n\\]"
  },
  {
    "objectID": "week2/slides.html#calculation",
    "href": "week2/slides.html#calculation",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Calculation",
    "text": "Calculation\n\nThe loading vector \\(\\phi_1 = [\\phi_{11},\\dots,\\phi_{p1}]^\\top\\) defines direction in feature space along which data vary most.\nIf we project the \\(n\\) data points \\({x}_1,\\dots,{x}_n\\) onto this direction, the projected values are the principal component scores \\(z_{11},\\dots,z_{n1}\\).\n\n\n\n\nThe second principal component is the linear combination \\(z_{i2} = \\phi_{12}x_{i1} + \\phi_{22}x_{i2} + \\dots + \\phi_{p2}x_{ip}\\) that has maximal variance among all linear combinations that are uncorrelated with \\(z_1\\).\nEquivalent to constraining \\(\\phi_2\\) to be orthogonal (perpendicular) to \\(\\phi_1\\). And so on.\nThere are at most \\(\\min(n - 1, p)\\) PCs."
  },
  {
    "objectID": "week2/slides.html#example",
    "href": "week2/slides.html#example",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Example",
    "text": "Example\n\n  \n\nIf you think of the first few PCs like a linear model fit, and the others as the error, it is like regression, except that errors are orthogonal to model.\n(Chapter6/6.15.pdf)"
  },
  {
    "objectID": "week2/slides.html#geometry",
    "href": "week2/slides.html#geometry",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Geometry",
    "text": "Geometry\nPCA can be thought of as fitting an \\(n\\)-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. The new variables produced by principal components correspond to rotating and scaling the ellipse into a circle. It spheres the data."
  },
  {
    "objectID": "week2/slides.html#computation",
    "href": "week2/slides.html#computation",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Computation",
    "text": "Computation\nSuppose we have a \\(n\\times p\\) data set \\(X = [x_{ij}]\\).\n\nCentre each of the variables to have mean zero (i.e., the column means of \\({X}\\) are zero).\nLet \\(z_{i1} = \\phi_{11}x_{i1} + \\phi_{21} x_{i2} + \\dots + \\phi_{p1} x_{ip}\\)\nCompute sample variance of \\(z_{i1}\\) is \\(\\displaystyle\\frac1n\\sum_{i=1}^n z_{i1}^2\\).\nEstimate \\(\\phi_{j1}\\)\n\n\\[\n\\mathop{\\text{maximize}}_{\\phi_{11},\\dots,\\phi_{p1}} \\frac{1}{n}\\sum_{i=1}^n\n\\left(\\sum_{j=1}^p \\phi_{j1}x_{ij}\\right)^{\\!\\!\\!2} \\text{ subject to }\n\\sum_{j=1}^p \\phi^2_{j1} = 1\n\\]\nRepeat optimisation to estimate \\(\\phi_{jk}\\), with additional constraint that \\(\\sum_{j=1, k&lt;k'}^p \\phi_{jk}\\phi_{jk'} = 0\\) (next vector is orthogonal to previous eigenvector)."
  },
  {
    "objectID": "week2/slides.html#alternative-forumulations",
    "href": "week2/slides.html#alternative-forumulations",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Alternative forumulations",
    "text": "Alternative forumulations\n\n\nEigen-decomposition\n\nCompute the covariance matrix (after centering the columns of \\({X}\\)) \\[S = {X}^T{X}\\]\nFind eigenvalues (diagonal elements of \\(D\\)) and eigenvectors ( \\(V\\) ): \\[{S}={V}{D}{V}^T\\] where columns of \\({V}\\) are orthonormal (i.e., \\({V}^T{V}={I}\\))\n\n\nSingular Value Decomposition\n\\[X = U\\Lambda V^T\\]\n\n\\(X\\) is an \\(n\\times p\\) matrix\n\\(U\\) is \\(n \\times r\\) matrix with orthonormal columns ( \\(U^TU=I\\) )\n\\(\\Lambda\\) is \\(r \\times r\\) diagonal matrix with non-negative elements. (Square root of the eigenvalues.)\n\\(V\\) is \\(p \\times r\\) matrix with orthonormal columns (These are the eigenvectors, and \\(V^TV=I\\) ).\n\nIt is always possible to uniquely decompose a matrix in this way."
  },
  {
    "objectID": "week2/slides.html#total-variance",
    "href": "week2/slides.html#total-variance",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Total variance",
    "text": "Total variance\nRemember, PCA is trying to summarise the variance in the data.\nTotal variance (TV) in data (assuming variables centered at 0):\n\\[\n\\text{TV} = \\sum_{j=1}^p \\text{Var}(x_j) = \\sum_{j=1}^p \\frac{1}{n}\\sum_{i=1}^n x_{ij}^2\n\\]\nIf variables are standardised, TV=number of variables.\n\nVariance explained by mâ€™th PC: \\(V_m = \\text{Var}(z_m) = \\frac{1}{n}\\sum_{i=1}^n z_{im}^2\\)\n\\[\n\\text{TV} = \\sum_{m=1}^M V_m \\text{  where }M=\\min(n-1,p).\n\\]"
  },
  {
    "objectID": "week2/slides.html#how-to-choose-k",
    "href": "week2/slides.html#how-to-choose-k",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "How to choose \\(k\\)?",
    "text": "How to choose \\(k\\)?\n\nPCA is a useful dimension reduction technique for large datasets, but deciding on how many dimensions to keep isnâ€™t often clear.\n\nHow do we know how many principal components to choose?"
  },
  {
    "objectID": "week2/slides.html#how-to-choose-k-1",
    "href": "week2/slides.html#how-to-choose-k-1",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "How to choose \\(k\\)?",
    "text": "How to choose \\(k\\)?\n\n\nProportion of variance explained:\n\\[\\text{PVE}_m = \\frac{V_m}{TV}\\]\nChoosing the number of PCs that adequately summarises the variation in \\(X\\), is achieved by examining the cumulative proportion of variance explained.\n\n\nCumulative proportion of variance explained:\n\\[\\text{CPVE}_k = \\sum_{m=1}^k\\frac{V_m}{TV}\\]"
  },
  {
    "objectID": "week2/slides.html#how-to-choose-k-2",
    "href": "week2/slides.html#how-to-choose-k-2",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "How to choose \\(k\\)?",
    "text": "How to choose \\(k\\)?\n\n\n\nScree plot: Plot of variance explained by each component vs number of component."
  },
  {
    "objectID": "week2/slides.html#how-to-choose-k-3",
    "href": "week2/slides.html#how-to-choose-k-3",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "How to choose \\(k\\)?",
    "text": "How to choose \\(k\\)?\n\n\n\nScree plot: Plot of variance explained by each component vs number of component."
  },
  {
    "objectID": "week2/slides.html#example---track-records",
    "href": "week2/slides.html#example---track-records",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Example - track records",
    "text": "Example - track records\nThe data on national track records for women (as at 1984).\n\ntrack &lt;- read_csv(here::here(\"data/womens_track.csv\"))\nglimpse(track)\n\nRows: 55\nColumns: 8\n$ m100     &lt;dbl&gt; 11.6, 11.2, 11.4, 11.4, 11.5, 11.3, 12.1,â€¦\n$ m200     &lt;dbl&gt; 22.9, 22.4, 23.1, 23.0, 23.1, 23.2, 24.5,â€¦\n$ m400     &lt;dbl&gt; 54.5, 51.1, 50.6, 52.0, 53.3, 52.8, 55.0,â€¦\n$ m800     &lt;dbl&gt; 2.15, 1.98, 1.99, 2.00, 2.16, 2.10, 2.18,â€¦\n$ m1500    &lt;dbl&gt; 4.43, 4.13, 4.22, 4.14, 4.58, 4.49, 4.45,â€¦\n$ m3000    &lt;dbl&gt; 9.79, 9.08, 9.34, 8.88, 9.81, 9.77, 9.51,â€¦\n$ marathon &lt;dbl&gt; 179, 152, 159, 158, 170, 169, 191, 149, 1â€¦\n$ country  &lt;chr&gt; \"argentin\", \"australi\", \"austria\", \"belgiâ€¦\n\n\nSource: Johnson and Wichern, Applied multivariate analysis"
  },
  {
    "objectID": "week2/slides.html#explore-the-data-scatterplot-matrix",
    "href": "week2/slides.html#explore-the-data-scatterplot-matrix",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Explore the data: scatterplot matrix",
    "text": "Explore the data: scatterplot matrix\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do you learn?\n\n\nLinear relationships between most variables\nOutliers in long distance events, and in 400m vs 100m, 200m\nNon-linear relationship between marathon and 400m, 800m"
  },
  {
    "objectID": "week2/slides.html#explore-the-data-tour",
    "href": "week2/slides.html#explore-the-data-tour",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Explore the data: tour",
    "text": "Explore the data: tour\n\n\n\n\n\n\nWhat do you learn?\n\nMostly like a very slightly curved pencil\nSeveral outliers, in different directions"
  },
  {
    "objectID": "week2/slides.html#compute-pca",
    "href": "week2/slides.html#compute-pca",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Compute PCA",
    "text": "Compute PCA\n\noptions(digits=2)\n\n\ntrack_pca &lt;- prcomp(track[,1:7], center=TRUE, scale=TRUE)\ntrack_pca\n\nStandard deviations (1, .., p=7):\n[1] 2.41 0.81 0.55 0.35 0.23 0.20 0.15\n\nRotation (n x k) = (7 x 7):\n          PC1   PC2    PC3    PC4    PC5     PC6    PC7\nm100     0.37  0.49 -0.286  0.319  0.231  0.6198  0.052\nm200     0.37  0.54 -0.230 -0.083  0.041 -0.7108 -0.109\nm400     0.38  0.25  0.515 -0.347 -0.572  0.1909  0.208\nm800     0.38 -0.16  0.585 -0.042  0.620 -0.0191 -0.315\nm1500    0.39 -0.36  0.013  0.430  0.030 -0.2312  0.693\nm3000    0.39 -0.35 -0.153  0.363 -0.463  0.0093 -0.598\nmarathon 0.37 -0.37 -0.484 -0.672  0.131  0.1423  0.070"
  },
  {
    "objectID": "week2/slides.html#summarise",
    "href": "week2/slides.html#summarise",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Summarise",
    "text": "Summarise\nSummary of the principal components:\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\n\n\n\n\nVariance\n5.81\n0.65\n0.30\n0.13\n0.05\n0.04\n0.02\n\n\nProportion\n0.83\n0.09\n0.04\n0.02\n0.01\n0.01\n0.00\n\n\nCum. prop\n0.83\n0.92\n0.97\n0.98\n0.99\n1.00\n1.00\n\n\n\n\n\n\n\nIncrease in variance explained large until \\(k=3\\) PCs, and then tapers off. A choice of 3 PCs would explain 97% of the total variance."
  },
  {
    "objectID": "week2/slides.html#decide",
    "href": "week2/slides.html#decide",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Decide",
    "text": "Decide\n\n\n\nScree plot: Where is the elbow?\n\n At \\(k=2\\), thus the scree plot suggests 2 PCs would be sufficient to explain the variability."
  },
  {
    "objectID": "week2/slides.html#assess-data-in-the-model-space",
    "href": "week2/slides.html#assess-data-in-the-model-space",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Assess: Data-in-the-model-space",
    "text": "Assess: Data-in-the-model-space\n\n\n\nVisualise model using a biplot: Plot the principal component scores, and also the contribution of the original variables to the principal component.\n\nA biplot is like a single projection from a tour."
  },
  {
    "objectID": "week2/slides.html#interpret",
    "href": "week2/slides.html#interpret",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Interpret",
    "text": "Interpret\n\nPC1 measures overall magnitude, the strength of the athletics program. High positive values indicate poor programs with generally slow times across events.\nPC2 measures the contrast in the program between short and long distance events. Some countries have relatively stronger long distance atheletes, while others have relatively stronger short distance athletes.\nThere are several outliers visible in this plot, wsamoa, cookis, dpkorea. PCA, because it is computed using the variance in the data, can be affected by outliers. It may be better to remove these countries, and re-run the PCA.\nPC3, may or may not be useful to keep. The interpretation would that this variable summarises countries with different middle distance performance."
  },
  {
    "objectID": "week2/slides.html#assess-model-in-the-data-space",
    "href": "week2/slides.html#assess-model-in-the-data-space",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Assess: Model-in-the-data-space",
    "text": "Assess: Model-in-the-data-space\n\n\n\ntrack_std &lt;- track |&gt;   \n  mutate_if(is.numeric, function(x) (x-\n      mean(x, na.rm=TRUE))/\n      sd(x, na.rm=TRUE))\ntrack_std_pca &lt;- prcomp(track_std[,1:7], \n               scale = FALSE, \n               retx=TRUE)\ntrack_model &lt;- pca_model(track_std_pca, d=2, s=2)\ntrack_all &lt;- rbind(track_model$points, track_std[,1:7])\nanimate_xy(track_all, edges=track_model$edges,\n           edges.col=\"#E7950F\", \n           edges.width=3, \n           axes=\"off\")\nrender_gif(track_all, \n           grand_tour(), \n           display_xy(\n                      edges=track_model$edges, \n                      edges.col=\"#E7950F\", \n                      edges.width=3, \n                      axes=\"off\"),\n           gif_file=\"gifs/track_model.gif\",\n           frames=500,\n           width=400,\n           height=400,\n           loop=FALSE)\n\nMostly captures the variance in the data. Seems to slightly miss the non-linear relationship."
  },
  {
    "objectID": "week2/slides.html#delectable-details",
    "href": "week2/slides.html#delectable-details",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Delectable details",
    "text": "Delectable details\n\n\nðŸ¤­\n\nSometimes the lowest PCs show the interesting patterns, like non-linear relationships, or clusters.\n\n\n\n\nPCA summarises linear relationships, and might not see other interesting dependencies. Projection pursuit is a generalisation that can find other interesting patterns.\nOutliers can affect results, because direction of outliers will appear to have larger variance\nScaling of variables matters, and typically you would first standardise each variable to have mean 0 and variance 1. Otherwise, PCA might simply report the variables with the largest variance, which we already know."
  },
  {
    "objectID": "week2/slides.html#non-linear-dimension-reduction",
    "href": "week2/slides.html#non-linear-dimension-reduction",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Non-linear dimension reduction",
    "text": "Non-linear dimension reduction"
  },
  {
    "objectID": "week2/slides.html#common-approaches",
    "href": "week2/slides.html#common-approaches",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Common approaches",
    "text": "Common approaches\n\n\nFind some low-dimensional layout of points which approximates the distance between points in high-dimensions, with the purpose being to have a useful representation that reveals high-dimensional patterns, like clusters.\nMultidimensional scaling (MDS) is the original approach:\n\\[\n\\mbox{Stress}_D(x_1, ..., x_n) = \\left(\\sum_{i, j=1; i\\neq j}^n (d_{ij} - d_k(i,j))^2\\right)^{1/2}\n\\] where \\(D\\) is an \\(n\\times n\\) matrix of distances \\((d_{ij})\\) between all pairs of points, and \\(d_k(i,j)\\) is the distance between the points in the low-dimensional space.\nPCA is a special case of MDS. The result from PCA is a linear projection, but generally MDS can provide some non-linear transformation.\n\n\nMany variations being developed:\n\nt-stochastic neighbourhood embedding (t-SNE): compares interpoint distances with a standard probability distribution (eg \\(t\\)-distribution) to exaggerate local neighbourhood differences.\nuniform manifold approximation and projection (UMAP): compares the interpoint distances with what might be expected if the data was uniformly distributed in the high-dimensional shapes.\n\n\nNLDR can be useful but it can also make some misleading representations."
  },
  {
    "objectID": "week2/slides.html#umap-12",
    "href": "week2/slides.html#umap-12",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "UMAP (1/2)",
    "text": "UMAP (1/2)\n\n\n\nUMAP 2D representation\n\n\n\n\n\n\n\n\n\n\n\nlibrary(uwot)\nset.seed(253)\np_tidy_umap &lt;- umap(p_tidy_std[,2:5], init = \"spca\")\n\n\n\nTour animation"
  },
  {
    "objectID": "week2/slides.html#umap-22",
    "href": "week2/slides.html#umap-22",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "UMAP (2/2)",
    "text": "UMAP (2/2)\n\n\n\nUMAP 2D representation\n\n\n\n\n\n\n\n\n\nTour animation"
  },
  {
    "objectID": "week2/slides.html#next-re-sampling-and-regularisation",
    "href": "week2/slides.html#next-re-sampling-and-regularisation",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Next: Re-sampling and regularisation",
    "text": "Next: Re-sampling and regularisation\n\n\n\nETC3250/5250 Lecture 2 | iml.numbat.space"
  },
  {
    "objectID": "week12/index.html#presentations-from-masters-students",
    "href": "week12/index.html#presentations-from-masters-students",
    "title": "Week 12: Project presentations by Masters students",
    "section": "Presentations from Masters students",
    "text": "Presentations from Masters students"
  },
  {
    "objectID": "week10/index.html",
    "href": "week10/index.html",
    "title": "Week 10: Model-based clustering and self-organising maps",
    "section": "",
    "text": "HOML Ch 22"
  },
  {
    "objectID": "week10/index.html#main-reference",
    "href": "week10/index.html#main-reference",
    "title": "Week 10: Model-based clustering and self-organising maps",
    "section": "",
    "text": "HOML Ch 22"
  },
  {
    "objectID": "week10/index.html#what-you-will-learn-this-week",
    "href": "week10/index.html#what-you-will-learn-this-week",
    "title": "Week 10: Model-based clustering and self-organising maps",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nModels of multimodality using Gaussian mixtures\nFitting model-based clustering\nDiagnostics for the model fit\nSelf-organising maps and dimension reduction"
  },
  {
    "objectID": "week10/index.html#assignments",
    "href": "week10/index.html#assignments",
    "title": "Week 10: Model-based clustering and self-organising maps",
    "section": "Assignments",
    "text": "Assignments\n\nProject is due on Friday 17 May."
  },
  {
    "objectID": "week1/slides.html#welcome-meet-the-teaching-team",
    "href": "week1/slides.html#welcome-meet-the-teaching-team",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Welcome! Meet the teaching team",
    "text": "Welcome! Meet the teaching team\n\nChief examiner: Professor Dianne Cook\nCommunication: All questions need to be communicated through the Discussion forum. Any of a private matter can be addressed to etc3250.clayton-x@monash.edu or through a private message on the edstem forum. Emails should never be sent directly to tutors or the instructor.\n Tutors:\n\nPatrick: 3rd year PhD student working on computer vision for reading residual plots\nHarriet: 2nd year PhD student working on visualisation of uncertainty\nJayani: 2nd year PhD student working on methods for understanding how non-linear dimension reduction warps your data\nKrisanat: MBAt graduate, aspiring to be PhD student in 2025"
  },
  {
    "objectID": "week1/slides.html#what-this-course-is-about",
    "href": "week1/slides.html#what-this-course-is-about",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "What this course is about",
    "text": "What this course is about\n\nselect and develop appropriate models for clustering, prediction or classification.\nestimate and simulate from a variety of statistical models.\nmeasure the uncertainty of a prediction or classification using resampling methods.\napply business analytic tools to produce innovative solutions in finance, marketing, economics and related areas.\nmanage very large data sets in a modern software environment.\nexplain and interpret the analyses undertaken clearly and effectively."
  },
  {
    "objectID": "week1/slides.html#assessment",
    "href": "week1/slides.html#assessment",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Assessment",
    "text": "Assessment\n\nWeekly learning quizzes: 3% DUE: Mondays 9am\nAssignment 1: 9%\nAssignment 2: 9%\nAssignment 3: 9%\nProject: 10%\nFinal exam: 60%"
  },
  {
    "objectID": "week1/slides.html#how-to-do-well",
    "href": "week1/slides.html#how-to-do-well",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "How to do well",
    "text": "How to do well\n\nKeep up-to-date with content:\n\nparticipate in the lecture each week\nattend tutorials\ncomplete weekly learning quiz to check your understanding\nread the relevant sections of the resource material\nrun the code from lectures in the qmd files\n\nBegin assessments early, when posted, map out a plan to complete it on time\nAsk questions"
  },
  {
    "objectID": "week1/slides.html#section",
    "href": "week1/slides.html#section",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "",
    "text": "Machine learning is a big, big area. This semester is like the tip of the iceberg, thereâ€™s a lot more, and interesting methods and problems, than what we can cover. Take this as a challenge to get you started, and become hungry to learn more!"
  },
  {
    "objectID": "week1/slides.html#types-of-problems",
    "href": "week1/slides.html#types-of-problems",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Types of problems",
    "text": "Types of problems"
  },
  {
    "objectID": "week1/slides.html#framing-the-problem",
    "href": "week1/slides.html#framing-the-problem",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Framing the problem",
    "text": "Framing the problem\n\nSupervised classification: categorical \\(y_i\\) is available for all \\(x_i\\)\nUnsupervised learning: \\(y_i\\) unavailable for all \\(x_i\\)"
  },
  {
    "objectID": "week1/slides.html#what-type-of-problem-is-this-13",
    "href": "week1/slides.html#what-type-of-problem-is-this-13",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "What type of problem is this? (1/3)",
    "text": "What type of problem is this? (1/3)\nFood serversâ€™ tips in restaurants may be influenced by many factors, including the nature of the restaurant, size of the party, and table locations in the restaurant. Restaurant managers need to know which factors matter when they assign tables to food servers. For the sake of staff morale, they usually want to avoid either the substance or the appearance of unfair treatment of the servers, for whom tips (at least in restaurants in the United States) are a major component of pay.\nIn one restaurant, a food server recorded the following data on all customers they served during an interval of two and a half months in early 1990. The restaurant, located in a suburban shopping mall, was part of a national chain and served a varied menu. In observance of local law the restaurant offered seating in a non-smoking section to patrons who requested it. Each record includes a day and time, and taken together, they show the serverâ€™s work schedule.\nWhat is \\(y\\)? What is \\(x\\)?"
  },
  {
    "objectID": "week1/slides.html#what-type-of-problem-is-this-23",
    "href": "week1/slides.html#what-type-of-problem-is-this-23",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "What type of problem is this? (2/3)",
    "text": "What type of problem is this? (2/3)\nEvery person monitored their email for a week and recorded information about each email message; for example, whether it was spam, and what day of the week and time of day the email arrived. We want to use this information to build a spam filter, a classifier that will catch spam with high probability but will never classify good email as spam.\nWhat is \\(y\\)? What is \\(x\\)?"
  },
  {
    "objectID": "week1/slides.html#what-type-of-problem-is-this-33",
    "href": "week1/slides.html#what-type-of-problem-is-this-33",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "What type of problem is this? (3/3)",
    "text": "What type of problem is this? (3/3)\nA health insurance company collected the following information about households:\n\nTotal number of doctor visits per year\nTotal household size\nTotal number of hospital visits per year\nAverage age of household members\nTotal number of gym memberships\nUse of physiotherapy and chiropractic services\nTotal number of optometrist visits\n\nThe health insurance company wants to provide a small range of products, containing different bundles of services and for different levels of cover, to market to customers.\nWhat is \\(y\\)? What is \\(x\\)?"
  },
  {
    "objectID": "week1/slides.html#math-and-computing",
    "href": "week1/slides.html#math-and-computing",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Math and computing",
    "text": "Math and computing"
  },
  {
    "objectID": "week1/slides.html#data-math",
    "href": "week1/slides.html#data-math",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Data: math",
    "text": "Data: math\n\\(n\\) number of observations or sample points\n\\(p\\) number of variables or the dimension of the data\nA data matrix is denoted as:\n\\[\\begin{align*}\n{\\mathbf X}_{n\\times p}= ({\\mathbf x}_1 ~ {\\mathbf x}_2 ~ \\dots  ~ {\\mathbf x}_p) = \\left(\\begin{array}{cccc}\nx_{11} & x_{12} & \\dots & x_{1p} \\\\\nx_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n1} & x_{n2} & \\dots & x_{np} \\end{array} \\right)\n\\end{align*}\\]\nThis is also considered the matrix of predictors, or explanatory or independent variables, features, attributes, or input."
  },
  {
    "objectID": "week1/slides.html#data-computing",
    "href": "week1/slides.html#data-computing",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Data: computing",
    "text": "Data: computing\n\n\n\nlibrary(mvtnorm)\nvc &lt;- matrix(c(1, 0.5, 0.2, \n               0.5, 1, -0.3, \n               0.2, -0.3, 1), \n             ncol=3, byrow=TRUE)\nset.seed(449)\nx &lt;- rmvnorm(5, \n             mean = c(-0.2, 0, 0.3), \n             sigma = vc)\nx\n\n       [,1]   [,2]   [,3]\n[1,] -0.423  1.011 -0.645\n[2,] -0.829 -0.988 -0.370\n[3,] -0.981  0.660 -0.133\n[4,] -0.129 -0.252  0.567\n[5,] -0.366  0.554  1.370\n\n\nWhatâ€™s the dimension of the data?\n\n\nlibrary(palmerpenguins)\np_tidy &lt;- penguins |&gt;\n  select(species, bill_length_mm:body_mass_g) |&gt;\n  rename(bl=bill_length_mm,\n         bd=bill_depth_mm,\n         fl=flipper_length_mm,\n         bm=body_mass_g) \np_tidy |&gt; slice_head(n=10)\n\n# A tibble: 10 Ã— 5\n   species    bl    bd    fl    bm\n   &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1 Adelie   39.1  18.7   181  3750\n 2 Adelie   39.5  17.4   186  3800\n 3 Adelie   40.3  18     195  3250\n 4 Adelie   NA    NA      NA    NA\n 5 Adelie   36.7  19.3   193  3450\n 6 Adelie   39.3  20.6   190  3650\n 7 Adelie   38.9  17.8   181  3625\n 8 Adelie   39.2  19.6   195  4675\n 9 Adelie   34.1  18.1   193  3475\n10 Adelie   42    20.2   190  4250\n\n\nWhatâ€™s the dimension of the data?"
  },
  {
    "objectID": "week1/slides.html#observations-and-variables-math",
    "href": "week1/slides.html#observations-and-variables-math",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Observations and variables: math",
    "text": "Observations and variables: math\n\n\nThe \\(i^{th}\\) observation is denoted as\n\\[\\begin{align*}\nx_i = \\left(\\begin{array}{cccc}\nx_{i1} & x_{i2} & \\dots & x_{ip} \\\\\n\\end{array} \\right)\n\\end{align*}\\]\n\nThe \\(j^{th}\\) variable is denoted as\n\\[\\begin{align*}\nx_j = \\left(\\begin{array}{c}\nx_{1j} \\\\ x_{2j} \\\\ \\vdots \\\\ x_{nj} \\\\\n\\end{array} \\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "week1/slides.html#observations-and-variables-computing",
    "href": "week1/slides.html#observations-and-variables-computing",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Observations and variables: computing",
    "text": "Observations and variables: computing\n\n\nObservations - rows\n\nx[2,]\n\n[1] -0.829 -0.988 -0.370\n\n\n\n\np_tidy |&gt; slice_sample()\n\n# A tibble: 1 Ã— 5\n  species      bl    bd    fl    bm\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n1 Chinstrap  51.5  18.7   187  3250\n\n\n\nVariables - columns\n\nx[,1]\n\n[1] -0.423 -0.829 -0.981 -0.129 -0.366\n\n\n\n\np_tidy |&gt; pull(fl)\n\n  [1] 181 186 195  NA 193 190 181 195 193 190 186 180 182\n [14] 191 198 185 195 197 184 194 174 180 189 185 180 187\n [27] 183 187 172 180 178 178 188 184 195 196 190 180 181\n [40] 184 182 195 186 196 185 190 182 179 190 191 186 188\n [53] 190 200 187 191 186 193 181 194 185 195 185 192 184\n [66] 192 195 188 190 198 190 190 196 197 190 195 191 184\n [79] 187 195 189 196 187 193 191 194 190 189 189 190 202\n [92] 205 185 186 187 208 190 196 178 192 192 203 183 190\n[105] 193 184 199 190 181 197 198 191 193 197 191 196 188\n[118] 199 189 189 187 198 176 202 186 199 191 195 191 210\n[131] 190 197 193 199 187 190 191 200 185 193 193 187 188\n[144] 190 192 185 190 184 195 193 187 201 211 230 210 218\n[157] 215 210 211 219 209 215 214 216 214 213 210 217 210\n[170] 221 209 222 218 215 213 215 215 215 216 215 210 220\n[183] 222 209 207 230 220 220 213 219 208 208 208 225 210\n[196] 216 222 217 210 225 213 215 210 220 210 225 217 220\n[209] 208 220 208 224 208 221 214 231 219 230 214 229 220\n[222] 223 216 221 221 217 216 230 209 220 215 223 212 221\n[235] 212 224 212 228 218 218 212 230 218 228 212 224 214\n[248] 226 216 222 203 225 219 228 215 228 216 215 210 219\n[261] 208 209 216 229 213 230 217 230 217 222 214  NA 215\n[274] 222 212 213 192 196 193 188 197 198 178 197 195 198\n[287] 193 194 185 201 190 201 197 181 190 195 181 191 187\n[300] 193 195 197 200 200 191 205 187 201 187 203 195 199\n[313] 195 210 192 205 210 187 196 196 196 201 190 212 187\n[326] 198 199 201 193 203 187 197 191 203 202 194 206 189\n[339] 195 207 202 193 210 198"
  },
  {
    "objectID": "week1/slides.html#matrix-multiplication-math",
    "href": "week1/slides.html#matrix-multiplication-math",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Matrix multiplication: math",
    "text": "Matrix multiplication: math\n\\[\\begin{align*}\n{\\mathbf A}_{2\\times 3} = \\left(\\begin{array}{ccc}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\ \\end{array} \\right)\n\\end{align*}\\]\n\\[\\begin{align*}\n{\\mathbf B}_{3\\times 4} = \\left(\\begin{array}{cccc}\nb_{11} & b_{12} & b_{13} & b_{14}\\\\\nb_{21} & b_{22} & b_{23} & b_{24}\\\\\nb_{31} & b_{32} & b_{33} & b_{34}\\\\ \\end{array} \\right)\n\\end{align*}\\]\nthen\n\\[\\begin{align*}\n{\\mathbf A}{\\mathbf B}_{2\\times 4} = \\left(\\begin{array}{cccc}\n\\sum_{j=1}^3 a_{1j}b_{j1} & \\sum_{j=1}^3 a_{1j}b_{j2} & \\sum_{j=1}^3 a_{1j}b_{j3} & \\sum_{j=1}^3 a_{1j}b_{j4}\\\\\n\\sum_{j=1}^3 a_{2j}b_{j1} & \\sum_{j=1}^3 a_{2j}b_{j2} & \\sum_{j=1}^3 a_{2j}b_{j3} & \\sum_{j=1}^3 a_{2j}b_{j4} \\end{array} \\right)\n\\end{align*}\\]\nPour the rows into the columns. Note: You canâ€™t do \\({\\mathbf B}{\\mathbf A}\\)!"
  },
  {
    "objectID": "week1/slides.html#matrix-multiplication-computing",
    "href": "week1/slides.html#matrix-multiplication-computing",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Matrix multiplication: computing",
    "text": "Matrix multiplication: computing\n\n\n\nx\n\n       [,1]   [,2]   [,3]\n[1,] -0.423  1.011 -0.645\n[2,] -0.829 -0.988 -0.370\n[3,] -0.981  0.660 -0.133\n[4,] -0.129 -0.252  0.567\n[5,] -0.366  0.554  1.370\n\nproj &lt;- matrix(c(1/sqrt(2), 1/sqrt(2), 0, \n                 0, 0, 1), ncol=2, byrow=FALSE)\nproj\n\n      [,1] [,2]\n[1,] 0.707    0\n[2,] 0.707    0\n[3,] 0.000    1\n\nx %*% proj\n\n       [,1]   [,2]\n[1,]  0.416 -0.645\n[2,] -1.285 -0.370\n[3,] -0.227 -0.133\n[4,] -0.269  0.567\n[5,]  0.133  1.370\n\n\n\nTry this:\n\nt(x) %*% proj\n\nIt produces an error because it canâ€™t be done\nError in t(x) %*% proj : non-conformable arguments\n Notice: %*% uses a * so it is NOT the tidyverse pipe."
  },
  {
    "objectID": "week1/slides.html#identity-matrix",
    "href": "week1/slides.html#identity-matrix",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Identity matrix",
    "text": "Identity matrix\n\n\n\\[\\begin{align*}\nI = \\left(\\begin{array}{cccc}\n1 & 0 & \\dots & 0 \\\\\n0 & 1 & & \\vdots \\\\\n\\vdots &  & \\ddots & 0\\\\\n0 & 0 & & 1 \\\\\n\\end{array}\\right)_{p\\times p}\n\\end{align*}\\]\n\n\ndiag(1, 8, 8)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    1    0    0    0    0    0    0    0\n[2,]    0    1    0    0    0    0    0    0\n[3,]    0    0    1    0    0    0    0    0\n[4,]    0    0    0    1    0    0    0    0\n[5,]    0    0    0    0    1    0    0    0\n[6,]    0    0    0    0    0    1    0    0\n[7,]    0    0    0    0    0    0    1    0\n[8,]    0    0    0    0    0    0    0    1"
  },
  {
    "objectID": "week1/slides.html#inverting-a-matrix-math",
    "href": "week1/slides.html#inverting-a-matrix-math",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Inverting a matrix: math",
    "text": "Inverting a matrix: math\n\n\nSuppose that \\({\\mathbf A}\\) is square\n\\[\\begin{align*}\n{\\mathbf A}_{2\\times 2} = \\left(\\begin{array}{cc}\na & b  \\\\\nc & d \\\\ \\end{array} \\right)\n\\end{align*}\\]\nthen the inverse is (if \\(ad-bc \\neq 0\\))\n\\[\\begin{align*}\n{\\mathbf A}^{-1}_{2\\times 2} = \\frac{1}{ad-bc} \\left(\\begin{array}{cc}\nd & -b \\\\\n-c & a \\\\ \\end{array} \\right)\n\\end{align*}\\]\nand \\({\\mathbf A}{\\mathbf A}^{-1} = I\\) where\n\\[\\begin{align*}\n{\\mathbf I}_{2\\times 2} = \\left(\\begin{array}{cc}\n1 & 0 \\\\\n0 & 1 \\\\ \\end{array} \\right)\n\\end{align*}\\]\n\nIf \\(AB=I\\), then \\(B=A^{-1}\\).\n\nvc\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.5  0.2\n[2,]  0.5  1.0 -0.3\n[3,]  0.2 -0.3  1.0\n\nvc_i &lt;- solve(vc)\nvc_i\n\n       [,1]   [,2]   [,3]\n[1,]  1.625 -1.000 -0.625\n[2,] -1.000  1.714  0.714\n[3,] -0.625  0.714  1.339\n\nvc %*% vc_i\n\n          [,1]     [,2]      [,3]\n[1,]  1.00e+00 7.45e-17 -7.34e-18\n[2,] -1.96e-16 1.00e+00  4.82e-17\n[3,]  0.00e+00 0.00e+00  1.00e+00"
  },
  {
    "objectID": "week1/slides.html#projections",
    "href": "week1/slides.html#projections",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Projections",
    "text": "Projections\n\n\n\\(d (\\leq p)\\) is used to denote the number of variables in a lower dimensional space, usually by taking a projection.\n\\(A\\) is a \\(p\\times d\\) orthonormal basis, \\(A^\\top A=I_d\\) ( \\(A'A=I_d\\) ).\nThe projection of \\({\\mathbf x_i}\\) onto \\(A\\) is \\(A^\\top{\\mathbf x}_i\\).\n\n\nproj\n\n      [,1] [,2]\n[1,] 0.707    0\n[2,] 0.707    0\n[3,] 0.000    1\n\nsum(proj[,1]^2)\n\n[1] 1\n\nsum(proj[,2]^2)\n\n[1] 1\n\nsum(proj[,1]*proj[,2])\n\n[1] 0\n\n\nproj would be considered to be a orthonormal projection matrix."
  },
  {
    "objectID": "week1/slides.html#conceptual-framework",
    "href": "week1/slides.html#conceptual-framework",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Conceptual framework",
    "text": "Conceptual framework"
  },
  {
    "objectID": "week1/slides.html#accuracy-vs-interpretability",
    "href": "week1/slides.html#accuracy-vs-interpretability",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Accuracy vs interpretability",
    "text": "Accuracy vs interpretability\n\n\nPredictive accuracy\nThe primary purpose is to be able to predict \\(\\widehat{Y}\\) for new data. And weâ€™d like to do that well! That is, accurately.\n\n\n\nFrom XKCD\n\n\n\nInterpretability\nAlmost equally important is that we want to understand the relationship between \\({\\mathbf X}\\) and \\(Y\\). The simpler model that is (almost) as accurate is the one we choose, always.\n\n\n\nFrom Interpretable Machine Learning"
  },
  {
    "objectID": "week1/slides.html#training-vs-test-splits",
    "href": "week1/slides.html#training-vs-test-splits",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Training vs test splits",
    "text": "Training vs test splits\n\nWhen data are reused for multiple tasks, instead of carefully spent from the finite data budget, certain risks increase, such as the risk of accentuating bias or compounding effects from methodological errors. Julia Silge\n\n\nTraining set: Used to fit the model, might be also broken into a validation set for frequent assessment of fit.\nTest set: Purely used to assess final models performance on future data."
  },
  {
    "objectID": "week1/slides.html#training-vs-test-splits-1",
    "href": "week1/slides.html#training-vs-test-splits-1",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Training vs test splits",
    "text": "Training vs test splits\n\n\n\nd_bal &lt;- tibble(y=c(rep(\"A\", 6), rep(\"B\", 6)),\n                x=c(runif(12)))\nd_bal$y\n\n [1] \"A\" \"A\" \"A\" \"A\" \"A\" \"A\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\"\n\nset.seed(130)\nd_bal_split &lt;- initial_split(d_bal, prop = 0.70)\ntraining(d_bal_split)$y\n\n[1] \"A\" \"A\" \"B\" \"A\" \"B\" \"A\" \"B\" \"A\"\n\ntesting(d_bal_split)$y\n\n[1] \"A\" \"B\" \"B\" \"B\"\n\n\n\n\nd_unb &lt;- tibble(y=c(rep(\"A\", 2), rep(\"B\", 10)),\n                x=c(runif(12)))\nd_unb$y\n\n [1] \"A\" \"A\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\"\n\nset.seed(132)\nd_unb_split &lt;- initial_split(d_unb, prop = 0.70)\ntraining(d_unb_split)$y\n\n[1] \"B\" \"B\" \"A\" \"B\" \"B\" \"A\" \"B\" \"B\"\n\ntesting(d_unb_split)$y\n\n[1] \"B\" \"B\" \"B\" \"B\"\n\n\n\nAlways stratify splitting by sub-groups, especially response variable classes.\n\nd_unb_strata &lt;- initial_split(d_unb, prop = 0.70, strata=y)\ntraining(d_unb_strata)$y\n\n[1] \"A\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\" \"B\"\n\ntesting(d_unb_strata)$y\n\n[1] \"A\" \"B\" \"B\" \"B\""
  },
  {
    "objectID": "week1/slides.html#measuring-accuracy-for-categorical-response",
    "href": "week1/slides.html#measuring-accuracy-for-categorical-response",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Measuring accuracy for categorical response",
    "text": "Measuring accuracy for categorical response\nCompute \\(\\widehat{y}\\) from training data, \\(\\{(y_i, {\\mathbf x}_i)\\}_{i = 1}^n\\). The error rate (fraction of misclassifications) to get the Training Error Rate\n\\[\\text{Error rate} = \\frac{1}{n}\\sum_{i=1}^n I(y_i \\ne \\widehat{y}({\\mathbf x}_i))\\]\nA better estimate of future accuracy is obtained using test data to get the Test Error Rate.\n\n\nTraining error will usually be smaller than test error. When it is much smaller, it indicates that the model is too well fitted to the training data to be accurate on future data (over-fitted)."
  },
  {
    "objectID": "week1/slides.html#confusion-misclassification-matrix",
    "href": "week1/slides.html#confusion-misclassification-matrix",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Confusion (misclassification) matrix",
    "text": "Confusion (misclassification) matrix\n\n\n\n\n\n\n\n\n\n\npredicted\n\n\n\n\n\n\n\n\n1\n\n\n0\n\n\n\n\ntrue\n\n\n1\n\n\na\n\n\nb\n\n\n\n\n\n\n0\n\n\nc\n\n\nd\n\n\n\n\nConsider 1=positive (P), 0=negative (N).\n\nTrue positive (TP): a\nTrue negative (TN): d\nFalse positive (FP): c (Type I error)\nFalse negative (FN): b (Type II error)\n\n\n\nSensitivity, recall, hit rate, or true positive rate (TPR): TP/P = a/(a+b)\nSpecificity, selectivity or true negative rate (TNR): TN/N = d/(c+d)\nPrevalence: P/(P+N) = (a+b)/(a+b+c+d)\nAccuracy: (TP+TN)/(P+N) = (a+d)/(a+b+c+d)\nBalanced accuracy: (TPR + TNR)/2 (or average class errors)"
  },
  {
    "objectID": "week1/slides.html#confusion-misclassification-matrix-computing",
    "href": "week1/slides.html#confusion-misclassification-matrix-computing",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Confusion (misclassification) matrix: computing",
    "text": "Confusion (misclassification) matrix: computing\n\n\nTwo classes\n\n# Write out the confusion matrix in standard form\n#| label: oconfusion-matrix-tidy\ncm &lt;- a2 %&gt;% count(y, pred) |&gt;\n  group_by(y) |&gt;\n  mutate(cl_err = n[pred==y]/sum(n)) \ncm |&gt;\n  pivot_wider(names_from = pred, \n              values_from = n) |&gt;\n  select(y, bilby, quokka, cl_err)\n\n# A tibble: 2 Ã— 4\n# Groups:   y [2]\n  y      bilby quokka cl_err\n  &lt;fct&gt;  &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt;\n1 bilby      9      3  0.75 \n2 quokka     5     10  0.667\n\n\n\naccuracy(a2, y, pred) |&gt; pull(.estimate)\n\n[1] 0.704\n\nbal_accuracy(a2, y, pred) |&gt; pull(.estimate)\n\n[1] 0.708\n\nsens(a2, y, pred) |&gt; pull(.estimate)\n\n[1] 0.75\n\nspecificity(a2, y, pred) |&gt; pull(.estimate)\n\n[1] 0.667\n\n\n\nMore than two classes\n\n# Write out the confusion matrix in standard form\ncm3 &lt;- a3 %&gt;% count(y, pred) |&gt;\n  group_by(y) |&gt;\n  mutate(cl_err = n[pred==y]/sum(n)) \ncm3 |&gt;\n  pivot_wider(names_from = pred, \n              values_from = n, values_fill=0) |&gt;\n  select(y, bilby, quokka, numbat, cl_err)\n\n# A tibble: 3 Ã— 5\n# Groups:   y [3]\n  y      bilby quokka numbat cl_err\n  &lt;fct&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt;\n1 bilby      9      3      0  0.75 \n2 numbat     0      2      6  0.75 \n3 quokka     5     10      0  0.667\n\n\n\naccuracy(a3, y, pred) |&gt; pull(.estimate)\n\n[1] 0.714\n\nbal_accuracy(a3, y, pred) |&gt; pull(.estimate)\n\n[1] 0.783"
  },
  {
    "objectID": "week1/slides.html#receiver-operator-curves-roc",
    "href": "week1/slides.html#receiver-operator-curves-roc",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Receiver Operator Curves (ROC)",
    "text": "Receiver Operator Curves (ROC)\n\n\nThe balance of getting it right, without predicting everything as positive.\n\n\n\nFrom wikipedia\n\n\nNeed predictive probabilities, probability of being each class.\n\n\na2 |&gt; slice_head(n=3)\n\n# A tibble: 3 Ã— 4\n  y     pred  bilby quokka\n  &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 bilby bilby   0.9    0.1\n2 bilby bilby   0.8    0.2\n3 bilby bilby   0.9    0.1\n\nroc_curve(a2, y, bilby) |&gt;\n  autoplot()"
  },
  {
    "objectID": "week1/slides.html#parametric-vs-non-parametric",
    "href": "week1/slides.html#parametric-vs-non-parametric",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Parametric vs non-parametric",
    "text": "Parametric vs non-parametric\n\n\nParametric methods\n\nAssume that the model takes a specific form\nFitting then is a matter of estimating the parameters of the model\nGenerally considered to be less flexible\nIf assumptions are wrong, performance likely to be poor\n\n\nNon-parametric methods\n\nNo specific assumptions\nAllow the data to specify the model form, without being too rough or wiggly\nMore flexible\nGenerally needs more observations, and not too many variables\nEasier to over-fit"
  },
  {
    "objectID": "week1/slides.html#section-1",
    "href": "week1/slides.html#section-1",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "",
    "text": "From XKCD\n\n\n\n\n\n\n\n\n\n\n\n\nBlack line is true boundary.\n\nGrids (right) show boundaries for two different models."
  },
  {
    "objectID": "week1/slides.html#reducible-vs-irreducible-error",
    "href": "week1/slides.html#reducible-vs-irreducible-error",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Reducible vs irreducible error",
    "text": "Reducible vs irreducible error\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf the model form is incorrect, the error (solid circles) may arise from wrong shape, and is thus reducible. Irreducible means that we have got the right model and mistakes (solid circles) are random noise."
  },
  {
    "objectID": "week1/slides.html#flexible-vs-inflexible",
    "href": "week1/slides.html#flexible-vs-inflexible",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Flexible vs inflexible",
    "text": "Flexible vs inflexible\n\nParametric models tend to be less flexible but non-parametric models can be flexible or less flexible depending on parameter settings."
  },
  {
    "objectID": "week1/slides.html#bias-vs-variance",
    "href": "week1/slides.html#bias-vs-variance",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Bias vs variance",
    "text": "Bias vs variance\n\n\nBias is the error that is introduced by modeling a complicated problem by a simpler problem.\n\nFor example, linear regression assumes a linear relationship and perhaps the relationship is not exactly linear.\nIn general, but not always, the more flexible a method is, the less bias it will have because it can fit a complex shape better.\n\n\n\nVariance refers to how much your estimate would change if you had different training data. Its measuring how much your model depends on the data you have, to the neglect of future data.\n\nIn general, the more flexible a method is, the more variance it has.\nThe size of the training data can impact on the variance."
  },
  {
    "objectID": "week1/slides.html#bias",
    "href": "week1/slides.html#bias",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Bias",
    "text": "Bias\n\n\n\n\n\n\n\n\n\n\n\nWhen you impose too many assumptions with a parametric model, or use an inadequate non-parametric model, such as not letting an algorithm converge fully.\n\n\n\n\n\n\n\n\n\n\nWhen the model closely captures the true shape, with a parametric model or a flexible model."
  },
  {
    "objectID": "week1/slides.html#variance",
    "href": "week1/slides.html#variance",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Variance",
    "text": "Variance\n\n\n\n\n\n\n\n\n\n\n\nThis fit will be virtually identical even if we had a different training sample.\n\n\n\n\n\n\n\n\n\n\nLikely to get a very different model if a different training set is used."
  },
  {
    "objectID": "week1/slides.html#bias-variance-tradeoff",
    "href": "week1/slides.html#bias-variance-tradeoff",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Bias-variance tradeoff",
    "text": "Bias-variance tradeoff\n \nGoal: Without knowing what the true structure is, fit the signal and ignore the noise. Be flexible but not too flexible.\nImages 2.16, 2.15 from ISLR"
  },
  {
    "objectID": "week1/slides.html#trade-off-between-accuracy-and-interpretability",
    "href": "week1/slides.html#trade-off-between-accuracy-and-interpretability",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Trade-off between accuracy and interpretability",
    "text": "Trade-off between accuracy and interpretability"
  },
  {
    "objectID": "week1/slides.html#diagnosing-the-fit",
    "href": "week1/slides.html#diagnosing-the-fit",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Diagnosing the fit",
    "text": "Diagnosing the fit\n\n\nCompute and examine the usual diagnostics, some methods have more\n\nfit statistics: accuracy, sensitivity, specificity\nerrors/misclassifications\nvariable importance\nplot residuals, examine the misclassifications\ncheck test set is similar to training\n\nGo beyond â€¦ Look at the data and the model together!\nWickham et al (2015) Removing the Blindfold\n\n\nTraining - plusses; Test - dots"
  },
  {
    "objectID": "week1/slides.html#feature-engineering",
    "href": "week1/slides.html#feature-engineering",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Feature engineering",
    "text": "Feature engineering\nCreating new variables to get better fits is a special skill! Sometimes automated by the method. All are transformations of the original variables. (See tidymodels steps.)\n\nscaling, centering, sphering (step_pca)\nlog or square root or box-cox transformation (step_log)\nratio of values (step_ratio)\npolynomials or splines: \\(x_1^2, x_1^3\\) (step_ns)\ndummy variables: categorical predictors expanded into multiple new binary variables (step_dummy)\nConvolutional Neural Networks: neural networks but with pre-processing of images to combine values of neighbouring pixels; flattening of images"
  },
  {
    "objectID": "week1/slides.html#the-big-picture",
    "href": "week1/slides.html#the-big-picture",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "The big picture",
    "text": "The big picture\n\n\n\nKnow your data\n\nCategorical response or no response\nTypes of predictors: quantitative, categorical\nIndependent observations\nDo you need to handle missing values?\nAre there anomalous observations?\n\nPlot your data\n\nWhat are the shapes (distribution and variance)?\nAre there gaps or separations (centres)?\n\n\n\n\nFit a model or two\n\nCompute fit statistics\nPlot the model\nExamine parameter estimates\n\nDiagnostics\n\nWhich is the better model\nIs there a simpler model?\nAre the errors reducible or systematic?\nAre you confident that your bias is low and variance is low?"
  },
  {
    "objectID": "week1/slides.html#next-visualisation",
    "href": "week1/slides.html#next-visualisation",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Next: Visualisation",
    "text": "Next: Visualisation\n\n\n\nETC3250/5250 Lecture 1 | iml.numbat.space"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "ETC3250/5250 Resources",
    "section": "",
    "text": "Books and articles\n\nAn Introduction to Statistical Learning (ISLR)\n\nThis book by James, Witten, Hastie and Tibshirani contains the primary content for the unit. It has the explanations for different methodology, practical labs, and a range of exercises to work through. Use the second edition, with Applications in R.\n\nHands-On Machine Learning with R\n\nThis book by Boehmke & Greenwell is an accessible and practical guide to many aspects of machine learning. Itâ€™s coverage of unsupervised classification is very good.\n\nTidy Modeling with R\n\nMachine learning is an active area of research across several disciplines, primarily statistics and computer science. Perhaps because of this there are many ways to define and fit models. The tidy modeling approach coordinates these into a consistent and understandable workflow. It doesnâ€™t interface to all software, but getting started with machine learning using this mind-set helps you get organised despite the fragmented landscape. This book accompanies the software tidymodels.\n\nISLR tidymodels labs\n\nThis book contains the code to do most of the exercises from ISLR using the tidymodels thinking and coding style.\n\nInteractively exploring high-dimensional data and models in R\n\nThis book by Cook and Laa is the primary resource for learning how to visualise high-dimensions, how to explore the data, and to visually examine and diagnose models.\n\nInterpretable Machine Learning\n\nThis book by Christoph Molnar serves as a guide for making black box models explainable. It is an excellent resource for developing your understanding of the different types of models and how to diagnose and interpret them\n\nFeature Engineering A-Z\n\nWritten by Emil Hvitfeldt to cover creating new variables as broadly as possibly. Has classical methods such as dummy variables and box-cox transformations, temporal and spatial data and missing value imputation.\n\n\nUseful links\n\nTensorFlow for R\nA gentle introduction to deep learning in R using Keras\n(M+C)Â² Blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "",
    "text": "Professor Di Cook\n\nEmail: etc3250.clayton-x@monash.edu\nConsultation: Thu 9:00-10:30 (zoom only)"
  },
  {
    "objectID": "index.html#lecturerchief-examiner",
    "href": "index.html#lecturerchief-examiner",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "",
    "text": "Professor Di Cook\n\nEmail: etc3250.clayton-x@monash.edu\nConsultation: Thu 9:00-10:30 (zoom only)"
  },
  {
    "objectID": "index.html#tutors",
    "href": "index.html#tutors",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Tutors",
    "text": "Tutors\n\nPatrick Li\n\nTutorials: Mon 15:00 (LTB 323), Fri 11:00 (CL_33 Innovation Walk, FG04 Bldg 73P)\nConsultation: Thu 10:30-12:00 (W9.20)\n\nHarriet Mason\n\nTutorials: Wed 18:00 (LTB G60), Fri 12:30 (CL_33 Innovation Walk, FG04 Bldg 73P)\nConsultation: Thu 3:00-4:30 (zoom only)\n\nJayani Lakshika\n\nTutorials: Wed 8:00, 9:30 (CL_33 Innovation Walk, FG04 Bldg 73P)\nConsultation: Thu 12:00-1:30 (W9.20)\n\nKrisanat Anukarnsakulchularp\n\nTutorials: Mon 12:00, 13:30 (LTB 323)\nConsultation: Fri 9:30-11:00 (W9.20)"
  },
  {
    "objectID": "index.html#weekly-schedule",
    "href": "index.html#weekly-schedule",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Weekly schedule",
    "text": "Weekly schedule\n\nLecture: Wed 1:05-2:45pm\nTutorial: 1.5 hours\nWeekly learning quizzes due Mondays 9am\n\n\n\n\nWeek\nTopic\nReference\nAssessments\n\n\n\n\n26 Feb\nFoundations of machine learning\nISLR 2.1, 2.2\n\n\n\n04 Mar\nVisualising your data and models\nCook and Laa Ch 1, 3, 4, 5, 6, 13\n\n\n\n11 Mar\nRe-sampling and regularisation\nISLR 5.1, 5.2, 6.2, 6.4\n\n\n\n18 Mar\nLogistic regression and discriminant analysis\nISLR 4.3, 4.4\nAssignment 1\n\n\n25 Mar\nTrees and forests\nISLR 8.1, 8.2\n\n\n\n01 Apr\nMid-semester break\n\n\n\n\n08 Apr\nNeural networks and deep learning\nISLR 10.1-10.3, 10.7\nAssignment 2\n\n\n15 Apr\nExplainable artificial intelligence (XAI)\nMolnar 8.1, 8.5, 9.2-9.6\n\n\n\n22 Apr\nSupport vector machines and nearest neighbours\nISLR 9.1-9.3\nAssignment 3\n\n\n29 Apr\nK-nearest neighbours and hierarchical clustering\nHOML Ch 20, 21\n\n\n\n06 May\nModel-based clustering and self-organising maps\nHOML Ch 22\n\n\n\n13 May\nEvaluating your clustering model\nCook and Laa Ch 12\nProject\n\n\n20 May\nProject presentations by Masters students"
  },
  {
    "objectID": "index.html#assessments",
    "href": "index.html#assessments",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Assessments",
    "text": "Assessments\n\nWeekly learning quizzes: 3%\nAssignment 1: Instructions, Submit to moodle: 9%\nAssignment 2: 9%\nAssignment 3: 9%\nProject: 10%\nFinal exam: 60%"
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "ETC3250/5250 Introduction to Machine Learning",
    "section": "Software",
    "text": "Software\nWe will be using the latest versions of R and RStudio.\nHere is the code to install (most of) the R packages we will be using in this unit.\ninstall.packages(c(\"tidyverse\", \"tidymodels\", \"tourr\", \"geozoo\", \"mulgar\", \"ggpcp\", \"plotly\", \"detourr\", \"langevitour\", \"ggbeeswarm\", \"MASS\", \"GGally\", \"ISLR\", \"mvtnorm\", \"rpart\", \"rpart.plot\", \"randomForest\", \"e1071\", \"xgboost\", \"Rtsne\", \"classifly\", \"penalizedLDA\", \"nnet\", \"kernelshap\", \"shapviz\", \"iml\", \"DALEX\", \"cxhull\", \"fpc\", \"mclust\", \"ggdendro\", \"kohonen\", \"aweSOM\", \"patchwork\", \"ggthemes\", \"colorspace\", \"palmerpenguins\"), dependencies = TRUE)\nIf you run into problems completing the full install, the likely culprits are tidyverse and tidymodels. These are bundles of packages, and might fail at individual packages. To resolve the problems, install each package from the bundle individually, and donâ€™t install any that fail on your system.\nIn addition, follow these instructions to set up tensorflow and keras, which requires having python installed.\nIf you are relatively new to R, working through the materials at https://learnr.numbat.space is an excellent way to up-skill. You are epsecially encouraged to work through Chapter 3, on Troubleshooting and asking for help, because at some point you will need help with your coding, and how you go about this matters and impacts the ability of others to help you.\nThe ISLR book also comes with python code, and you are welcome to do most of your work with python instead of R. However, what you submit for marking must be done with R."
  },
  {
    "objectID": "week1/index.html",
    "href": "week1/index.html",
    "title": "Week 1: Foundations of machine learning",
    "section": "",
    "text": "ISLR 2.1, 2.2"
  },
  {
    "objectID": "week1/index.html#main-reference",
    "href": "week1/index.html#main-reference",
    "title": "Week 1: Foundations of machine learning",
    "section": "",
    "text": "ISLR 2.1, 2.2"
  },
  {
    "objectID": "week1/index.html#what-you-will-learn-this-week",
    "href": "week1/index.html#what-you-will-learn-this-week",
    "title": "Week 1: Foundations of machine learning",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nFraming the problems\nNotation and math\nBias variance-tradeoff\nFitting your models: training/test splits, optimisation\nMeasuring fit: accuracy, loss\nDiagnostics: residuals\nFeature engineering: combining variables to better match purpose and help the model fitting"
  },
  {
    "objectID": "week1/index.html#lecture-slides",
    "href": "week1/index.html#lecture-slides",
    "title": "Week 1: Foundations of machine learning",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week1/index.html#tutorial-instructions",
    "href": "week1/index.html#tutorial-instructions",
    "title": "Week 1: Foundations of machine learning",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:\n\nhtml\nqmd"
  },
  {
    "objectID": "week1/index.html#assignments",
    "href": "week1/index.html#assignments",
    "title": "Week 1: Foundations of machine learning",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week1/tutorialsol.html",
    "href": "week1/tutorialsol.html",
    "title": "ETC53250/5250 Tutorial 1",
    "section": "",
    "text": "The goal for this week is for you to get up and running with the computing environment needed to successfully complete this unit."
  },
  {
    "objectID": "week1/tutorialsol.html#objectives",
    "href": "week1/tutorialsol.html#objectives",
    "title": "ETC53250/5250 Tutorial 1",
    "section": "",
    "text": "The goal for this week is for you to get up and running with the computing environment needed to successfully complete this unit."
  },
  {
    "objectID": "week1/tutorialsol.html#preparation",
    "href": "week1/tutorialsol.html#preparation",
    "title": "ETC53250/5250 Tutorial 1",
    "section": "ðŸ”§ Preparation",
    "text": "ðŸ”§ Preparation\n\nInstall the latest versions of R and RStudio on your computer\n\n\ninstall.packages(c(\"tidyverse\", \"tidymodels\", \"tourr\", \"geozoo\", \"mulgar\", \"ggpcp\", \"plotly\", \"detourr\", \"langevitour\", \"ggbeeswarm\", \"MASS\", \"GGally\", \"ISLR\", \"mvtnorm\", \"rpart\", \"rpart.plot\", \"randomForest\", \"e1071\", \"xgboost\", \"Rtsne\", \"classifly\", \"penalizedLDA\", \"nnet\", \"kernelshap\", \"shapviz\", \"iml\", \"DALEX\", \"cxhull\", \"fpc\", \"mclust\", \"ggdendro\", \"kohonen\", \"aweSOM\", \"patchwork\", \"ggthemes\", \"colorspace\", \"palmerpenguins\"), dependencies = TRUE)\n\n\nCreate a project for this unit called iml.Rproj. All of your tutorial work and assignments should be completed in this workspace."
  },
  {
    "objectID": "week1/tutorialsol.html#exercises",
    "href": "week1/tutorialsol.html#exercises",
    "title": "ETC53250/5250 Tutorial 1",
    "section": "Exercises:",
    "text": "Exercises:\n\n1. The materials at https://learnr.numbat.space are an especially good way to check your R skills are ready for the unit. Regardless how advanced you are, at some point you will need help. How you ask for help is a big factor in getting your problem fixed. The following code generates an error.\n\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(palmerpenguins)\np_sub &lt;- penguins |&gt;\n  select(species, flipper_length_mm) |&gt;\n  filter(species == \"Adelie\")\n\n\nCan you work out why?\nUse the reprex package to create a text where the code and error are visible, and can be shared with someone that might be able to help.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe error is\nError in select(penguins, species, flipper_length_mm) : \n  unused arguments (species, flipper_length_mm)\nand is caused by a conflict in functions between the dplyr and MASS packages. If you read the warning messages when the packages were loaded you might have been aware of this before trying to run code.\nYou can fix it by:\n\nPrefacing functions that have conflicts with their package name, eg dplyr::select()\nUse the conflicted package to set your preferences at the start of any document.\n\nTo make the reprex, copy the code to clipboard, and run reprex(). This will generate:\n\n\n\n\n\n\n\n2. Your turn to write some code that generates an error. Create a reprex, and share with your tutor or neighbour, to see if they can fix the error.\n\n\n3. Follow the guidelines at https://tensorflow.rstudio.com/install/ to setup python and tensorflow on your computer. Then test your installation by following the beginner tutorial.\n\n\n4. Download the slides.qmd file for week 1 lecture.\n\nUse knitr::purl() to extract the R code for the class.\nOpen the resulting slides.R file in your RStudio file browser. What code is in the setup.R file that is sourced at the top?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nLibraries are loaded.\nThere are some global options for slides set, and styling of plots.\nConflicts for some common functions are resolved with preferences.\n\n\n\n\n\n\nRun the rest of the code in small chunks. Does it all work for you? Do you get any errors? Do you have any suggestions on making it easier to run or understand the code?"
  },
  {
    "objectID": "week1/tutorialsol.html#finishing-up",
    "href": "week1/tutorialsol.html#finishing-up",
    "title": "ETC53250/5250 Tutorial 1",
    "section": "ðŸ‘‹ Finishing up",
    "text": "ðŸ‘‹ Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week1/tutorial.html",
    "href": "week1/tutorial.html",
    "title": "ETC53250/5250 Tutorial 1",
    "section": "",
    "text": "The goal for this week is for you to get up and running with the computing environment needed to successfully complete this unit."
  },
  {
    "objectID": "week1/tutorial.html#objectives",
    "href": "week1/tutorial.html#objectives",
    "title": "ETC53250/5250 Tutorial 1",
    "section": "",
    "text": "The goal for this week is for you to get up and running with the computing environment needed to successfully complete this unit."
  },
  {
    "objectID": "week1/tutorial.html#preparation",
    "href": "week1/tutorial.html#preparation",
    "title": "ETC53250/5250 Tutorial 1",
    "section": "ðŸ”§ Preparation",
    "text": "ðŸ”§ Preparation\n\nInstall the latest versions of R and RStudio on your computer\n\n\ninstall.packages(c(\"tidyverse\", \"tidymodels\", \"tourr\", \"geozoo\", \"mulgar\", \"ggpcp\", \"plotly\", \"detourr\", \"langevitour\", \"ggbeeswarm\", \"MASS\", \"GGally\", \"ISLR\", \"mvtnorm\", \"rpart\", \"rpart.plot\", \"randomForest\", \"e1071\", \"xgboost\", \"Rtsne\", \"classifly\", \"penalizedLDA\", \"nnet\", \"kernelshap\", \"shapviz\", \"iml\", \"DALEX\", \"cxhull\", \"fpc\", \"mclust\", \"ggdendro\", \"kohonen\", \"aweSOM\", \"patchwork\", \"ggthemes\", \"colorspace\", \"palmerpenguins\"), dependencies = TRUE)\n\n\nCreate a project for this unit called iml.Rproj. All of your tutorial work and assignments should be completed in this workspace."
  },
  {
    "objectID": "week1/tutorial.html#exercises",
    "href": "week1/tutorial.html#exercises",
    "title": "ETC53250/5250 Tutorial 1",
    "section": "Exercises:",
    "text": "Exercises:\n\n1. The materials at https://learnr.numbat.space are an especially good way to check your R skills are ready for the unit. Regardless how advanced you are, at some point you will need help. How you ask for help is a big factor in getting your problem fixed. The following code generates an error.\n\nlibrary(dplyr)\nlibrary(MASS)\nlibrary(palmerpenguins)\np_sub &lt;- penguins |&gt;\n  select(species, flipper_length_mm) |&gt;\n  filter(species == \"Adelie\")\n\n\nCan you work out why?\nUse the reprex package to create a text where the code and error are visible, and can be shared with someone that might be able to help.\n\n\n\n2. Your turn to write some code that generates an error. Create a reprex, and share with your tutor or neighbour, to see if they can fix the error.\n\n\n3. Follow the guidelines at https://tensorflow.rstudio.com/install/ to setup python and tensorflow on your computer. Then test your installation by following the beginner tutorial.\n\n\n4. Download the slides.qmd file for week 1 lecture.\n\nUse knitr::purl() to extract the R code for the class.\nOpen the resulting slides.R file in your RStudio file browser. What code is in the setup.R file that is sourced at the top?\n\n\nRun the rest of the code in small chunks. Does it all work for you? Do you get any errors? Do you have any suggestions on making it easier to run or understand the code?"
  },
  {
    "objectID": "week1/tutorial.html#finishing-up",
    "href": "week1/tutorial.html#finishing-up",
    "title": "ETC53250/5250 Tutorial 1",
    "section": "ðŸ‘‹ Finishing up",
    "text": "ðŸ‘‹ Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week11/index.html",
    "href": "week11/index.html",
    "title": "Week 11: Evaluating your clustering model",
    "section": "",
    "text": "Cook and Laa Ch 12"
  },
  {
    "objectID": "week11/index.html#main-reference",
    "href": "week11/index.html#main-reference",
    "title": "Week 11: Evaluating your clustering model",
    "section": "",
    "text": "Cook and Laa Ch 12"
  },
  {
    "objectID": "week11/index.html#what-you-will-learn-this-week",
    "href": "week11/index.html#what-you-will-learn-this-week",
    "title": "Week 11: Evaluating your clustering model",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nConfusion tables\nCluster metrics"
  },
  {
    "objectID": "week11/index.html#assignments",
    "href": "week11/index.html#assignments",
    "title": "Week 11: Evaluating your clustering model",
    "section": "Assignments",
    "text": "Assignments\n\nProject is due on Friday 17 May."
  },
  {
    "objectID": "week2/index.html",
    "href": "week2/index.html",
    "title": "Week 2: Visualising your data and models",
    "section": "",
    "text": "Cook and Laa Ch 1, 3, 4, 5, 6, 13"
  },
  {
    "objectID": "week2/index.html#main-reference",
    "href": "week2/index.html#main-reference",
    "title": "Week 2: Visualising your data and models",
    "section": "",
    "text": "Cook and Laa Ch 1, 3, 4, 5, 6, 13"
  },
  {
    "objectID": "week2/index.html#what-you-will-learn-this-week",
    "href": "week2/index.html#what-you-will-learn-this-week",
    "title": "Week 2: Visualising your data and models",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nDimension reduction methods: linear and non-linear\nVisualising high-dimensions using animations of linear projections\nScatterplot matrices\nParallel coordinate plots\nConcept of model-in-the-data-space, relative to data-in-the-moel-space"
  },
  {
    "objectID": "week2/index.html#lecture-slides",
    "href": "week2/index.html#lecture-slides",
    "title": "Week 2: Visualising your data and models",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week2/index.html#tutorial-instructions",
    "href": "week2/index.html#tutorial-instructions",
    "title": "Week 2: Visualising your data and models",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:\n\nhtml\nqmd"
  },
  {
    "objectID": "week2/index.html#assignments",
    "href": "week2/index.html#assignments",
    "title": "Week 2: Visualising your data and models",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 1 is due on Friday 22 March."
  },
  {
    "objectID": "week2/tutorialsol.html",
    "href": "week2/tutorialsol.html",
    "title": "ETC3250/5250 Tutorial 2",
    "section": "",
    "text": "The goal for this week is for you to learn and practice some of the basics of machine learning."
  },
  {
    "objectID": "week2/tutorialsol.html#objectives",
    "href": "week2/tutorialsol.html#objectives",
    "title": "ETC3250/5250 Tutorial 2",
    "section": "",
    "text": "The goal for this week is for you to learn and practice some of the basics of machine learning."
  },
  {
    "objectID": "week2/tutorialsol.html#preparation",
    "href": "week2/tutorialsol.html#preparation",
    "title": "ETC3250/5250 Tutorial 2",
    "section": "ðŸ”§ Preparation",
    "text": "ðŸ”§ Preparation\n\nComplete the quiz\nDo the reading related to week 1"
  },
  {
    "objectID": "week2/tutorialsol.html#exercises",
    "href": "week2/tutorialsol.html#exercises",
    "title": "ETC3250/5250 Tutorial 2",
    "section": "Exercises:",
    "text": "Exercises:\nOpen your project for this unit called iml.Rproj.\n\n1. Answer the following questions for this data matrix,\n\\[\\begin{align*}\n{\\mathbf X} = \\left[\\begin{array}{rrrrr}\n2 & -2 & -8 & 6 & -7 \\\\\n6 & 6 & -4 & 9 & 6 \\\\\n5 & 4 & 3 & -7 & 8 \\\\\n1 & -7 & 6 & 7 & -1\n\\end{array}\\right]\n\\end{align*}\\]\n\nWhat is \\(X_1\\) (variable 1)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(X_1 = (2 ~6 ~5 ~1)\\)\n\n\n\n\n\nWhat is observation 3?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(5 ~ 4 ~ 3 ~ -7 ~ 8\\)\n\n\n\n\n\nWhat is \\(n\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(4\\)\n\n\n\n\n\nWhat is \\(p\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(5\\)\n\n\n\n\n\nWhat is \\(X^\\top\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\[\\begin{align*}\n{\\mathbf X}^\\top = \\left[\\begin{array}{rrrr}\n2 & 6 & 5 & 1\\\\\n-2 & 6 & 4 & -7\\\\\n-8 & -4 & 3 & 6 \\\\\n6 & 9 & -7 & 7 \\\\\n-7 & 6 & 8 & -1\n\\end{array}\\right]\n\\end{align*}\\]\n\n\n\n\n\nWrite a projection matrix which would generate a 2D projection where the first data projection has variables 1 and 4 combined equally, and the second data projection has one third of variable 2 and two thirds of 5.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\[\\begin{align*}\n{\\mathbf A} = \\left[\\begin{array}{rr}\n\\frac{1}{\\sqrt{2}} & 0 \\\\\n0 & \\frac{1}{\\sqrt{3}} \\\\\n0 & 0  \\\\\n\\frac{1}{\\sqrt{2}} & 0  \\\\\n0 & \\frac{\\sqrt{2}}{\\sqrt{3}} \\\\\n\\end{array}\\right]\n\\end{align*}\\]\n\n\n\n\n\nWhy canâ€™t the following matrix considered a projection matrix?\n\n\\[\\begin{align*}\n{\\mathbf A} = \\left[\\begin{array}{rr}\n-1/\\sqrt{2} & 1/\\sqrt{3} \\\\\n0 & 0  \\\\\n1/\\sqrt{2} & 0 \\\\\n0 & \\sqrt{2}/\\sqrt{3} \\\\\n\\end{array}\\right]\n\\end{align*}\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe columns are not orthonormal. The cross-product is not equal to 0.\n\n\n\n\n\n\n2. Which of these statements is the most accurate? And which is the most precise?\nA. It is almost certain to rain in the next week.\nB. It is 90% likely to get at least 10mm of rain tomorrow.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nA is more accurate, but B is more precise.\n\n\n\n\n\n\n3. For the following data, make an appropriate training test split of 60:40. The response variable is cause. Deomstrate that you have made an appropriate split.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(rsample)\n\nbushfires &lt;- read_csv(\"https://raw.githubusercontent.com/dicook/mulgar_book/pdf/data/bushfires_2019-2020.csv\")\nbushfires |&gt; count(cause)\n\n# A tibble: 4 Ã— 2\n  cause           n\n  &lt;chr&gt;       &lt;int&gt;\n1 accident      138\n2 arson          37\n3 burning_off     9\n4 lightning     838\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe data is unbalanced, so it is especially important to stratify the sampling by the response variable. Without stratifying the test set is likely missing observations in the burning_off category.\n\nset.seed(1156)\nbushfires_split &lt;- initial_split(bushfires, prop = 0.60, strata=cause)\ntraining(bushfires_split) |&gt; count(cause)\n\n# A tibble: 4 Ã— 2\n  cause           n\n  &lt;chr&gt;       &lt;int&gt;\n1 accident       84\n2 arson          21\n3 burning_off     5\n4 lightning     502\n\ntesting(bushfires_split) |&gt; count(cause)\n\n# A tibble: 4 Ã— 2\n  cause           n\n  &lt;chr&gt;       &lt;int&gt;\n1 accident       54\n2 arson          16\n3 burning_off     4\n4 lightning     336\n\n\n\n\n\n\n\n\n4. In the lecture slides from week 1 on bias vs variance, these four images were shown.\n \n \nMark the images with the labels â€œtrue modelâ€, â€œfitted modelâ€, â€œbiasâ€. Then explain in your own words why the different model shown in each has (potentially) large bias or small bias, and small variance or large variance.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe linear model will be very similar regardless of the training sample, so it has small variance. But because it misses the curved nature of the true model, it has large bias, missing critical parts of the two classes that are different.\nThe non-parametric model which captures the curves thus has small bias, but the fitted model might vary a lot from one training sample to another which would result in it being considered to have large variance.\n \n\n\n\n\n\n\n5. The following data contains true class and predictive probabilities for a model fit. Answer the questions below for this data.\n\npred_data &lt;- read_csv(\"https://raw.githubusercontent.com/numbats/iml/master/data/pred_data.csv\") |&gt;\n  mutate(true = factor(true))\n\n\nHow many classes?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\npred_data |&gt; count(true)\n\n# A tibble: 2 Ã— 2\n  true          n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie       30\n2 Chinstrap     5\n\n\n\n\n\n\n\nCompute the confusion table, using the maximum predictive probability to label the observation.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nlibrary(tidyr)\npred_data &lt;- pred_data |&gt;\n  mutate(pred = levels(pred_data$true)[apply(pred_data[,-1], 1, which.max)])\npred_data |&gt; count(true, pred) |&gt;  \n  group_by(true) |&gt;\n  mutate(cl_err = n[pred==true]/sum(n)) |&gt;\n  pivot_wider(names_from = pred, \n              values_from = n,\n              values_fill = 0) |&gt;\n  dplyr::select(true, Adelie, Chinstrap, cl_err)\n\n# A tibble: 2 Ã— 4\n# Groups:   true [2]\n  true      Adelie Chinstrap cl_err\n  &lt;fct&gt;      &lt;int&gt;     &lt;int&gt;  &lt;dbl&gt;\n1 Adelie        30         0    1  \n2 Chinstrap      2         3    0.6\n\n\n\n\n\n\n\nCompute the accuracy, and accuracy if all observations were classified as Adelie. Why is the accuracy almost as good when all observations are predicted to be the majority class?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nAccuracy = 33/35 = 0.94\nAccuracy when all predicted to be Adelie = 30/35 = 0.86\nThere are only 5 observations in the Chinstrap class. So accuracy remains high, if we simply ignore this class.\n\n\n\n\n\nCompute the balanced accuracy, by averaging the class errors. Why is it lower than the overall accuracy? Which is the better accuracy to use to reflect the ability to classify this data?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe balanced accuracy is 0.8. This is a better reflection on the predictive ability of the model for this data because it reflects the difficulty in predicting the Chinstrap group.\n\n\n\n\n\n\n6. This question relates to feature engineering, creating better variables on which to build your model.\n\nThe following spam data has a heavily skewed distribution for the size of the email message. How would you transform this variable to better see differences between spam and ham emails?\n\n\nlibrary(ggplot2)\nlibrary(ggbeeswarm)\nspam &lt;- read_csv(\"http://ggobi.org/book/data/spam.csv\")\nggplot(spam, aes(x=spam, y=size.kb, colour=spam)) +\n  geom_quasirandom() +\n  scale_color_brewer(\"\", palette = \"Dark2\") + \n  coord_flip() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nggplot(spam, aes(x=spam, y=size.kb, colour=spam)) +\n  geom_quasirandom() +\n  scale_color_brewer(\"\", palette = \"Dark2\") + \n  coord_flip() +\n  theme(legend.position=\"none\") +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the following data, how would you construct a new single variable which would capture the difference between the two classes using a linear model?\n\n\nolive &lt;- read_csv(\"http://ggobi.org/book/data/olive.csv\") |&gt;\n  dplyr::filter(region != 1) |&gt;\n  dplyr::select(region, arachidic, linoleic) |&gt;\n  mutate(region = factor(region))\nggplot(olive, aes(x=linoleic, \n                  y=arachidic, \n                  colour=region)) +\n  geom_point() +\n  scale_color_brewer(\"\", palette = \"Dark2\") + \n   theme(legend.position=\"none\", \n        aspect.ratio=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nolive &lt;- olive |&gt;\n  mutate(linoarch = 0.377 * linoleic + \n           0.926 * arachidic)\nggplot(olive, aes(x=region, \n                  y=linoarch, \n                  colour=region)) +\n  geom_quasirandom() +\n  scale_color_brewer(\"\", palette = \"Dark2\") + \n  coord_flip() +\n  theme(legend.position=\"none\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7. Discuss with your neighbour, what you found the most difficult part of last weekâ€™s content. Find some material (from resources or googling) together that gives alternative explanations that make it clearer."
  },
  {
    "objectID": "week2/tutorialsol.html#finishing-up",
    "href": "week2/tutorialsol.html#finishing-up",
    "title": "ETC3250/5250 Tutorial 2",
    "section": "ðŸ‘‹ Finishing up",
    "text": "ðŸ‘‹ Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week2/tutorial.html",
    "href": "week2/tutorial.html",
    "title": "ETC3250/5250 Tutorial 2",
    "section": "",
    "text": "The goal for this week is for you to learn and practice some of the basics of machine learning."
  },
  {
    "objectID": "week2/tutorial.html#objectives",
    "href": "week2/tutorial.html#objectives",
    "title": "ETC3250/5250 Tutorial 2",
    "section": "",
    "text": "The goal for this week is for you to learn and practice some of the basics of machine learning."
  },
  {
    "objectID": "week2/tutorial.html#preparation",
    "href": "week2/tutorial.html#preparation",
    "title": "ETC3250/5250 Tutorial 2",
    "section": "ðŸ”§ Preparation",
    "text": "ðŸ”§ Preparation\n\nComplete the quiz\nDo the reading related to week 1"
  },
  {
    "objectID": "week2/tutorial.html#exercises",
    "href": "week2/tutorial.html#exercises",
    "title": "ETC3250/5250 Tutorial 2",
    "section": "Exercises:",
    "text": "Exercises:\nOpen your project for this unit called iml.Rproj.\n\n1. Answer the following questions for this data matrix,\n\\[\\begin{align*}\n{\\mathbf X} = \\left[\\begin{array}{rrrrr}\n2 & -2 & -8 & 6 & -7 \\\\\n6 & 6 & -4 & 9 & 6 \\\\\n5 & 4 & 3 & -7 & 8 \\\\\n1 & -7 & 6 & 7 & -1\n\\end{array}\\right]\n\\end{align*}\\]\n\nWhat is \\(X_1\\) (variable 1)?\n\n\nWhat is observation 3?\n\n\nWhat is \\(n\\)?\n\n\nWhat is \\(p\\)?\n\n\nWhat is \\(X^\\top\\)?\n\n\nWrite a projection matrix which would generate a 2D projection where the first data projection has variables 1 and 4 combined equally, and the second data projection has one third of variable 2 and two thirds of 5.\n\n\nWhy canâ€™t the following matrix considered a projection matrix?\n\n\\[\\begin{align*}\n{\\mathbf A} = \\left[\\begin{array}{rr}\n-1/\\sqrt{2} & 1/\\sqrt{3} \\\\\n0 & 0  \\\\\n1/\\sqrt{2} & 0 \\\\\n0 & \\sqrt{2}/\\sqrt{3} \\\\\n\\end{array}\\right]\n\\end{align*}\\]\n\n\n2. Which of these statements is the most accurate? And which is the most precise?\nA. It is almost certain to rain in the next week.\nB. It is 90% likely to get at least 10mm of rain tomorrow.\n\n\n3. For the following data, make an appropriate training test split of 60:40. The response variable is cause. Deomstrate that you have made an appropriate split.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(rsample)\n\nbushfires &lt;- read_csv(\"https://raw.githubusercontent.com/dicook/mulgar_book/pdf/data/bushfires_2019-2020.csv\")\nbushfires |&gt; count(cause)\n\n# A tibble: 4 Ã— 2\n  cause           n\n  &lt;chr&gt;       &lt;int&gt;\n1 accident      138\n2 arson          37\n3 burning_off     9\n4 lightning     838\n\n\n\n\n4. In the lecture slides from week 1 on bias vs variance, these four images were shown.\n \n \nMark the images with the labels â€œtrue modelâ€, â€œfitted modelâ€, â€œbiasâ€. Then explain in your own words why the different model shown in each has (potentially) large bias or small bias, and small variance or large variance.\n\n\n5. The following data contains true class and predictive probabilities for a model fit. Answer the questions below for this data.\n\npred_data &lt;- read_csv(\"https://raw.githubusercontent.com/numbats/iml/master/data/pred_data.csv\") |&gt;\n  mutate(true = factor(true))\n\n\nHow many classes?\n\n\nCompute the confusion table, using the maximum predictive probability to label the observation.\n\n\nCompute the accuracy, and accuracy if all observations were classified as Adelie. Why is the accuracy almost as good when all observations are predicted to be the majority class?\n\n\nCompute the balanced accuracy, by averaging the class errors. Why is it lower than the overall accuracy? Which is the better accuracy to use to reflect the ability to classify this data?\n\n\n\n6. This question relates to feature engineering, creating better variables on which to build your model.\n\nThe following spam data has a heavily skewed distribution for the size of the email message. How would you transform this variable to better see differences between spam and ham emails?\n\n\nlibrary(ggplot2)\nlibrary(ggbeeswarm)\nspam &lt;- read_csv(\"http://ggobi.org/book/data/spam.csv\")\nggplot(spam, aes(x=spam, y=size.kb, colour=spam)) +\n  geom_quasirandom() +\n  scale_color_brewer(\"\", palette = \"Dark2\") + \n  coord_flip() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\nFor the following data, how would you construct a new single variable which would capture the difference between the two classes using a linear model?\n\n\nolive &lt;- read_csv(\"http://ggobi.org/book/data/olive.csv\") |&gt;\n  dplyr::filter(region != 1) |&gt;\n  dplyr::select(region, arachidic, linoleic) |&gt;\n  mutate(region = factor(region))\nggplot(olive, aes(x=linoleic, \n                  y=arachidic, \n                  colour=region)) +\n  geom_point() +\n  scale_color_brewer(\"\", palette = \"Dark2\") + \n   theme(legend.position=\"none\", \n        aspect.ratio=1)\n\n\n\n\n\n\n\n\n\n\n7. Discuss with your neighbour, what you found the most difficult part of last weekâ€™s content. Find some material (from resources or googling) together that gives alternative explanations that make it clearer."
  },
  {
    "objectID": "week2/tutorial.html#finishing-up",
    "href": "week2/tutorial.html#finishing-up",
    "title": "ETC3250/5250 Tutorial 2",
    "section": "ðŸ‘‹ Finishing up",
    "text": "ðŸ‘‹ Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week3/tutorialsol.html",
    "href": "week3/tutorialsol.html",
    "title": "ETC3250/5250 Tutorial 3",
    "section": "",
    "text": "Load the libraries and avoid conflicts\n# Load libraries used everywhere\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(conflicted)\nlibrary(colorspace)\nlibrary(patchwork)\nlibrary(MASS)\nlibrary(randomForest)\nlibrary(gridExtra)\nlibrary(GGally)\nlibrary(geozoo)\nlibrary(mulgar)\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(dplyr::select)\nconflicts_prefer(dplyr::slice)\nconflicts_prefer(palmerpenguins::penguins)\nconflicts_prefer(tourr::flea)"
  },
  {
    "objectID": "week3/tutorialsol.html#objectives",
    "href": "week3/tutorialsol.html#objectives",
    "title": "ETC3250/5250 Tutorial 3",
    "section": "ðŸŽ¯ Objectives",
    "text": "ðŸŽ¯ Objectives\nThe goal for this week is for you to learn and practice visualising high-dimensional data."
  },
  {
    "objectID": "week3/tutorialsol.html#preparation",
    "href": "week3/tutorialsol.html#preparation",
    "title": "ETC3250/5250 Tutorial 3",
    "section": "ðŸ”§ Preparation",
    "text": "ðŸ”§ Preparation\n\nComplete the quiz\nDo the reading related to week 2"
  },
  {
    "objectID": "week3/tutorialsol.html#exercises",
    "href": "week3/tutorialsol.html#exercises",
    "title": "ETC3250/5250 Tutorial 3",
    "section": "Exercises:",
    "text": "Exercises:\nOpen your project for this unit called iml.Rproj.\n\n1. The sparseness of high dimensions\nRandomly generate data points that are uniformly distributed in a hyper-cube of 3, 5 and 10 dimensions, with 500 points in each sample, using the cube.solid.random function of the geozoo package. What differences do we expect to see? Now visualise each set in a grand tour and describe how they differ, and whether this matched your expectations?\nThe code to generate and view the cubes is:\n\n\nCode to generate the data and show in a tour\nlibrary(tourr)\nlibrary(geozoo)\nset.seed(1234)\ncube3 &lt;- cube.solid.random(3, 500)$points\ncube5 &lt;- cube.solid.random(5, 500)$points\ncube10 &lt;- cube.solid.random(10, 500)$points\n\nanimate_xy(cube3, axes=\"bottomleft\")\nanimate_xy(cube5, axes=\"bottomleft\")\nanimate_xy(cube10, axes=\"bottomleft\")\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nEach of the projections has a boxy shape, which gets less distinct as the dimension increases.\nAs the dimension increases, the points tend to concentrate in the centre of the plot window, with a smattering of points in the edges.\n\n\n\n\n\n\n2. Detecting clusters\nFor the data sets, c1, c3 from the mulgar package, use the grand tour to view and try to identify structure (outliers, clusters, non-linear relationships).\n\n\nCode to show in a tour\nanimate_xy(c1)\nanimate_xy(c3)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe first data set c1 has 6 clusters, 4 small ones, and two big ones. The two big ones look like planes because they have no variation in some dimensions.\nThe second data set c2 has a triangular prism shape, which itself is divided into several smaller triangular prisms. It also has several dimensions with no variation, because the points collapse into a line in some projections.\n\n\n\n\n\n\n3. Effect of covariance\nExamine 5D multivariate normal samples drawn from populations with a range of variance-covariance matrices. (You can use the mvtnorm package to do the sampling, for example.) Examine the data using a grand tour. What changes when you change the correlation from close to zero to close to 1? Can you see a difference between strong positive correlation and strong negative correlation?\n\n\nCode to generate the samples\nlibrary(mvtnorm)\nset.seed(501)\n\ns1 &lt;- diag(5)\ns2 &lt;- diag(5)\ns2[3,4] &lt;- 0.7\ns2[4,3] &lt;- 0.7\ns3 &lt;- s2\ns3[1,2] &lt;- -0.7\ns3[2,1] &lt;- -0.7\n\ns1\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    0    0    0    0\n[2,]    0    1    0    0    0\n[3,]    0    0    1    0    0\n[4,]    0    0    0    1    0\n[5,]    0    0    0    0    1\n\n\nCode to generate the samples\ns2\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    0  0.0  0.0    0\n[2,]    0    1  0.0  0.0    0\n[3,]    0    0  1.0  0.7    0\n[4,]    0    0  0.7  1.0    0\n[5,]    0    0  0.0  0.0    1\n\n\nCode to generate the samples\ns3\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  1.0 -0.7  0.0  0.0    0\n[2,] -0.7  1.0  0.0  0.0    0\n[3,]  0.0  0.0  1.0  0.7    0\n[4,]  0.0  0.0  0.7  1.0    0\n[5,]  0.0  0.0  0.0  0.0    1\n\n\nCode to generate the samples\nset.seed(1234)\nd1 &lt;- as.data.frame(rmvnorm(500, sigma = s1))\nd2 &lt;- as.data.frame(rmvnorm(500, sigma = s2))\nd3 &lt;- as.data.frame(rmvnorm(500, sigma = s3))\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nanimate_xy(d1)\nanimate_xy(d2)\nanimate_xy(d3)\n\nThe points in data d1 are pretty spread in every projection. For the data d2, d3 have some projections where the data is concentrated along a line. This should be seen to be when variables 3 and 4 are contributing to the projection in d2, and when variables 1, 2, 3, 4 contributing to the projection in d3.\n\n\n\n\n\n\n4. Principal components analysis on the simulated data\nðŸ§ For data sets d2 and d3 what would you expect would be the number of PCs suggested by PCA?\nðŸ‘¨ðŸ½â€ðŸ’»ðŸ‘©â€ðŸ’»Conduct the PCA. Report the variances (eigenvalues), and cumulative proportions of total variance, make a scree plot, and the PC coefficients.\nðŸ¤¯Often, the selected number of PCs are used in future work. For both d3 and d4, think about the pros and cons of using 4 PCs and 3 PCs, respectively.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThinking about it: In d2 there is strong correlation between variables 3 and 4, which means probably only 4PC s would be needed. In d3 there is strong correlation also between variables 1 and 2 which would mean that only 3 PCs would be needed.\n\nd2_pca &lt;- prcomp(d2, scale=TRUE)\nd2_pca\n\nStandard deviations (1, .., p=5):\n[1] 1.2944925 1.0120246 0.9995775 0.9840652 0.5766767\n\nRotation (n x k) = (5 x 5):\n           PC1         PC2          PC3         PC4         PC5\nV1 0.009051897  0.60982755  0.600760775  0.51637067 -0.02182300\nV2 0.042039564  0.44070702 -0.798335151  0.40808929  0.01158053\nV3 0.702909484  0.03224989  0.034228444 -0.06034512  0.70715280\nV4 0.702411571  0.03021836  0.002269932 -0.08050218 -0.70655437\nV5 0.103377852 -0.65721722  0.023890154  0.74612487 -0.01027051\n\nd2_pca$sdev^2/5\n\n[1] 0.33514216 0.20483875 0.19983102 0.19367686 0.06651121\n\nmulgar::ggscree(d2_pca, q=5)\n\n\n\n\n\n\n\n\nFour PCs explain 93% of the variation. PC1 is the combination of variables 3 and 4, which captures this reduced dimension.\n\nd3_pca &lt;- prcomp(d3, scale=TRUE)\nd3_pca\n\nStandard deviations (1, .., p=5):\n[1] 1.3262816 1.2831152 0.9984103 0.5561311 0.5371102\n\nRotation (n x k) = (5 x 5):\n           PC1         PC2          PC3         PC4          PC5\nV1  0.47372917  0.52551030  0.007091154 -0.55745578  0.434295265\nV2 -0.49362867 -0.50367594 -0.047544823 -0.58444458  0.398503844\nV3 -0.50057768  0.49960926  0.030888892 -0.40488840 -0.578726039\nV4 -0.52968729  0.46318477  0.073441704  0.42649507  0.563559684\nV5  0.02765464 -0.07745919  0.995661287 -0.04283613 -0.007678753\n\nd3_pca$sdev^2/5\n\n[1] 0.35180458 0.32927695 0.19936462 0.06185637 0.05769748\n\nmulgar::ggscree(d3_pca, q=5)\n\n\n\n\n\n\n\n\nThree PCs explain 88% of the variation, and the last two PCs have much smaller variance than the others. PC 1 and 2 are combinations of variables 1, 2, 3 and 4, which captures this reduced dimension, and PC 3 is primarily variable 5.\nThe PCs are awkward combinations of the original variables. For d2, it would make sense to use PC1 (or equivalently and equal combination of V3, V4), and then keep the original variables V1, V2, V5.\nFor d3 itâ€™s harder to make this call because the first two PCs are combinations of four variables. Its hard to see from this that the ideal solution would be to use an equal combination of V1, V2, and equal combination of V3, V4 and V5 on its own.\nOften understanding the variance that is explained by the PCs is hard to interpret.\n\n\n\n\n\n\n5. PCA on cross-currency time series\nThe rates.csv data has 152 currencies relative to the USD for the period of Nov 1, 2019 through to Mar 31, 2020. Treating the dates as variables, conduct a PCA to examine how the cross-currencies vary, focusing on this subset: ARS, AUD, BRL, CAD, CHF, CNY, EUR, FJD, GBP, IDR, INR, ISK, JPY, KRW, KZT, MXN, MYR, NZD, QAR, RUB, SEK, SGD, UYU, ZAR.\n\nrates &lt;- read_csv(\"https://raw.githubusercontent.com/numbats/iml/master/data/rates_Nov19_Mar20.csv\") |&gt;\n  select(date, ARS, AUD, BRL, CAD, CHF, CNY, EUR, FJD, GBP, IDR, INR, ISK, JPY, KRW, KZT, MXN, MYR, NZD, QAR, RUB, SEK, SGD, UYU, ZAR)\n\n\nStandardise the currency columns to each have mean 0 and variance 1. Explain why this is necessary prior to doing the PCA or is it? Use this data to make a time series plot overlaying all of the cross-currencies.\n\n\n\nCode to standardise currencies\nlibrary(plotly)\nrates_std &lt;- rates |&gt;\n  mutate_if(is.numeric, function(x) (x-mean(x))/sd(x))\nrownames(rates_std) &lt;- rates_std$date\np &lt;- rates_std |&gt;\n  pivot_longer(cols=ARS:ZAR, \n               names_to = \"currency\", \n               values_to = \"rate\") |&gt;\n  ggplot(aes(x=date, y=rate, \n             group=currency, label=currency)) +\n    geom_line() \nggplotly(p, width=400, height=300)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nIt isnâ€™t necessary to standardise the variables before using the prcomp function because we can set scale=TRUE to have it done as part of the PCA computation. However, it is useful to standardise the variables to make the time series plot where all the currencies are drawn. This is useful for interpreting the principal components.\n\n\n\n\n\nConduct a PCA. Make a scree plot, and summarise proportion of the total variance. Summarise these values and the coefficients for the first five PCs, nicely.\n\n\n\nCode to do PCA and screeplot\nrates_pca &lt;- prcomp(rates_std[,-1], scale=FALSE)\nmulgar::ggscree(rates_pca, q=24)\noptions(digits=2)\nsummary(rates_pca)\n\n\n\n\nCode to make a nice summary\n# Summarise the coefficients nicely\nrates_pca_smry &lt;- tibble(evl=rates_pca$sdev^2) |&gt;\n  mutate(p = evl/sum(evl), \n         cum_p = cumsum(evl/sum(evl))) |&gt; \n  t() |&gt;\n  as.data.frame()\ncolnames(rates_pca_smry) &lt;- colnames(rates_pca$rotation)\nrates_pca_smry &lt;- bind_rows(as.data.frame(rates_pca$rotation),\n                            rates_pca_smry)\nrownames(rates_pca_smry) &lt;- c(rownames(rates_pca$rotation),\n                              \"Variance\", \"Proportion\", \n                              \"Cum. prop\")\nrates_pca_smry[,1:5]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportance of components:\n                         PC1   PC2    PC3    PC4    PC5    PC6     PC7     PC8\nStandard deviation     4.193 1.679 1.0932 0.9531 0.7358 0.5460 0.38600 0.33484\nProportion of Variance 0.733 0.118 0.0498 0.0379 0.0226 0.0124 0.00621 0.00467\nCumulative Proportion  0.733 0.850 0.8999 0.9377 0.9603 0.9727 0.97893 0.98360\n                           PC9    PC10    PC11    PC12    PC13    PC14    PC15\nStandard deviation     0.30254 0.25669 0.25391 0.17893 0.16189 0.15184 0.14260\nProportion of Variance 0.00381 0.00275 0.00269 0.00133 0.00109 0.00096 0.00085\nCumulative Proportion  0.98741 0.99016 0.99284 0.99418 0.99527 0.99623 0.99708\n                          PC16    PC17    PC18    PC19    PC20    PC21    PC22\nStandard deviation     0.11649 0.10691 0.09923 0.09519 0.08928 0.07987 0.07222\nProportion of Variance 0.00057 0.00048 0.00041 0.00038 0.00033 0.00027 0.00022\nCumulative Proportion  0.99764 0.99812 0.99853 0.99891 0.99924 0.99950 0.99972\n                          PC23    PC24\nStandard deviation     0.05985 0.05588\nProportion of Variance 0.00015 0.00013\nCumulative Proportion  0.99987 1.00000\n\n\n\n\n              PC1    PC2      PC3    PC4     PC5\nARS         0.215 -0.121  0.19832  0.181 -0.2010\nAUD         0.234  0.013  0.11466  0.018  0.0346\nBRL         0.229 -0.108  0.10513  0.093 -0.0526\nCAD         0.235 -0.025 -0.02659 -0.037  0.0337\nCHF        -0.065  0.505 -0.33521 -0.188 -0.0047\nCNY         0.144  0.237 -0.45337 -0.238 -0.5131\nEUR         0.088  0.495  0.24474  0.245 -0.1416\nFJD         0.234  0.055  0.04470  0.028  0.0330\nGBP         0.219  0.116 -0.00915 -0.073  0.3059\nIDR         0.218 -0.022 -0.24905 -0.117  0.2362\nINR         0.223 -0.147 -0.00734 -0.014  0.0279\nISK         0.230 -0.016  0.10979  0.093  0.1295\nJPY        -0.022  0.515  0.14722  0.234  0.3388\nKRW         0.214  0.063  0.17488  0.059 -0.3404\nKZT         0.217  0.013 -0.23244 -0.119  0.3304\nMXN         0.229 -0.059 -0.13804 -0.102  0.2048\nMYR         0.227  0.040 -0.13970 -0.115 -0.2009\nNZD         0.230  0.061  0.04289 -0.056 -0.0354\nQAR        -0.013  0.111  0.55283 -0.807  0.0078\nRUB         0.233 -0.102 -0.05863 -0.042  0.0063\nSEK         0.205  0.240  0.07570  0.085  0.0982\nSGD         0.227  0.057  0.14225  0.115 -0.2424\nUYU         0.231 -0.101  0.00064 -0.053  0.0957\nZAR         0.232 -0.070 -0.00328  0.042 -0.0443\nVariance   17.582  2.820  1.19502  0.908  0.5413\nProportion  0.733  0.118  0.04979  0.038  0.0226\nCum. prop   0.733  0.850  0.89989  0.938  0.9603\n\n\n\nThe first two principal components explain 85% of the total variation.\nPC1 is a combination of all of the currencies except for CHF, EUR, JPY, QAR.\nPC2 is a combination of CHF, EUR, JPY.\n\n\n\n\n\n\nMake a biplot of the first two PCs. Explain what you learn.\n\n\n\nBiplot code\nlibrary(ggfortify)\nautoplot(rates_pca, loadings = TRUE, \n         loadings.label = TRUE) \n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost of the currencies contribute substantially to PC1. Only three contribute strongly to PC2: CHF, JPY, EUR. Similar to what is learned from the summary table (made in b).\nThe pattern of the points is most unusual! It has a curious S shape. Principal components are supposed to be a random scattering of values, with no obvious structure. This is a very strong pattern.\n\n\n\n\n\n\nMake a time series plot of PC1 and PC2. Explain why this is useful to do for this data.\n\n\n\nCode to plot PCs\nrates_pca$x |&gt;\n  as.data.frame() |&gt;\n  mutate(date = rates_std$date) |&gt;\n  ggplot(aes(x=date, y=PC1)) + geom_line()\n\nrates_pca$x |&gt;\n  as.data.frame() |&gt;\n  mutate(date = rates_std$date) |&gt;\n  ggplot(aes(x=date, y=PC2)) + geom_line()\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBecause there is a strong pattern in the first two PCs, it could be useful to understand if this is related to the temporal context of the data.\nHere we might expect that the PCs extract the main temporal patterns. We see this is the case.\nPC1 reflects the large group of currencies that greatly increase in mid-March.\nPC2 reflects the few currencies that decrease at the start of March.\n\nNote that: increase here means that the value of the currency declines relative to the USD and a decrease indicates stronger relative to the USD. Is this correct?\n\n\n\n\n\nYouâ€™ll want to drill down deeper to understand what the PCA tells us about the movement of the various currencies, relative to the USD, over the volatile period of the COVID pandemic. Plot the first two PCs again, but connect the dots in order of time. Make it interactive with plotly, where the dates are the labels. What does following the dates tell us about the variation captured in the first two principal components?\n\n\n\nCode to use interaction of the PC plot\nlibrary(plotly)\np2 &lt;- rates_pca$x |&gt;\n  as.data.frame() |&gt;\n  mutate(date = rates_std$date) |&gt;\n  ggplot(aes(x=PC1, y=PC2, label=date)) +\n    geom_point() +\n    geom_path()\nggplotly(p2, width=400, height=400)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\nThe pattern in PC1 vs PC2 follows time. Prior to the pandemic there is a tangle of values on the left. Towards the end of February, when the world was starting to realise that COVID was a major health threat, there is a dramatic reaction from the world currencies, at least in relation to the USD. Currencies such as EUR, JPY, CHF reacted first, gaining strength relative to USD, and then they lost that strength. Most other currencies reacted later, losing value relative to the USD.\n\n\n\n\n\n\n6. Write a simple question about the weekâ€™s material and test your neighbour, or your tutor."
  },
  {
    "objectID": "week3/tutorialsol.html#finishing-up",
    "href": "week3/tutorialsol.html#finishing-up",
    "title": "ETC3250/5250 Tutorial 3",
    "section": "ðŸ‘‹ Finishing up",
    "text": "ðŸ‘‹ Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week3/tutorial.html",
    "href": "week3/tutorial.html",
    "title": "ETC3250/5250 Tutorial 3",
    "section": "",
    "text": "Load the libraries and avoid conflicts\n# Load libraries used everywhere\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(conflicted)\nlibrary(colorspace)\nlibrary(patchwork)\nlibrary(MASS)\nlibrary(randomForest)\nlibrary(gridExtra)\nlibrary(GGally)\nlibrary(geozoo)\nlibrary(mulgar)\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(dplyr::select)\nconflicts_prefer(dplyr::slice)\nconflicts_prefer(palmerpenguins::penguins)\nconflicts_prefer(tourr::flea)"
  },
  {
    "objectID": "week3/tutorial.html#objectives",
    "href": "week3/tutorial.html#objectives",
    "title": "ETC3250/5250 Tutorial 3",
    "section": "ðŸŽ¯ Objectives",
    "text": "ðŸŽ¯ Objectives\nThe goal for this week is for you to learn and practice visualising high-dimensional data."
  },
  {
    "objectID": "week3/tutorial.html#preparation",
    "href": "week3/tutorial.html#preparation",
    "title": "ETC3250/5250 Tutorial 3",
    "section": "ðŸ”§ Preparation",
    "text": "ðŸ”§ Preparation\n\nComplete the quiz\nDo the reading related to week 2"
  },
  {
    "objectID": "week3/tutorial.html#exercises",
    "href": "week3/tutorial.html#exercises",
    "title": "ETC3250/5250 Tutorial 3",
    "section": "Exercises:",
    "text": "Exercises:\nOpen your project for this unit called iml.Rproj.\n\n1. The sparseness of high dimensions\nRandomly generate data points that are uniformly distributed in a hyper-cube of 3, 5 and 10 dimensions, with 500 points in each sample, using the cube.solid.random function of the geozoo package. What differences do we expect to see? Now visualise each set in a grand tour and describe how they differ, and whether this matched your expectations?\nThe code to generate and view the cubes is:\n\n\nCode to generate the data and show in a tour\nlibrary(tourr)\nlibrary(geozoo)\nset.seed(1234)\ncube3 &lt;- cube.solid.random(3, 500)$points\ncube5 &lt;- cube.solid.random(5, 500)$points\ncube10 &lt;- cube.solid.random(10, 500)$points\n\nanimate_xy(cube3, axes=\"bottomleft\")\nanimate_xy(cube5, axes=\"bottomleft\")\nanimate_xy(cube10, axes=\"bottomleft\")\n\n\n\n\n2. Detecting clusters\nFor the data sets, c1, c3 from the mulgar package, use the grand tour to view and try to identify structure (outliers, clusters, non-linear relationships).\n\n\nCode to show in a tour\nanimate_xy(c1)\nanimate_xy(c3)\n\n\n\n\n3. Effect of covariance\nExamine 5D multivariate normal samples drawn from populations with a range of variance-covariance matrices. (You can use the mvtnorm package to do the sampling, for example.) Examine the data using a grand tour. What changes when you change the correlation from close to zero to close to 1? Can you see a difference between strong positive correlation and strong negative correlation?\n\n\nCode to generate the samples\nlibrary(mvtnorm)\nset.seed(501)\n\ns1 &lt;- diag(5)\ns2 &lt;- diag(5)\ns2[3,4] &lt;- 0.7\ns2[4,3] &lt;- 0.7\ns3 &lt;- s2\ns3[1,2] &lt;- -0.7\ns3[2,1] &lt;- -0.7\n\ns1\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    0    0    0    0\n[2,]    0    1    0    0    0\n[3,]    0    0    1    0    0\n[4,]    0    0    0    1    0\n[5,]    0    0    0    0    1\n\n\nCode to generate the samples\ns2\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    0  0.0  0.0    0\n[2,]    0    1  0.0  0.0    0\n[3,]    0    0  1.0  0.7    0\n[4,]    0    0  0.7  1.0    0\n[5,]    0    0  0.0  0.0    1\n\n\nCode to generate the samples\ns3\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  1.0 -0.7  0.0  0.0    0\n[2,] -0.7  1.0  0.0  0.0    0\n[3,]  0.0  0.0  1.0  0.7    0\n[4,]  0.0  0.0  0.7  1.0    0\n[5,]  0.0  0.0  0.0  0.0    1\n\n\nCode to generate the samples\nset.seed(1234)\nd1 &lt;- as.data.frame(rmvnorm(500, sigma = s1))\nd2 &lt;- as.data.frame(rmvnorm(500, sigma = s2))\nd3 &lt;- as.data.frame(rmvnorm(500, sigma = s3))\n\n\n\n\n4. Principal components analysis on the simulated data\nðŸ§ For data sets d2 and d3 what would you expect would be the number of PCs suggested by PCA?\nðŸ‘¨ðŸ½â€ðŸ’»ðŸ‘©â€ðŸ’»Conduct the PCA. Report the variances (eigenvalues), and cumulative proportions of total variance, make a scree plot, and the PC coefficients.\nðŸ¤¯Often, the selected number of PCs are used in future work. For both d3 and d4, think about the pros and cons of using 4 PCs and 3 PCs, respectively.\n\n\n5. PCA on cross-currency time series\nThe rates.csv data has 152 currencies relative to the USD for the period of Nov 1, 2019 through to Mar 31, 2020. Treating the dates as variables, conduct a PCA to examine how the cross-currencies vary, focusing on this subset: ARS, AUD, BRL, CAD, CHF, CNY, EUR, FJD, GBP, IDR, INR, ISK, JPY, KRW, KZT, MXN, MYR, NZD, QAR, RUB, SEK, SGD, UYU, ZAR.\n\nrates &lt;- read_csv(\"https://raw.githubusercontent.com/numbats/iml/master/data/rates_Nov19_Mar20.csv\") |&gt;\n  select(date, ARS, AUD, BRL, CAD, CHF, CNY, EUR, FJD, GBP, IDR, INR, ISK, JPY, KRW, KZT, MXN, MYR, NZD, QAR, RUB, SEK, SGD, UYU, ZAR)\n\n\nStandardise the currency columns to each have mean 0 and variance 1. Explain why this is necessary prior to doing the PCA or is it? Use this data to make a time series plot overlaying all of the cross-currencies.\n\n\n\nCode to standardise currencies\nlibrary(plotly)\nrates_std &lt;- rates |&gt;\n  mutate_if(is.numeric, function(x) (x-mean(x))/sd(x))\nrownames(rates_std) &lt;- rates_std$date\np &lt;- rates_std |&gt;\n  pivot_longer(cols=ARS:ZAR, \n               names_to = \"currency\", \n               values_to = \"rate\") |&gt;\n  ggplot(aes(x=date, y=rate, \n             group=currency, label=currency)) +\n    geom_line() \nggplotly(p, width=400, height=300)\n\n\n\nConduct a PCA. Make a scree plot, and summarise proportion of the total variance. Summarise these values and the coefficients for the first five PCs, nicely.\n\n\n\nCode to do PCA and screeplot\nrates_pca &lt;- prcomp(rates_std[,-1], scale=FALSE)\nmulgar::ggscree(rates_pca, q=24)\noptions(digits=2)\nsummary(rates_pca)\n\n\n\n\nCode to make a nice summary\n# Summarise the coefficients nicely\nrates_pca_smry &lt;- tibble(evl=rates_pca$sdev^2) |&gt;\n  mutate(p = evl/sum(evl), \n         cum_p = cumsum(evl/sum(evl))) |&gt; \n  t() |&gt;\n  as.data.frame()\ncolnames(rates_pca_smry) &lt;- colnames(rates_pca$rotation)\nrates_pca_smry &lt;- bind_rows(as.data.frame(rates_pca$rotation),\n                            rates_pca_smry)\nrownames(rates_pca_smry) &lt;- c(rownames(rates_pca$rotation),\n                              \"Variance\", \"Proportion\", \n                              \"Cum. prop\")\nrates_pca_smry[,1:5]\n\n\n\nMake a biplot of the first two PCs. Explain what you learn.\n\n\n\nBiplot code\nlibrary(ggfortify)\nautoplot(rates_pca, loadings = TRUE, \n         loadings.label = TRUE) \n\n\n\nMake a time series plot of PC1 and PC2. Explain why this is useful to do for this data.\n\n\n\nCode to plot PCs\nrates_pca$x |&gt;\n  as.data.frame() |&gt;\n  mutate(date = rates_std$date) |&gt;\n  ggplot(aes(x=date, y=PC1)) + geom_line()\n\nrates_pca$x |&gt;\n  as.data.frame() |&gt;\n  mutate(date = rates_std$date) |&gt;\n  ggplot(aes(x=date, y=PC2)) + geom_line()\n\n\n\nYouâ€™ll want to drill down deeper to understand what the PCA tells us about the movement of the various currencies, relative to the USD, over the volatile period of the COVID pandemic. Plot the first two PCs again, but connect the dots in order of time. Make it interactive with plotly, where the dates are the labels. What does following the dates tell us about the variation captured in the first two principal components?\n\n\n\nCode to use interaction of the PC plot\nlibrary(plotly)\np2 &lt;- rates_pca$x |&gt;\n  as.data.frame() |&gt;\n  mutate(date = rates_std$date) |&gt;\n  ggplot(aes(x=PC1, y=PC2, label=date)) +\n    geom_point() +\n    geom_path()\nggplotly(p2, width=400, height=400)\n\n\n\n\n6. Write a simple question about the weekâ€™s material and test your neighbour, or your tutor."
  },
  {
    "objectID": "week3/tutorial.html#finishing-up",
    "href": "week3/tutorial.html#finishing-up",
    "title": "ETC3250/5250 Tutorial 3",
    "section": "ðŸ‘‹ Finishing up",
    "text": "ðŸ‘‹ Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week5/index.html",
    "href": "week5/index.html",
    "title": "Week 5: Trees and forests",
    "section": "",
    "text": "ISLR 8.1, 8.2"
  },
  {
    "objectID": "week5/index.html#main-reference",
    "href": "week5/index.html#main-reference",
    "title": "Week 5: Trees and forests",
    "section": "",
    "text": "ISLR 8.1, 8.2"
  },
  {
    "objectID": "week5/index.html#what-you-will-learn-this-week",
    "href": "week5/index.html#what-you-will-learn-this-week",
    "title": "Week 5: Trees and forests",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nClassification trees, algorithm, stopping rules\nDifference between algorithm and parametric methods, especially trees vs LDA\nForests: ensembles of bagged trees\nDiagnostics: vote matrix, variable importance, proximity\nBoosted trees"
  },
  {
    "objectID": "week5/index.html#assignments",
    "href": "week5/index.html#assignments",
    "title": "Week 5: Trees and forests",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 2 is due on Friday 12 April."
  },
  {
    "objectID": "week7/index.html",
    "href": "week7/index.html",
    "title": "Week 7: Explainable artificial intelligence (XAI)",
    "section": "",
    "text": "Molnar 8.1, 8.5, 9.2-9.6"
  },
  {
    "objectID": "week7/index.html#main-reference",
    "href": "week7/index.html#main-reference",
    "title": "Week 7: Explainable artificial intelligence (XAI)",
    "section": "",
    "text": "Molnar 8.1, 8.5, 9.2-9.6"
  },
  {
    "objectID": "week7/index.html#what-you-will-learn-this-week",
    "href": "week7/index.html#what-you-will-learn-this-week",
    "title": "Week 7: Explainable artificial intelligence (XAI)",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nLocal explainability\nLIME\nShapley values"
  },
  {
    "objectID": "week7/index.html#assignments",
    "href": "week7/index.html#assignments",
    "title": "Week 7: Explainable artificial intelligence (XAI)",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 3 is due on Friday 26 April."
  },
  {
    "objectID": "week9/index.html",
    "href": "week9/index.html",
    "title": "Week 9: K-nearest neighbours and hierarchical clustering",
    "section": "",
    "text": "HOML Ch 20, 21"
  },
  {
    "objectID": "week9/index.html#main-reference",
    "href": "week9/index.html#main-reference",
    "title": "Week 9: K-nearest neighbours and hierarchical clustering",
    "section": "",
    "text": "HOML Ch 20, 21"
  },
  {
    "objectID": "week9/index.html#what-you-will-learn-this-week",
    "href": "week9/index.html#what-you-will-learn-this-week",
    "title": "Week 9: K-nearest neighbours and hierarchical clustering",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nDefining distance measure\nk-means algorithm\nHierarchical algorithms\nMaking and using dendrograms"
  },
  {
    "objectID": "week9/index.html#assignments",
    "href": "week9/index.html#assignments",
    "title": "Week 9: K-nearest neighbours and hierarchical clustering",
    "section": "Assignments",
    "text": "Assignments\n\nProject is due on Friday 17 May."
  }
]