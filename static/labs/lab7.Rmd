---
title: "Lab 7"
subtitle: "Monash University, Econ & Bus Stat, ETC3250/5250"
author: "prepared by Professor Di Cook"
date: "Materials for Week 7"
output:
  html_document: default
editor_options: 
  chunk_output_type: console
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  eval = FALSE,
  collapse = TRUE,
  comment = "#",
  fig.height = 4,
  fig.width = 6,
  fig.align = "center",
  cache = FALSE
)
```

# Objective

The objectives for this week are:

- Learn about fitting a forest model and use the diagnostics 
- Compare the classification tree with a forest
- Understand the the idea of bagging to refine model fitting

# Class discussion 

The focus this week is understanding how to determine the importance of variables in tree and forest models. 

*Question 1: In the following tree,  what would you says is the order of importance of the variables for classifying region?*

```{r eval=TRUE}
library(tidyverse)
library(rpart)
library(rpart.plot)
olive <- read_csv("http://www.ggobi.org/book/data/olive.csv") %>%
  rename(name=X1) %>%
  dplyr::select(-name, -area) %>%
  mutate(region = factor(region))
olive_rp <- rpart(region~., data=olive)
prp(olive_rp)
```

*Question 2: Read  the explanation of how variable importance is calculated from the original developers of the algorithm (Leo  Breiman and Adele Cutler), at https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm. Make a sketch to explain how this permutation approach can be used to measure variable importance.*

# Practice

1. Fit the tree to olive oils, using a training split of 2/3, using only regions 2, 3,  and  the predictors linoleic and arachidic. Report the training and test error, and make a plot of the boundary.

```{r out.width="100%", fig.width=8}
olive <- olive %>%
  filter(region != ???) %>%
  mutate(region = factor(region))  # need to reset levels
olive_sub <- olive %>%
  select(region, linoleic, ???)
library(caret)
set.seed(20200501)
tr_indx <- createDataPartition(???, times=10, p=???)
olive_tr <- olive_sub[tr_indx$Resample03,]
olive_ts <- olive_sub[-tr_indx$Resample03,]
olive_rp <- rpart(region~., data=olive_tr)
# Training error
1-confusionMatrix(olive_tr$region, predict(???, newdata=???, type="class"))$overall[1]
# Test error
1-confusionMatrix(olive_ts$region, predict???, newdata=olive_ts, type="class"))$overall[1]
olive_grid <- expand_grid(linoleic = seq(500, 1500, 10), 
                          arachidic = seq(0, 105, 1))
olive_grid$region = predict(olive_rp, newdata=???, type="class")
ggplot() +
  geom_point(data=olive_grid, aes(x=???, y=???, colour=???), alpha=0.1) +
  geom_point(data=olive_sub, aes(x=x=???, y=???, colour=???, shape=region), size=2) +
  scale_colour_brewer("region", palette="Dark2") +
  theme(aspect.ratio=1)
```

2. Fit a random forest to the full data, using only linoleic and arachidic as predictors, report the out-of-bag error, and make a plot of the boundary.

```{r}
library(randomForest)
olive_rf <- randomForest(region~., data=???, importance=TRUE)
olive_rf
olive_grid <- expand_grid(linoleic = seq(500, 1500, 10), 
                          arachidic = seq(0, 105, 1))
olive_grid$region = predict(???, newdata=???, type="class")
ggplot() +
  geom_point(data=olive_grid, aes(x=???, y=???, colour=???), alpha=0.1) +
  geom_point(data=olive_sub, aes(x=???, y=???, colour=???, shape=region), size=2) +
  scale_colour_brewer("region", palette="Dark2") +
  theme(aspect.ratio=1)
```


3. Explain the difference between the single tree and random forest boundaries.

4. Fit the random forest again to the full set of variables, and  compute the variable importance. Describe the order of importance of variables.

```{r}
olive_rf <- randomForest(region~., data=???, importance=???)
olive_rf
olive_rf$importance
```

5. Create a new variable called `linoarch` that is $0.377 \times linoleic + 0.926\times arachidic$. Make a plot of this variable against arachidic. Fit the tree model to the same training data using this variable in addition to linoleic and arachidic. Check the test error too. Why doesn't the tree use this new variable? It has a bigger difference between the two groups than linoleic?

```{r}
olive$linoarch <-???*olive$linoleic +???*olive$arachidic
olive_sub <- olive %>%
  select(???, ???, ???, linoarch)
ggplot() +
  geom_point(data=olive_sub, aes(x=???, y=???, colour=???, shape=region), size=2) +
  scale_colour_brewer("region", palette="Dark2") +
  theme(aspect.ratio=1)
olive_tr <- olive_sub[tr_indx$Resample03,]
olive_ts <- olive_sub[-tr_indx$Resample03,]
olive_rp <- rpart(region~., data=olive_tr) 
olive_rp
1-confusionMatrix(olive_ts$region, predict(olive_rp, newdata=???, type="class"))$overall[1]
olive_sub <- olive %>%
  select(region, ???, linoarch)
olive_tr <- olive_sub[tr_indx$Resample03,]
olive_ts <- olive_sub[-tr_indx$Resample03,]
olive_rp <- rpart(region~., data=???) 
olive_rp
1-confusionMatrix(olive_ts$region, predict(olive_rp, newdata=???, type="class"))$overall[1]
```


6. Fit the random forest again to the full set of variables, including linoarch and  compute the variable importance. Describe the order of importance of variables. Does the forest see the new variable?

```{r}
olive_rf <- randomForest(region~., data=???, importance=???)
olive_rf
olive_rf$importance
```

7. This last question is see how random forests  can be  used in really tough problems. We use a forest to analyse the happy paintings by Bob Ross. This was the subject of the [538 post](http://fivethirtyeight.com/features/a-statistical-analysis-of-the-work-of-bob-ross/), "A Statistical Analysis of the Work of Bob Ross".

We have taken the painting images from the [sales site](http://www.saleoilpaintings.com/paintings/bob-ross/bob-ross-sale-3_1.html), read the images into R, and resized them all to be 20 by 20 pixels. Each painting has been classified into one of 8 classes based on the title of the painting. This is the data that you will work with.

It is provided in wide and long form. Long form is good for making pictures of the original painting, and the wide form is what you will need to use for fitting the classification models. In wide form, each row corresponds to one painting, and the rgb color values at each pixel are in each column. With a $20\times20$ image, this leads to $400\times3=1200$ columns.

Here are three of the original paintings in the collection, labelled as "scene", "water", "flowers":

![bobross5](images/bobross5.jpg)
![bobross140](images/bobross140.jpg)
![bobross167](images/bobross167.jpg)

```{r fig.width=8, fig.height=2.5}
library(gridExtra)
paintings <- read_csv("data/paintings-train.csv")
paintings_long <- read_csv("data/paintings-long-train.csv")
paintings_test <- read_csv("data/paintings-test.csv")
df <- filter(paintings_long, id == 5)
p1 <- ggplot(data=df, aes(x, -y, fill=h)) + geom_tile() + 
  scale_fill_identity(labels=df$h) + theme_bw() +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank())
df <- filter(paintings_long, id == 140)
p2 <- ggplot(data=df, aes(x, -y, fill=h)) + geom_tile() + 
  scale_fill_identity(labels=df$h) + theme_bw() +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank())
df <- filter(paintings_long, id == 167)
p3 <- ggplot(data=df, aes(x, -y, fill=h)) + geom_tile() + 
  scale_fill_identity(labels=df$h) + theme_bw() +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank())
grid.arrange(p1, p2, p3, ncol=3)
```


a. How  many  paintings  in the training data? Explain the difference between the long and the wide format of the data. What is the dimension of the data, used for the classification?
b. Build a random forest for the training data, for two classes, `flowers` and `cold`. 
c. Predict the class of test set, report the error.
d. Which pixels are the most important for distinguishing these two types of paintings? 

```{r results='hide', fig.show='hide'}
p_sub <- paintings %>% 
  filter(class %in% c(???, ???)) %>% 
  arrange(class) %>%
  mutate(class = factor(class))
p_rf <- randomForest(class~., data=???, ntree=10000,
                     importance=TRUE)
p_rf

# Predict test
p_test_sub <- paintings_test %>% 
  filter(class %in% c((???, ???)) %>% 
  arrange(class) %>%
  mutate(class = factor(class))

table(predict(p_rf, newdata=???, type="class"), p_test_sub$class)

# Variable importance
p_rf_varimp <- p_rf$importance %>% 
  as_tibble() %>%
  mutate(var = rownames(p_rf$importance)) %>%
  arrange(desc(MeanDecreaseAccuracy))
p_rf_varimp %>% head()

```
