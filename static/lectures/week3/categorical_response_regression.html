<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>ETC3250: Categorical response</title>
    <meta charset="utf-8" />
    <meta name="author" content="  Professor Di Cook     Econometrics and Business Statistics   Monash University" />
    <link href="libs/remark-css/kunoichi.css" rel="stylesheet" />
    <link href="libs/remark-css/ninjutsu.css" rel="stylesheet" />
    <link href="libs/countdown/countdown.css" rel="stylesheet" />
    <script src="libs/countdown/countdown.js"></script>
    <link href="libs/font-awesome-animation/font-awesome-animation-emi.css" rel="stylesheet" />
    <script src="libs/fontawesome/js/fontawesome-all.min.js"></script>
    <link href="libs/font-awesome/css/fontawesome-all.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="mystyle.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# ETC3250: Categorical response
## Semester 1, 2020
### <br> Professor Di Cook <br> <br> Econometrics and Business Statistics <br> Monash University
### Week 3 (a)

---




# Categorical responses

In **classification**, the output `\(Y\)` is a .orange[categorical variable]. For example,

- Loan approval: `\(Y \in \{\mbox{successful}, \mbox{unsuccessful}\}\)` 
- Type of business culture: `\(Y \in \{\mbox{clan}, \mbox{adhocracy}, \mbox{market}, \mbox{hierarchical}\}\)`
- Historical document author: `\(Y \in \{\mbox{Austen}, \mbox{Dickens}, \mbox{Imitator}\}\)`
- Email: `\(Y \in \{\mbox{spam}, \mbox{ham}\}\)`

Map the categories to a numeric variable, or possibly a binary matrix.



---
layout: true
class: shuriken-full white 

.blade1.bg-green[.content.center.vmiddle[
A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?
]]
.blade2.bg-purple[.content.center.vmiddle[
An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history, and so forth.
]]
.blade3.bg-deep-orange[.content.center.vmiddle[
On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not.
]]
.blade4.bg-pink[.content.center.vmiddle[
An email comes into the server. Should it be moved into the inbox or the junk mail box, based on header text, sender, origin, time of day, ...?
]]

---

class: hide-blade2 hide-blade3 hide-blade4 hide-hole

---

class: hide-blade3 hide-blade4 hide-hole
count: false

---

class: hide-blade4 hide-hole
count: false

---
class: hide-hole
count: false
---
layout: false

class: split-two
layout: false

.column[.pad50px[

# When linear regression is not appropriate

&lt;br&gt;

Consider the following data `simcredit` which looks at the default status based on credit balance.

 **Question**: Why is a linear model not appropriate for this data?

<div class="countdown" id="timer_5e840616" style="right:0;bottom:0;left:0;margin:1%;padding:1px;font-size:1em;position: relative; width: min-content;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">00</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">30</span></code>
</div>

]]
.column[.content.vmiddle.center[


&lt;img src="categorical_response_regression_files/figure-html/unnamed-chunk-2-1.png" width="504" style="display: block; margin: auto;" /&gt;

]]

---
class: split-two

.column[.pad50px[

# Modelling binary responses

&lt;br&gt;

- To model **binary data**, we need to .orange[link] our **predictors** to our response using a *link function*.
- There are many different types of link functions we could use, but we will focus today on the .orange[logistic] link function.

]]
.column[.content.vmiddle.center[
&lt;img src="categorical_response_regression_files/figure-html/unnamed-chunk-3-1.png" width="504" style="display: block; margin: auto;" /&gt;
]]

---
class: split-two

.column[.pad50px[
# The logistic function

Instead of predicting the outcome directly, we instead predict the probability of being class 1, given the linear combination of predictors, using the .orange[logistic] link function.

$$ p(y=1|\beta_0 + \beta_1 x)  = f(\beta_0 + \beta_1 x) $$
where

`$$f(\beta_0 + \beta_1 x) = \frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$$`

]]
.column[.content.vmiddle.center[
&lt;img src="categorical_response_regression_files/figure-html/unnamed-chunk-4-1.png" width="504" style="display: block; margin: auto;" /&gt;

]]


---
class: split-two

.column[.pad50px[

## Logistic function

Transform the function: 

`$$~~~~f(x) = \frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$$`

`\(\rightarrow  f(x) = \frac{1}{1/e^{\beta_0+\beta_1x}+1}\)`

`\(\rightarrow  1/f(x) = 1/e^{\beta_0+\beta_1x}+1\)`

`\(\rightarrow 1/f(x) - 1 = 1/e^{\beta_0+\beta_1x}\)`

`\(\rightarrow  \frac{1}{1/f(x) - 1} = e^{\beta_0+\beta_1x}\)`

`\(\rightarrow ~~ ...\)`
]]
.column[.pad50px[

`\(\rightarrow ~~ ...\)`

`\(\rightarrow \frac{f(x)}{1 - f(x)} = e^{\beta_0+\beta_1x}\)`

`\(\rightarrow \log_e\frac{f(x)}{1 - f(x)} = \beta_0+\beta_1x\)`

&lt;br&gt;
&lt;br&gt;


The left-hand side `\(\log_e\frac{f(x)}{1 - f(x)}\)` is called the .orange[log-odds ratio] or logit.

<span>&lt;i class="fas  fa-dice fa-lg faa-bounce animated faa-slow " style=" color:orange;"&gt;&lt;/i&gt;</span>

]]
---
## The logistic regression model 

The fitted model, where `\(P(Y=0|X) = 1 - P(Y=1|X)\)`, is then written as:

.tip[
`\(\log_e\frac{P(Y=1|X)}{1 - P(Y=1|X)} = \beta_0+\beta_1X\)`
]

&lt;details&gt;
*Multiple categories*: 
  This formula can be extended to more than binary response variables. Writing the equation is not simple, but follows from   the above, extending it to provide probabilities for each level/category. The sum of all  probabilities is 1.
&lt;/details&gt;
---
## Interpretation

- .green[**Linear regression**]	
    - `\(\beta_1\)` gives the average change in `\(Y\)` associated with a one-unit increase in `\(X\)`
- .green[**Logistic regression**]
    - Increasing `\(X\)` by one unit changes the log odds by `\(\beta_1\)`, or equivalently it multiplies the odds by `\(e^{\beta_1}\)`
    - However, because the model is not linear in `\(X\)`, `\(\beta_1\)` does not correspond to the change in response associated with a one-unit increase in `\(X\)`
    
---
## Maximum Likelihood Estimation

Given the logistic `\(p(x_i) = \frac{1}{e^{-(\beta_0+\beta_1x_i)}+1}\)`

We choose parameters `\(\beta_0, \beta_1\)` to maximize the likelihood of the data given the model. The likelihood function is

`$$\mathcal{l}_n(\beta_0, \beta_1) = \prod_{y_i=1,i}^n p(x_i)\prod_{y_i=0,i}^n (1-p(x_i)).$$`

It is more convenient to maximize the *log-likelihood*:

`$$\text{max}_{\beta_0, \beta_1} ~ \log  l_n(\beta_0, \beta_1) = \text{max}_{\beta_0, \beta_1}  - \sum_{i = 1}^n \log\big(1 + e^{-(\beta_0+\beta_1x_i)} \big)$$`

---
## Making predictions 

With estimates from the model fit, `\(\hat{\beta_0}, \hat{\beta_1}\)`, we can predict the **probability of belonging to class 1** using:


`$$p(y=1|\hat{\beta_0} + \hat{\beta_1} x) = \frac{e^{\hat{\beta_0}+ \hat{\beta_1}x}}{1+e^{\hat{\beta_0}+ \hat{\beta_1}x}}$$`

This probability can be rounded to 0 or 1 for class prediction.

In `R`, we simply use the `predict()` function.


---
class: center

### Of course, probabilities close to 0.5 are hard to classify!

&lt;br&gt;

&lt;img src="images/response.jpg", width="60%"&gt;


.font_tiny[Source: Statistical Statistics Memes]

---
class: split-40

.column[.pad50px[

## Fitting credit data in R <i class="fas  fa-credit-card "></i>

&lt;br&gt;

 We use the `glm` function in R to fit a logistic regression model. The `glm` function can support many response types, so we specify `family="binomial"` to let R know that our response is *binary*.
]]
.column[.content.vmiddle[


```r
library(broom)
*fit &lt;- glm(default~balance,
*          data=simcredit, family="binomial")
tidy(fit)
```

```
## # A tibble: 2 x 5
##   term         estimate std.error statistic   p.value
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept) -10.7      0.361        -29.5 3.62e-191
## 2 balance       0.00550  0.000220      25.0 1.98e-137
```

]]


---

class: split-40

.column[.pad50px[

# Fitting credit data in R <i class="fas  fa-credit-card "></i>

&lt;br&gt;

We can use the `predict()` function to predict the probability of default, given credit balance. We then round these probabilities to predict default status.

]]
.column[.content.vmiddle[


```r
*probs &lt;- predict(fit, simcredit ,type="response")
head(probs, 4) 
```

```
##            1            2            3            4 
## 0.0013056797 0.0021125949 0.0085947405 0.0004344368
```

```r
head(round(probs), 4) 
```

```
## 1 2 3 4 
## 0 0 0 0
```

]]

---

class: middle center purple

<span>&lt;i class="fas  fa-exclamation-triangle fa-7x faa-flash animated "&gt;&lt;/i&gt;</span>

# .purple[A warning for using GLMs!]

---
class: split-40

.column[.pad50px[

# When logistic regression fails

&lt;br&gt;

Consider the case when the data is *perfectly* separated. 

Here, we can see that all balances above $1500 default.
]]
.column[.content.vmiddle[

&lt;img src="categorical_response_regression_files/figure-html/unnamed-chunk-8-1.png" width="504" style="display: block; margin: auto;" /&gt;

]]

---

class: split-40

.column[.pad50px[

## When logistic regression fails

&lt;br&gt;

If we fit a `glm` model to this data, the MLE fit will try and fit a step-wise function to this graph, pushing coefficients sizes towards infinity and produce large standard errors. R will warn us that the algorithm does not converge.
]]
.column[.content.vmiddle[


```r
fit &lt;- glm(default_new~balance, 
           data=simcredit, family="binomial")
tidy(fit)
```


```r
# A tibble: 2 x 5
  term        estimate std.error statistic p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
*1 (Intercept)   -41692.    57581.   -0.724   0.469
*2 balance         27.8      38.4     0.724   0.469
```
]]

---

class: center 

## Take home message - take note of R warnings

&lt;img src="images/warning.jpg", width="60%"&gt;

.font_tiny[Source: R Memes for Statistical Fiends]

---
## Linear Discriminant Analysis

Logistic regression involves directly modeling `\(P(Y = k|X = x)\)` using the logistic function. Rounding the probabilities produces class predictions, in two class problems; selecting the class with the highest probability produces class predictions in multi-class problems.

Another approach for building a classification model is .orange[linear discriminant analysis]. This involves directly estimating the .orange[distribution of the predictors], separately for each class.

---

class: center

## Compare the pair

&lt;div style="line-height:80%;"&gt;
    &lt;br&gt;
&lt;/div&gt;

| .green[Logistic Regression]     | &lt;span style="color:#3F9F7A"&gt; Linear Discriminant Analysis  &lt;/span&gt;   | 
| :-------------------: |:-------------------:| 
| **Goal** - directly estimate `\(P(Y \lvert X)\)` (*the dashed line*)     | **Goal** - estimate `\(P(X \lvert Y)\)` (*the contours*) to then deduce `\(P(Y \lvert X)\)`  | 
| **Assumptions** - no assumptions on predictor space      | **Assumptions** - predictors are normally distributed      |   
| &lt;img src="images/LR.JPG", width="60%"&gt; | &lt;img src="images/LDA.JPG", width="60%"&gt;      |   



---

class: center

## Assumptions are critical in LDA

&lt;img src="images/assumptions.png", width="70%"&gt;

.font_tiny[Source: Statistical Statistics Memes]

---

## Bayes Theorem

Let `\(f_k(x)\)` be the density function for predictor `\(x\)` for class `\(k\)`. If `\(f\)` is small, the probability that `\(x\)` belongs to class `\(k\)` is small, and conversely if `\(f\)` is large.

Bayes theorem (for `\(K\)` classes) states:

.tip[

`$$P(Y = k|X = x) = p_k(x) = \frac{\pi_kf_k(x)}{\sum_{i=1}^K \pi_kf_k(x)}$$`
]

where `\(\pi_k = P(Y = k)\)` is the prior probability that the observation comes from class `\(k\)`. 


---
## LDA with `\(p=1\)` predictors

&lt;img src="categorical_response_regression_files/figure-html/unnamed-chunk-11-1.png" width="800" style="display: block; margin: auto;" /&gt;


---
## LDA with `\(p=1\)` predictors

We assume `\(f_k(x)\)` is univariate .orange[Normal] (Gaussian):

`$$f_k(x) = \frac{1}{\sqrt{2 \pi} \sigma_k} \text{exp}~ \left( - \frac{1}{2 \sigma^2_k} (x - \mu_k)^2 \right)$$`

where `\(\mu_k\)` and `\(\sigma^2_k\)` are the mean and variance parameters for the `\(k\)`th class. Further assume that `\(\sigma_1^2 = \sigma_2^2 = \dots = \sigma_K^2\)`; then the conditional probabilities are 

`$$p_k(x) = \frac{\pi_k \frac{1}{\sqrt{2 \pi} \sigma} \text{exp}~ \left( - \frac{1}{2 \sigma^2} (x - \mu_k)^2 \right) }{ \sum_{l = 1}^K \pi_l \frac{1}{\sqrt{2 \pi} \sigma} \text{exp}~ \left( - \frac{1}{2 \sigma^2} (x - \mu_l)^2 \right) }$$`




---
## LDA with `\(p=1\)` predictors

The Bayes classifier is assign new observation `\(X=x_0\)` to the class with the highest `\(p_k(x_0)\)`. A simplification of `\(p_k(x_0)\)` yields the .orange[discriminant functions]: 

`$$\delta_k(x_0) = x_0 \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2 \sigma^2} + log(\pi_k)$$`
and the rule Bayes classifier will assign `\(x_0\)` to the class with the largest value. 


---
## LDA with `\(p=1\)` predictors

If `\(K = 2\)` and `\(\pi_1 = \pi_2\)`, we assign `\(x_0\)` to class if

`$$\delta_1(x_0) &gt; \delta_2(x_0)$$`

$$x_0 \frac{\mu_1}{\sigma^2} - \frac{\mu_1^2}{2 \sigma^2} + \log(\pi) &gt; x_0 \frac{\mu_2}{\sigma^2} - \frac{\mu_2^2}{2 \sigma^2} + \log(\pi) $$

which simplifies to  `\(x_0 &gt; \frac{\mu_1+\mu_2}{2}\)`.

.tip[
This is estimated on the data with 
`\(x_0 &gt; \frac{\bar{x}_1 + \bar{x}_2}{2}\)`.
]



---
## Multivariate LDA

To indicate that a p-dimensional random variable X has a multivariate Gaussian distribution with `\(E[X] = \mu\)` and `\(\text{Cov}(X) = \Sigma\)`, we write `\(X \sim N(\mu, \Sigma)\)`.

The multivariate normal density function is:

`$$f(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp\{-\frac{1}{2}(x-\mu)'\Sigma^{-1}(x-\mu)\}$$`

with `\(x, \mu\)` are `\(p\)`-dimensional vectors, `\(\Sigma\)` is a `\(p\times p\)` variance-covariance matrix. 

---
## Multivariate LDA

The discriminant functions are:

`$$\delta_k(x) = x'\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k'\Sigma^{-1}\mu_k + \pi_k$$`

and Bayes classifier is .orange[assign a new observation] `\(x_0\)` .orange[to the class with the highest] `\(\delta_k(x_0)\)`.

When `\(K=2\)` and `\(\pi_1=\pi_2\)` this reduces to 

Assign observation `\(x_0\)` to class 1 if 

`$$x_0'\Sigma^{-1}(\mu_1-\mu_2) &gt; \frac{1}{2}(\mu_1+\mu_2)'\Sigma^{-1}(\mu_1-\mu_2)$$`


---
## Multivariate LDA

.orange[Discriminant space]: a benefit of LDA is that it provides a low-dimensional projection of the `\(p\)`-dimensional space, where the groups are the most separated. For `\(K=2\)`, this is

`$$\Sigma^{-1}(\mu_1-\mu_2)$$`

For `\(K&gt;2\)`, the discriminant space is found be taking an eigen-decomposition of `\(\Sigma^{-1}\Sigma_B\)`, where

`$$\Sigma_B = \frac{1}{K}\sum_{i=1}^{K} (\mu_i-\mu)(\mu_i-\mu)'$$`



---
## Multivariate LDA


The dashed lines are the Bayes decision boundaries. Ellipses
that contain 95% of the probability for each of the three classes are shown. Solid line corresponds to the class boundaries from the LDA model fit to the sample.

&lt;a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter4/4.6.pdf" target="_BLANK"&gt; &lt;img src="images/4.6.png" style="width: 90%; align: center"/&gt; &lt;/a&gt;

.font_tiny[(Chapter4/4.6.pdf)]




---
## Quadratic DA (QDA)
A quadratic boundary is obtained by relaxing the assumption of equal variance-covariance, and assume that `\(\Sigma_k \neq \Sigma_l, ~~k\neq l, k,l=1,...,K\)`


&lt;a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter4/4.9.pdf" target="_BLANK"&gt; &lt;img src="images/4.9.png" style="width: 90%; align: center"/&gt; &lt;/a&gt;

.font_tiny[(Chapter4/4.9.pdf)]




---
layout: false
# 👨‍💻 Made by a human with a computer

### Slides at [https://iml.numbat.space](https://iml.numbat.space).
### Code and data at [https://github.com/numbats/iml](https://github.com/numbats/iml).
&lt;br&gt;

### Created using [R Markdown](https://rmarkdown.rstudio.com) with flair by [**xaringan**](https://github.com/yihui/xaringan), and [**kunoichi** (female ninja) style](https://github.com/emitanaka/ninja-theme).

&lt;br&gt; 
&lt;a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"&gt;&lt;img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /&gt;&lt;/a&gt;&lt;br /&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
