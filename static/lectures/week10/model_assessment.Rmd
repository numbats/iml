---
title: "ETC3250/5250: Model assessment"
subtitle: "Semester 1, 2020"
author: "<br> Professor Di Cook <br> <br> Econometrics and Business Statistics <br> Monash University"
date: "Week 10 (a)"
output: 
  xaringan::moon_reader:
    css: ["kunoichi", "ninjutsu", "mystyle.css", "libs/animate.css"]
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
header-includes:
  - \usepackage{xcolor}
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(tidy = FALSE, 
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE, 
                      out.width = "100%",
                      fig.width=8,
                      fig.height=6,
                      fig.align = "center",
                      fig.retina = 4)
options(htmltools.dir.version = FALSE)
library(magick)
```


class: middle center

```{r echo=TRUE}
library(statquotes)
search_quotes(search="Holdane", fuzzy=TRUE)
```

---

class: middle center

```{r echo=TRUE}
statquote(source="Box")
```


---
layout: true
class: shuriken-full white 

.blade1.bg-green[.content[
.white.font_large[Know your data.] `r set.seed(1);emo::ji("airplane")`<br>Quantitative or qualitative response?  Predictors all quantitative? Do you have independent observations? 
]]
.blade2.bg-purple[.content[
.white.font_large[Plot your data.] `r set.seed(1);emo::ji("painting")`<br>Is there a relationship between response and predictors?  Is the relationship linear? Are boundaries linear?Is variability heterogeneous? Are groups distinct? Are there unusual observations? 
]]
.blade3.bg-deep-orange[.content[
.white.font_large[Check for missing values.] `r set.seed(1);emo::ji("tool")`<br>Do some variables have too many missings to use them? Do some observations have too many missings to use them? What would be a useful imputation method to fix the sporadic missing value? 
]]
.blade4.bg-pink[.content[
.white.font_large[Fit a versatile model.] `r set.seed(1);emo::ji("computer")` <br>Compute and plot model diagnostics. Where doesn't the model do well? How can it be refined? 
]]

---

class: hide-blade2 hide-blade3 hide-blade4 hide-hole

---

class: hide-blade3 hide-blade4 hide-hole
count: false

---

class: hide-blade4 hide-hole
count: false

---

class: hide-hole
count: false

---

count: false 

---


layout: false

## ROC for classification

The .orange[ROC curve] is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds. It is a common method for comparing classification models. Below: ROC curve for the LDA classifier on the training set of `credit` data. 


<center>
<a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter4/4.8.pdf" target="_BLANK"> <img src="images/4.8.png" style="width: 40%; align: center"/> </a>
</center>

.font_tiny[Fig 4.8]
---

class: split-50
layout: false

.column[.pad10px[

<a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter4/4.8.pdf" target="_BLANK"> <img src="images/4.8.png" style="width: 90%; align: center"/> </a>


]]
.column[.top50px[


The true positive rate is the .orange[sensitivity]: the fraction of defaulters that are correctly identified, using a given threshold value. 

The false positive rate is .orange[1-specificity]: the fraction of non-defaulters that we classify incorrectly as defaulters, using that same threshold value.

The dotted line is "no information" classifier; class and predictor are not associated.

The .orange[ideal ROC curve hugs the top left corner], indicating a high true positive rate and a low false positive rate.

]]



---
class: split-50
layout: false

.column[.pad10px[

<a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter4/4.8.pdf" target="_BLANK"> <img src="images/4.8.png" style="width: 100%; align: center"/> </a>

]]
.column[.pad10px[

<br>

<br>

If the classifier returns a prediction between 0 and 1, interpret as the probability of a positive, then threshold (split the data) at different values, e.g. 0.1, 0.2, 0.3, 0.4, 0.5, ... 

Compute the confusion table for each split, record the sensitivity and specificity and plot the resulting numbers. 

]]
---
<center>
<table>
<tr>  <td> </td><td> </td> <td colspan="2" align="center" > true </td> </tr>
<tr>  <td> </td><td> </td> <td align="center" bgcolor="#daf2e9" width="80px"> C1 (positive) </td> <td align="center" bgcolor="#daf2e9" width="80px"> C2 (negative) </td> </tr>
<tr height="50px">  <td> pred- </td><td bgcolor="#daf2e9"> C1 </td> <td align="center" bgcolor="#D3D3D3"> <em>a</em> </td> <td align="center" bgcolor="#D3D3D3"> <em>b</em> </td> </tr>
<tr height="50px">  <td>icted </td><td bgcolor="#daf2e9"> C2</td> <td align="center" bgcolor="#D3D3D3"> <em>c</em> </td> <td align="center" bgcolor="#D3D3D3"> <em>d</em> </td> </tr>
</table>
</center>

- Sensitivity: *a/(a+c)*  (true positive, recall)
- Specificity: *d/(b+d)* (true negative)

---
class: split-50

.column[
```{r echo=TRUE}
library(tidyverse)
library(yardstick)
glimpse(two_class_example) #<<
```

Set threshold to 0.5

```{r}
two_class_example %>%
  mutate(my_predicted = ifelse(Class1 > 0.5, "Class1", "Class2")) %>%
  mutate(my_predicted = factor(my_predicted)) %>%
  conf_mat(my_predicted, truth)  
```

sensitivity = 0.82, 1-specificity = 0.14
]

.column[
```{r}
roc_curve(two_class_example, truth, Class1) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw()
```

Your turn: Set the threshold to be 0.75, re-compute the confusion matrix, and sensitivity, specificity.
]

---

Really nice explanation by Parul Pandey [here](https://towardsdatascience.com/understanding-the-roc-and-auc-curves-a05b68550b69): 

<img src="images/ROC.png" width="80%">

---

## ROC for classification

<a href="http://www-bcf.usc.edu/~gareth/ISL/Chapter9/9.10.pdf" target="_BLANK"> <img src="images/9.10.png" style="width: 80%; align: center"/> </a>

(left) LDA and SVM similar.       
(right) SVM radial basis with $\gamma=10^{-1}$ is the best.

.font_tiny[Fig 9.10]


---

class: split-30
layout: false

## Multivariate outliers

.orange[Mahalanobis distance] measures the distance from the mean, relative to the variance-covariance matrix, and is useful for .orange[outlier detection]: $D^2 = (X-\mu)'\Sigma^{-1}(X-\mu)$

```{r fig.height=3, out.width="70%", results='hide'}
library(tidyverse)
library(ISLR)
library(broom)
library(mvtnorm)
library(gridExtra)
set.seed(20190505)
x <- rmvnorm(500, mean=rep(0,2), sigma = matrix(c(1,-0.8, -0.8, 1), ncol=2, byrow=TRUE))
df <- as_tibble(x) %>% bind_rows(tibble(V1=2, V2=2))
p1 <- ggplot(df, aes(x=V1, y=V2)) + geom_point() +
  annotate("text", 2.3, 2.2, label="outlier", colour="orange") +
  annotate("point", 2, 2, colour="orange") +
  coord_equal()
df <- df %>% mutate(mah=mahalanobis(df[,1:2], center=c(0,0),
                                    cov=cov(df[,1:2])))
p2 <- ggplot(df, aes(x=mah)) + geom_histogram() + 
  xlab("Mahalanobis distance") +
  annotate("text", 33, 10, label="outlier", colour="orange") 
grid.arrange(p1, p2, ncol=2)
```

Related to "leverage" in regression diagnostics.



---

## Influential observations


.orange[Cook's distance] measures the change in the model estimates due to the observation: $D_i = \frac{e_i^2}{MSE\times p}\frac{h_i}{(1-h_i)^2}$ where $h_i$ is the leverage of observation $i$. 

```{r fig.height=3, out.width="80%", results='hide'}
set.seed(20190505)
x <- rmvnorm(500, mean=rep(0,2), sigma = matrix(c(1,-0.8, -0.8, 1), ncol=2, byrow=TRUE))
df <- as_tibble(x) %>% bind_rows(tibble(V1=5, V2=2))
p1 <- ggplot(df, aes(x=V1, y=V2)) + geom_point() +
  annotate("text", 4.5, 2.2, label="influential", colour="orange") +
  annotate("point", 5, 2, colour="orange") 
fit <- lm(V2~V1, data=df)
df <- augment(fit, df)
p2 <- ggplot(df, aes(x=.cooksd)) + geom_histogram() + 
  xlab("Cooks distance") +
  annotate("text", 1.7, 15, label="influential", colour="orange") 
grid.arrange(p1, p2, ncol=2)
```

.font_tiny[Developed by Dennis Cook, University of Minnesota. ]


---

## Utilising bagging

Remember the vote matrix available from random forests:

$$V = (V_1  V_2 ... V_K) \\
  = \left[ \begin{eqnarray*}
                          p_{11} & p_{12} & ... & p_{1K}\\
                          p_{21} & p_{22} & ... & p_{2K}\\
                          ...   &  ...    &     & ...\\
                          p_{n1} & p_{n2} & ... & p_{nK}
                          \end{eqnarray*} \right]$$

With bagging, multiple out of bag predictions produces uncertainty measure for each observation. It's possible that observations with .orange[higher uncertainty are outliers].

---

## Variable importance

<br>

- Working with .orange[standardised variables] helps, because magnitude of coefficients is then directly interpreted as importance
- .orange[Permutation] approach in random forests is useful more broadly. Compare magnitude of coefficients between models built on original and permuted variable.
- .orange[Effect of one predictor with the response] can depend on their relationship with one another. Called multicollinearity in regression. 


---
layout: false
# Bigger picture

.orange[All possible model fits] to housing data with 7 variables, from [Wickham et al (2015) Removing the Blindfold](http://onlinelibrary.wiley.com/doi/10.1002/sam.11271/abstract)

<img src="images/houses1.png" style="width:70%" />
---
class: split-60
layout: false

.column[.pad10px[
<img src="images/houses2.png" style="width:110%" />
]]
.column[.pad10px[
.font_small[Three typical estimates for bedrooms: big positive, close to 0, big negative.] 

.font_small[Models with big .orange[positive coefficients] for bedrooms tend to have .orange[weaker fits]. They tend to occur with models that have no livingArea contribution, and more negative coefficients for zoneRM, and no air con.]

.font_small[Models with big .orange[negative coefficients] on bedrooms tend to have .orange[stronger fits]. All contrast with livingArea (high positive coefficients).]

.font_small[If bedrooms contribute to the model, bathrooms do not.]

]]

---

## Model choice - robustness of conclusions

Whatever way you model the data, the .orange[interpretations should be consistent]. 

- Bias can explain difference in predictions between models, flexible vs inflexible can provide a spectrum on what the data predicts.
- Broad changes in a model when some cases or some variables are not used, should evoke suspicions (your "spidey sense"). 
- Model fit statistics are a measure of predictive power. A weak model can still be useful if there is a large cost involved.  


---

layout: false
# `r set.seed(2020); emo::ji("technologist")` Made by a human with a computer

### Slides at [https://iml.numbat.space](https://iml.numbat.space).
### Code and data at [https://github.com/numbats/iml](https://github.com/numbats/iml).
<br>

### Created using [R Markdown](https://rmarkdown.rstudio.com) with flair by [**xaringan**](https://github.com/yihui/xaringan), and [**kunoichi** (female ninja) style](https://github.com/emitanaka/ninja-theme).

<br> 
<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
