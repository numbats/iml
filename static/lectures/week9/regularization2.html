<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>ETC3250/5250: Regularization</title>
    <meta charset="utf-8" />
    <meta name="author" content="  Professor Di Cook     Econometrics and Business Statistics   Monash University" />
    <link href="libs/remark-css/kunoichi.css" rel="stylesheet" />
    <link href="libs/remark-css/ninjutsu.css" rel="stylesheet" />
    <link href="libs/countdown/countdown.css" rel="stylesheet" />
    <script src="libs/countdown/countdown.js"></script>
    <link href="libs/font-awesome-animation/font-awesome-animation-emi.css" rel="stylesheet" />
    <script src="libs/fontawesome/js/fontawesome-all.min.js"></script>
    <link rel="stylesheet" href="mystyle.css" type="text/css" />
    <link rel="stylesheet" href="libs/animate.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# ETC3250/5250: Regularization
## Semester 1, 2020
### <br> Professor Di Cook <br> <br> Econometrics and Business Statistics <br> Monash University
### Week 9 (b)

---





## Penalised LDA

Recall: LDA involves the eigen-decomposition of `\(\color{orange}{\Sigma^{-1}\Sigma_B}\)`, where

`$$\small{\Sigma_B = \frac{1}{K}\sum_{i=1}^{K} (\mu_i-\mu)(\mu_i-\mu)'}$$`

`$$\small{\Sigma = \frac{1}{n} \sum_{i=1}^{n} (x_i-\mu_i)(x_i-\mu_i)'}$$`

The eigen-decomposition is an analytical solution to a sequential optimisation problem: 


`\begin{align*}
&amp; \small{\underset{{\beta_k}}{\text{maximize}} \beta_k^T\hat{\Sigma}_B \beta_k} \\
&amp; \small{\mbox{ subject to  }  \beta_k^T\hat{\Sigma} \beta_k \leq 1, \beta_k^T\hat{\Sigma}\beta_j = 0 \mbox{  } \forall i&lt;k}
\end{align*}`


---
## Penalised LDA

The problem is inverting `\(\color{orange}{\Sigma^{-1}}\)`, fix it by

`\begin{align*}
&amp; \underset{{\beta_k}}{\text{maximize}} \left(\beta_k^T\hat{\Sigma}_B \beta_k + \lambda_k \sum_{j=1}^p |\hat{\sigma}_j\beta_{kj}|\right)\\
&amp; \mbox{ subject to  }  \beta_k^T\tilde{\Sigma} \beta_k \leq 1
\end{align*}`

where `\(\hat{\sigma}_j\)` is the within-class standard deviation for variable `\(j\)`. This is 
.orange[penalised LDA], and see [reference](https://faculty.washington.edu/dwitten/Papers/JRSSBPenLDA.pdf), and the [R package](https://cran.r-project.org/web/packages/penalizedLDA/index.html). 





---
## PDA Index

&lt;br&gt;

Penalised LDA projection pursuit index. Available in the `tourr` package. 

`\begin{align*}
I_{PDA}(A,\lambda) =
1-\frac{\Big|A'\big\{(1-\lambda)\hat{\Sigma}+n\lambda I_p\big\}A\Big|} {\Big|A'\big\{(1-\lambda)(\hat{\Sigma}_B +\hat{\Sigma})+n\lambda I_p\big\} A\Big|}
\end{align*}`

Optimising this function over `\(p\times d\)` projection matrix `\(A\)`. 




---

class: split-two

.column[.pad50px[

## Diagonal Discriminant Analysis

&lt;br&gt;
- The simplest form of regularisation assumes that the features are independent within each class. 
- Consider a *diagonal-covariance* LDA rule for classifying classes
- A special case of the naive-Bayes classifier

]]
.column[.content.vmiddle.center[

&lt;img src="regularization2_files/figure-html/unnamed-chunk-3-1.png" width="576" style="display: block; margin: auto;" /&gt;

]]


---
## Discriminant Function

It can be shown that the discriminant score for a new observation `\(\mathbf{x}^*\)` when the features are considered independent reduces to the following:

`$$\delta_k(\mathbf{x}^*) = - \sum_{j=1}^{p}\frac{(x_j^* - \bar{x}_{kj})^2}{s^2_j} + 2\log \pi_k.$$`

The classification rule is then

`$$C(\mathbf{x}^*) = \ell \quad \mbox{if} \quad \delta_{\ell}(\mathbf{x}^*) = \max_k \delta_k(\mathbf{x}^*).$$`


---

## Filter features for prediction

To motivate the upcoming method, consider a binary classification DLDA problem. 

One way we could establish which of the features are driving prediction would be to perform a two-sample `\(t\)`-test 

`$$t_{j} = \frac{\bar{x}_{1j} - \bar{x}_{0j}}{s_j}$$`
with the `\(t\)` statistic providing a measure of how significant the difference in class means for predictor `\(j\)`. 



---

## Filter features for prediction

.green[Think about it:] Using the `\(t\)` statistic -  `\(t_{j} = \frac{\bar{x}_{1j} - \bar{x}_{0j}}{s_j}\)` for all features, what is one way we can determine important features for prediction?


<div class="countdown" id="timer_5ec5c148" style="bottom:0;left:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">00</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">32</span></code>
</div>
---

## Filter features for prediction

.orange[Answer:] Can consider filtering for features with `\(\lvert t_j \lvert &gt; 2\)`, as this is deemed significant at the 5% level.

&lt;img src="regularization2_files/figure-html/unnamed-chunk-5-1.png" width="576" style="display: block; margin: auto;" /&gt;

.font_tiny[Note - further consideration can be given to the issue of [*Multiple Testing*](https://en.wikipedia.org/wiki/Multiple_comparisons_problem)]

---

## Nearest Shrunken Centroids (NSC)

Now consider the following statistic,

&lt;br&gt;

`$$d_{kj} = \frac{\bar{x}_{kj} - \bar{x}_j}{m_k(s_j + s_0)} \quad \mbox{with} \quad m_k^2 = \frac{1}{N_k} - \frac{1}{N}$$`
and `\(s_0\)` a small value to protect `\(d_{kj}\)` from small expression values.

&lt;br&gt;
This statistic is a  measure for how significant the difference between the class `\(k\)` mean for predictor `\(j\)`, and the overall mean for predictor `\(j\)`.

---

##  Soft Thresholding

Each `\(d_{kj}\)` is reduced by an amount `\(\Delta\)` in absolute value, and is set to zero if its absolute value is less than zero.

`$$d'_{kj} = \mbox{sign}(d_{kj})( \lvert d_{kj} \lvert - \Delta)_{+},$$`
&lt;center&gt;
 &lt;img src="images/soft.JPG", width="40%"&gt;
&lt;/center&gt;
---
## Nearest Shrunken Centroids Classifier

The NSC uses either version of the statistic `\(d'_{kj}\)` to regularise by shrinking the class means towards the overall mean for each predictor separately as follows:

`$$\bar{x}'_{kj} = \bar{x}_j + m_k(s_j + s_0)d'_{kj}$$`

&lt;br&gt;

.green.center[Unless a predictor has a significant difference to the overall mean for at least one class, it is useless for classification.]

We then use the shrunken centroids `\(\bar{x}'_{kj}\)` in place of `\(\bar{x}_{kj}\)` in the DLDA discriminant function.

---

## Alternative - penalised multiple hypothesis testing (multiDA)

&lt;br&gt;

Another approach to high dimensional DA involves formulating the problem as a multiple hypothesis testing problem, and asking the question - .orange["What defines a discriminative feature?"], and then choosing discriminative features through a penalised likelihood ratio test.






---
class: split-50

.column[.pad10px[
## LRT ‚Äì compare to the null

.green[For `\\(K=3\\)` classes, there are `\\(m=5\\)` potential partitions of the data.]

&lt;img src="images/bell_number.jpg" width="80%"&gt;


]]
.column[.pad10px[
For all 5 hypotheses, compare the likelihood to the null. Pick the "partition" that is the most likely.

&lt;center&gt;

&lt;img src="images/LRT.png" width="80%"&gt;

&lt;/center&gt;
]]

---

## A penalised likelihood ratio test statistic

Two forms of penalisation can be considered:

- .green[The BIC] - useful when Positive Selection Rate is preferred to controlling False Discovery Rate (FDR). 
`$$\nu_m \log(n)$$`
- .green[The Extended BIC] - useful for high dimensional data, penalising additionally on the number of features `\(p\)`.
`$$\nu_m[\log(n) + 2\log(p)]$$`

.font_small[(Note - `\\(\nu_m = g_m - 1\\)` where `\\(g_m\\)` is the number of groupings considered in model `\\(m\\)`). ]

---

class: split-two

.column[.pad50px[
## Recall -  SRBCT cancer prediction 

- The SRBCT dataset (Khan et al., 2001) looks at classifying 4 classes of different childhood tumours sharing similar visual features during routine histology.
- Data contains 83 microarray samples with 1586 features.
- .orange[We will revisit this data later on in the course to explore high dimensional DA.] <span class=" faa-flash animated " style=" display: -moz-inline-stack; display: inline-block; transform: rotate(0deg);">Now is that time</span>


]]

.column[.content.vmiddle.center[

 &lt;img src="images/SRBCT-nature.jpg", width="70%"&gt;

.purple[Source:] [Nature](https://www.nature.com/articles/modpathol2016119)

]]

---
class: split-60

.column[.pad50px[

## multiDA in R
 
&lt;br&gt;


```r
#remotes::install_github("sarahromanes/multiDA")
library(multiDA)
res &lt;- multiDA(y = SRBCT$y, 
               X = SRBCT$X,
               penalty = "EBIC",
               equal.var = TRUE,
               set.options = "exhaustive")
```

We can then examine the class groupings using the `plot()` method for `multiDA`:


```r
plot(res, ranks= 1)
```


 ]]

.column[.content.vmiddle.center[

&lt;img src="regularization2_files/figure-html/unnamed-chunk-8-1.png" width="576" style="display: block; margin: auto;" /&gt;



]]




---

## Compare performance - 100 trial, 5 fold CV

&lt;img src="regularization2_files/figure-html/unnamed-chunk-9-1.png" width="864" style="display: block; margin: auto;" /&gt;

---

layout: false
# üë©‚Äçüíª Made by a human with a computer

### Slides at [https://iml.numbat.space](https://iml.numbat.space).
### Code and data at [https://github.com/numbats/iml](https://github.com/numbats/iml).
&lt;br&gt;

### Created using [R Markdown](https://rmarkdown.rstudio.com) with flair by [**xaringan**](https://github.com/yihui/xaringan), and [**kunoichi** (female ninja) style](https://github.com/emitanaka/ninja-theme).

&lt;br&gt; 
&lt;a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"&gt;&lt;img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /&gt;&lt;/a&gt;&lt;br /&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
