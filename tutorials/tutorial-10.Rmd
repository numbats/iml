---
title: "ETC3250/5250 Tutorial 10 Instructions"
subtitle: "Model choice, and regularisation"
author: "prepared by Professor Di Cook"
date: "Week 10"
output:
  html_document:
    after_body: tutorial-footer.html
    css: tutorial.css
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "#",
  fig.height = 4,
  fig.width = 8,
  fig.align = "center",
  cache = FALSE
)
library(emo)
```


```{r, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "#",
  fig.height = 4,
  fig.width = 8,
  fig.align = "center",
  cache = FALSE
)
```

## `r emo::ji("target")` Objective

The objectives of this tutorial are to

- conduct variable selection and examine importance
- practice methods for choosing models
- fit a regularised model

## `r emo::ji("wrench")` Preparation 

Make sure you have these packages installed:

```
install.packages(c("tidyverse", "ISLR", "leaps", "patchwork", "rsample", "tidymodels", "glmnet"))
```

### 1. Choosing variables

Work your way through the textbook lab 6.5.1 Best Subset Selection, and the forward stepwise procedure in 6.5.2, then answer these questions.


a. The `regsubsets()` function (part of the `leaps` library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. By default it only examines up to 8 variable models. Which variables make the best 8 variable model?


b. Set the max number of variables to be 19, by adding the argument `nvmax=19`. Plot the model fit diagnostics for the best model of each size. What would these diagnostics suggest about an appropriate choice of models? Do your results compare with the text book results? Why not?



c. Fit forward stepwise selection. How would the decision about best model change?  

### 2. Training and testing sets with variable selection

a. Break the data into a 2/3 training and 1/3 test set. 
b. Fit the best subsets. Compute the mean square error for the test set. Which model would it suggest? Is the subset of models similar to produced on the full data set? Do your results compare with the text book results? Why not?




### 3. Cross-validation with variable selection

It is said that 10-fold cross-validation is a reasonable choice for dividing the data. What size data sets would this create for this data? Argue whether this is good or bad. 

### 4. Regularisation for variable selection

Here we will use lasso to fit a regularised regression model and compare the results with the best subset model.

a. Using your results from questions 1-3, fit the best least squares model, to your training set. Write down the mean square error and estimates for the final model. We'll use these to compare with the lasso fit.


b. Fit the lasso to a range of $\lambda$ values. Plot the standardised coefficients against $\lambda$. What does this suggest about the predictors?




c. Now use cross-validation to choose the best $\lambda$.


d. Fit the final model using the best $\lambda$. What are the estimated coefficients? What predictors contribute to the model?



e. Does the best lasso model beat the best least squares model (best subsets)? 


### 5. Making sense of it all

Only one variable is very important for the model, which is it? (It occurs in every list of variables returned as important, and has the highest coefficient in the lasso model.) Several more variables frequently appear as important, which ones are these? Several others, appear occasionally in some models but always have very small coefficients. Can you name one of these? What does this tell you about the relationship between Salary and all the predictors?

##### Â© Copyright 2021 Monash University
