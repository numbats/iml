---
title: "ETC3250/5250 Tutorial 8 Instructions"
subtitle: "Forests and hyperplanes"
author: "prepared by Professor Di Cook"
date: "Week 8"
output:
  html_document:
    after_body: tutorial-footer.html
    css: tutorial.css
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  eval = FALSE,
  collapse = TRUE,
  comment = "#",
  fig.height = 4,
  fig.width = 8,
  fig.align = "center",
  cache = FALSE
)
library(emo)
```


## `r emo::ji("target")` Objective

The objectives for this week are to 

- Learn about fitting a forest model and use the diagnostics 
- Compare the classification tree with a forest
- Understand the the idea of bagging to refine model fitting
- Learn about fitting a support vector machine model 
- Examine the differences between results from different models

## `r emo::ji("wrench")` Preparation 

Make sure you have these packages installed:

```
install.packages(c("tidyverse", "tidymodels", "tourr", "rpart.plot", "discrim"))
```

### `r emo::ji("book")` Reading 

- Textbook sections 8.2, 9.2-3

### Group discussion 

Estimating the maximal margin hyperplane corresponds to maximising (believe me, or work your way through [Sidharth's page](https://medium.com/analytics-vidhya/math-behind-support-vector-machines-642421e45b08))

$$\sum_i{\alpha_i} - \frac12 \sum_i\sum_k\alpha_i\alpha_ky_iy_k\mathbf{x}_i^T\mathbf{x}_k$$
where $\mathbf{x}_i, \mathbf{x}_k$ are two $p$-dimensional data vectors (observations), and the coefficients of the separating hyperplane

$$\beta_0 + \beta_1 x_1 + \dots + \beta_p x_p = 0$$

are computed from the support vectors and weights $\alpha$'s from the optimisation as follows:

$$\mathbf{\beta}_j=\sum_{i=1}^s (\alpha_iy_i)\mathbf{x}_{ij}$$

Now, $i, k = 1, ..., n$ but because only some observations are used to compute $\beta$ most are 0, and we can sum only over $1, ..., s$, where $s$ is the number of support vectors. 

With the kernel trick, 

$$\sum_i{\alpha_i} - \frac12\sum_i\sum_k\alpha_i\alpha_ky_iy_kK(\mathbf{x}_i^T\mathbf{x}_k)$$

Try one kernel function transformation to show that $K(\mathbf{x}_i^T\mathbf{x}_k) = \psi(\mathbf{x}_i)^T\psi(\mathbf{x}_k)$. You can think of $\psi()$ as transformations of the predictors, $\mathbf{x}$.

Fill in the steps to go from the first line to the last. Note that $p=2$. (You can find all the steps in the lecture notes.)

$$
\begin{align*}
\mathcal{K}(\mathbf{x_i}, \mathbf{x_k}) & =  (1 + \langle \mathbf{x_i}, \mathbf{x_k}\rangle) ^2 \\
                                    & = ??? \\
                                    & =  ???\\
                                    & =  ???\\
                                    & =  (1, x_{i1}^2, x_{i2}^2, \sqrt2x_{i1}, \sqrt2x_{i2}, \sqrt2x_{i1}x_{i2})^T(???) \\
                                    & = \langle \psi(\mathbf{x_i}), \psi(\mathbf{x_k}) \rangle
\end{align*}
$$

Have a chat about why this algebraic "trick" is neat.

## `r emo::ji("waving_hand")` Getting started

If you are in a zoom tutorial, say hello in the chat. If in person, do say hello to your tutor and to your neighbours. 

## `r emo::ji("gear")` Exercises 

### 1. Effect of different variables

a. Fit the tree to olive oils, using a training split of 2/3, using only regions 2, 3,  and  the predictors linoleic and arachidic. Report the balanced accuracy of the test set, and make a plot of the boundary.

b. Fit a random forest to the full data, using only linoleic and arachidic as predictors, report the  balanced accuracy for the test set, and make a plot of the boundary.

c. Explain the difference between the single tree and random forest boundaries.

d. Fit the random forest again to the full set of variables, and  compute the variable importance. Describe the order of importance of variables.

e. Create a new variable called `linoarch` that is $0.377 \times linoleic + 0.926\times arachidic$. Make a plot of this variable against arachidic. Fit the tree model to the same training data using this variable in addition to linoleic and arachidic. Why doesn't the tree use this new variable? It has a bigger difference between the two groups than linoleic? Change the order of the variables, so that linoarch is before linoleic and re-fit the tree. Does it use this variable now? Why do you think this is?

f. Fit the random forest again to the full set of variables, including linoarch and  compute the variable importance. Describe the order of importance of variables. Does the forest see the new variable?


### 2. Support vector machine model fitting

a. Fit the linear SVM to olive oils, using a training split of 2/3, using only regions 2, 3,  and  the predictors linoleic and arachidic. Report the training and test error, list the support vectors, the coefficients for the support vectors and the equation for the separating hyperplane, and
$$???\times\text{linoleic}+???\times\text{arachidic}+??? > 0$$ and make a plot of the boundary.

b. Fit a radial kernel SVM, with a variety of cost values, to examine the effect on the boundary. (Try changing the cost from 0.3 up to 10.)

##### Â© Copyright 2021 Monash University
