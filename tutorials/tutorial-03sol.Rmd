---
title: "ETC3250/5250 Tutorial 3 Solution"
subtitle: "Fitting nonlinear regression models"
author: "prepared by Professor Di Cook"
date: "Week 3"
output:
  html_document:
    after_body: tutorial-footer.html
    css: tutorial.css
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "#",
  fig.height = 4,
  fig.width = 8,
  fig.align = "center",
  cache = FALSE
)
library(emo)
```


```{r, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "#",
  fig.height = 4,
  fig.width = 8,
  fig.align = "center",
  cache = FALSE
)
```


## ðŸ’¬ Class discussion exercises

Why do you think polynomials and splines considered to be part of recipes rather than the model fitting?

*These are both transformations of the predictors, and are best considered to be feature engineering.* 

## `r emo::ji("gear")` Exercise 

```{r load_libraries, echo=FALSE}
library(tidyverse)
library(ISLR)
library(recipes)
library(parsnip)
library(yardstick)
library(rsample)
library(GGally)
library(mgcv)
library(patchwork)
```

### 1. Explore the polynomial model fitting 

Following the textbook lab exercise for Chapter 7. Read through the textbook lab instructions for the first half of section 7.8.1 pages 288-289, *BUT* use the code below. 

a. This builds from the polynomial model fit for the Wage data, using variables wage and age, in Figure 7.1. 

The function `poly` is a convenient way to generate a fourth-degree polynomial. By default it uses "orthonormal polynomials". 

```{r fig.width=3, fig.height=12, out.width="50%"}
# Old style way to fit because recipes doesn't seem to all to choose orthogonal polynomials
fit1 <- lm(wage ~ poly(age, 4), data=Wage)
tidy(fit1)
```

b. We can request that "raw" polynomials are generated instead, with the `raw=TRUE` argument. 

```{r}
# Old style way to fit because recipes doesn't seem to all to choose orthogonal polynomials
fit2 <- lm(wage ~ poly(age, 4, raw=TRUE), data=Wage)
tidy(fit2)
```

c. The coefficients are different, but effectively the fit is the same, which can be seen by plotting the fitted values from the two models.

```{r}
wage_fit <- Wage %>% 
  mutate(yhat1 = predict(fit1, Wage), 
         yhat2 = predict(fit2, Wage))
ggplot(wage_fit, aes(x=yhat1, y=yhat2)) + 
  geom_point() + theme(aspect.ratio = 1)
```

d. To examine the differences between orthonormal polynomials and "raw" polynomials, we can make scatterplot matrices of the two sets of polynomials. 

```{r}
p_orth <- as_tibble(poly(Wage$age, 4))
ggscatmat(p_orth)
p_raw <- as_tibble(poly(Wage$age, 4, raw=TRUE))
ggscatmat(p_raw)
```

e. **Please discuss:** What is the benefit of using orthonomal polynomials?

*As higher order raw polynomials are added multicollinearity is introduced. The orthogonal polynomials add perturbations to the function preventing linear dependency between terms.*

### 2. Lurking variables

This question is based on "Why not to do what the textbook suggests!"

a. The wages data poses an interesting analytical dilemma. There appear to be two wage clusters, one small group with relatively consistent high wage, and the other big group with lower and varied wages. Make a histogram or a density plot of wage to check this more carefully. 

```{r}
ggplot(Wage, aes(x=wage)) + geom_density()
```

b. What do you think might have caused this? Check the relationship between wage, and other variables such as `jobclass`. Do any of the other variables provided in the data explain the clustering?

```{r}
a1 <- ggplot(Wage, aes(x=age, y=logwage, colour=jobclass)) + 
  geom_point(alpha=0.5) +
  scale_colour_brewer("", palette="Dark2") +
  theme(legend.position = "none")
a2 <- ggplot(Wage, aes(x=age, y=logwage, colour=health)) + 
  geom_point(alpha=0.5) +
  scale_colour_brewer("", palette="Dark2") +
  theme(legend.position = "none")
a3 <- ggplot(Wage, aes(x=age, y=logwage, colour=health_ins)) + 
  geom_point(alpha=0.5) +
  scale_colour_brewer("", palette="Dark2") +
  theme(legend.position = "none")
a4 <- ggplot(Wage, aes(x=age, y=logwage, colour=race)) +
  geom_point(alpha=0.5) +
  scale_colour_brewer("", palette="Dark2") +
  theme(legend.position = "none")
a5 <- ggplot(Wage, aes(x=age, y=logwage, colour=maritl)) + 
  geom_point(alpha=0.5) +
  scale_colour_brewer("", palette="Dark2") +
  theme(legend.position = "none")
a1+a2+a3+a4+a5
```

*No other variable in this collected data explains the cluster of salaries higher than $250. This is very likely, though, to be something simple like a "manager" position, and would be best called a "lurking variable".*

c. The textbook includes an analysis where a separate probabilistic model (a logistic model, which we will learn about this week) where a binary response ($\leq 250$, $>250$) is used. Why doesn't this make sense? An alternative approach is to treat this as a "lurking variable", let's call it "manager", create a new predictor and include this in the model. Alternatively, we could treat the high group as outliers, and exclude them from the analysis. The argument for these values are likely due to some unobserved factor, and their presence affects the ability to accurately model the rest of the data. Try to fit the model (using polynomials) with either approach, and then explain the relationship (on average) between wage and age. Do you need to use a degree=4 polynomial, or would degree=3 be sufficient?

```{r eval=FALSE, echo=FALSE}
# Old style way to fit
fit_low <- lm(wage ~ poly(age, 3), data=filter(Wage, wage < 250))
tidy(fit_low)
glance(fit_low)
wage_fit <- augment(fit_low, filter(Wage, wage < 250))
ggplot(wage_fit) + 
  geom_point(aes(x=age, y=wage)) +
  geom_line(aes(x=age, y=.fitted), 
            colour="blue", size=2)
ggplot(wage_fit) + 
  geom_point(aes(x=age, y=.resid))
```

*A degree=3 polynomial is sufficient for this data, because there is no difference in model fit statistics. Use the simpler model.*

### 3. Fitting NRC rankings

In 2010, the National Research Council released rankings for all doctorate programs in the USA (https://en.wikipedia.org/wiki/United_States_National_Research_Council_rankings). The data was initially released and then only available for a fee. I managed to get a copy during the free period, and this is the data that we will use for this exercise. There hasn't been another set of rankings publicly released since then, and I don't know why. Only the rankings and statistics for Statistics programs are included in this data set.

Your job is to answer the question: "How is R Ranking related to rankings on research, student support and diversity?" using the 5th percentile for each of these quantities. Fit your best model, try using splines, and justify your choices.

a. Read the data. Rename the columns containing the variables of interest to `rank`, `research`, `student` and `diversity`). Using recipes, split the data into 2/3 training and 1/3 test sets.
b. Make response vs predictor plots and predictor vs predictor plots of the training data. Explain the relationships, outliers, and how this would affect the modeling, and what you would expect the results of the modeling to be. 

```{r fig.width=10}
# Read data
nrc <- read_csv(here::here("data/nrc.csv")) %>%
  mutate(rank = R.Rankings.5th.Percentile,
         research = Research.Activity.5th.Percentile,
         student = Student.Support.Outcomes.5th.Percentile,
         diversity = Diversity.5th.Percentile) %>%
  select(rank, research, student, diversity) 

# Split the data into training and test
set.seed(21)
train_test_split <- initial_split(nrc, prop = 2/3)

nrc_train <- training(train_test_split)
nrc_test <- testing(train_test_split)

# Original data with loess smooth 
ggduo(nrc_train, columnsX = c(2,3,4), columnsY = 1)
```

*Rank has a strong relationship with research but very weak with the other two. There is one outlier with good rank but not so good research rank.*

c. Fit a linear model. Report estimates, model fit statistics, and the test MSE. Plot the fitted model against each predictor.


```{r fig.width=8}
# Fit a linear model to the original data
lm_mod <- 
  linear_reg() %>% 
  set_engine("lm")

lm_fit <- 
  lm_mod %>% 
  fit(rank ~ ., data = nrc_train)

   # Summarise model and check the fit
tidy(lm_fit)
glance(lm_fit)

nrc_lm_train_pred <- augment(lm_fit, nrc_train)
nrc_lm_test_pred <- augment(lm_fit, nrc_test)
metrics(nrc_lm_test_pred, truth = rank, 
        estimate = .pred)

p_f <- ggplot(nrc_lm_train_pred) +
  geom_point(aes(x = .pred, y = rank)) 
p_e <- ggplot(nrc_lm_train_pred) +
  geom_point(aes(x = .pred, y = .resid)) 
p_f + p_e
```

```{r fig.width=12}
  # Plot the fitted model against each predictor
p1 <- ggplot(nrc_lm_train_pred) +
  geom_point(aes(x = research, y = rank)) +
  geom_point(aes(x = research, y = .pred),
             colour="blue")
p2 <- ggplot(nrc_lm_train_pred) +
  geom_point(aes(x = student, y = rank)) +
  geom_point(aes(x = student, y = .pred),
             colour="blue")
p3 <- ggplot(nrc_lm_train_pred) +
  geom_point(aes(x = diversity, y = rank)) +
  geom_point(aes(x = diversity, y = .pred),
             colour="blue")
p1 + p2 + p3
```

*Training R^2 is 0.584. The model explains about 60% of the variation in rank. Only research is statistically significant for the model. Test MSE is 11.8.*

d. Fit a GAM model. Report estimates, model fit statistics, and the test MSE. Plot the fitted model against each predictor.

```{r fig.width=4}
nrc_prep <-    
  recipe(rank ~ research + student + diversity, 
         data = nrc_train) %>% 
  step_ns(research, student, diversity, deg_free = 3) %>%
  prep()
# if the tune package is installed try tune() instead of 3
nrc_train_baked <- bake(nrc_prep, new_data = NULL)
nrc_test_baked <- bake(nrc_prep, new_data = nrc_test)

# Plot transformed data
nrc_train_all <- nrc_train %>%
  select(research, student, diversity) %>%
  bind_cols(nrc_train_baked)
ggduo(nrc_train_all, 
            columnsX = 1, 
            columnsY = 5:7, 
            types = list(continuous = "points"))
ggduo(nrc_train_all, 
            columnsX = 2, 
            columnsY = 8:10, 
            types = list(continuous = "points"))
ggduo(nrc_train_all, 
            columnsX = 3, 
            columnsY = 11:13, 
            types = list(continuous = "points"))
```

```{r fig.width=8}
# Fit a linear model to the transformed data
gam_fit <- 
  lm_mod %>% 
  fit(rank ~ ., data = nrc_train_baked)

  # Summarise model and check the fit
tidy(gam_fit)
glance(gam_fit)

nrc_gam_train_pred <- augment(gam_fit,
                              nrc_train_baked)
nrc_gam_test_pred <- augment(gam_fit,
                             nrc_test_baked)
metrics(nrc_gam_test_pred, truth = rank, 
        estimate = .pred)

p_fg <- ggplot(nrc_gam_train_pred) +
  geom_point(aes(x = .pred, y=rank)) 
p_fe <- ggplot(nrc_gam_train_pred) +
  geom_point(aes(x = .pred, y = .resid)) 
p_fg + p_fe
```

```{r fig.width=12}
  # Plot the fitted model against each predictor
nrc_gam_train_pred_all <- nrc_train %>%
  select(research, student, diversity) %>%
  bind_cols(nrc_gam_train_pred)
p4 <- ggplot(nrc_gam_train_pred_all) +
  geom_point(aes(x = research, y = rank)) +
  geom_point(aes(x = research, y = .pred),
             colour="blue")
p5 <- ggplot(nrc_gam_train_pred_all) +
  geom_point(aes(x = student, y = rank)) +
  geom_point(aes(x = student, y = .pred),
             colour="blue")
p6 <- ggplot(nrc_gam_train_pred_all) +
  geom_point(aes(x = diversity, y = rank)) +
  geom_point(aes(x = diversity, y = .pred),
             colour="blue")
p4 + p5 + p6
```

*Training R^2 is 0.614 . The model explains about 60% of the variation in rank. Only research is statistically significant for the model. Test MSE is 12.0.*

e. Based on the model fit statistics, and test mse, and plots, which model is best? How do the predictors relate to the rank? Are all the predictors important for predicting rank? Could the model be simplified?

*Both models perform almost identically, so we would choose the simpler one, linear regression. And recommend that for a final model, only to use research as a predictor.*

